{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction gitbook上传 1. cd F:\\Typora数据储存\\gitbook_blog 2. cmd上：进入H:\\software\\nvm\\v12.16.3\\node_global\\gitbook build ./ ./docs 3. 在gitbook_blog 目录中打开gitbash 4. git add ./docs 5. git commit -m “message” 6. git push origin gitbook_blog gitbook插件 https://segmentfault.com/a/1190000019806829 git .gitignore 文件不生效 git rm -r --cached . powered by Gitbook该文件最后修改时间： 2021-09-26 23:47:24 "},"数据库/Hibernate.html":{"url":"数据库/Hibernate.html","title":"Hibernate","keywords":"","body":"powered by Gitbook该文件最后修改时间： 2021-09-26 23:53:18 "},"数据库/mybaties与Redies简介.html":{"url":"数据库/mybaties与Redies简介.html","title":"Mybaties与Redies简介","keywords":"","body":"一。MyBatis 1.简介 ① MyBatis是一个持久层框架，完成的是对数据库的访问和操作；（CRUD） ② 它解决了JDBC对数据库的操作与访问过程中存在的问题，是对原有JDBC技术的封装 ③ MyBatis解决JDBC的问题 【1】虽然JDBC操作数据库的方式很直观，但其核心就是对于数据库的操作。多个方法间存在大量的冗余 【2】基于Java中面向对象的基本思想，所以我们会将查询出来的结果封装成一个对象，这一系列操作都需要手动完成；将对象和表单做一个映射关系，将一个个属性提取出来并赋值给对象，手动ORM映射； 【3】对于多个用户的相同查询操作，没有进行优化，这样会造成运行效率偏低（主要指缓存层Cache） ④ 以.xml配置文件的方式实现DAO接口去除其中冗余代码 2.MyBatis运行原理与流程图 ① 运行原理 MyBatis应用程序根据XML配置文件创建SqlSessionFactory，SqlSessionFactory在根据配置，获取一个SqlSession，配置和来源于两个地方，一处是配置文件，一处来源于Java代码的注解。SqlSession包含了执行sql所需要的所有方法，可以通过SqlSession实例直接运行映射的sql语句，完成对数据的CRUD操作和事务提交等，完成后关闭。 ​ 3.MyBatis缓存 ① 原理： 将频繁查询的数据存储在硬盘中，作为缓存区;当客户端发送请求时，缓存区没有相应的结果，那么就进入数据库查询结果，先在缓存区中存储在返回给客户端，如果缓存区中有，那么就直接返回给客户端. 目的：较少与数据库的通信次数，提高程序的查询效率 缺点：成本高 不安全 缓存在实践张一丁只存储那些频繁查询的数据 以硬盘的空间换取程序运行的时间 ② 注意事项： 【1】 只有当SqlSession关闭时，数据才会存入缓存区 【2】 脏数据问题：当缓存区中数据与数据库中数据不一致时，我们成缓存中的这一部分为脏数据 【3】 MyBatis在进行事务提交时，会自动清空缓存 【4】 在查询操作后一定要关闭SqlSession 增删改操作一定要控制事务 4.$与#的区别 ① #{} 是以？占位符的形式成成sql语句，然后在赋值，相当于调用了pstmt.setXXX()这个方法给sql语句中的条件赋值 ${} 是以字符串拼接的形式来给sql语句赋值的（将sql语句拆分成多个字符串，将所要插入的值放入字符串拼接的部分，并将两部分sql语句连接起来） ② #{}替换的是值，不管输入什么内容都将其当做一个字符串看待 ${}替换的是字段 存在sql注入问题 5.事务的四个特性 ① 原子性：同一事务的多条sql语句时不可分割的整体，都成功才成功，有一条失败则全部失败 ② 一致性：事务开始前和事务结束后，数据库的数据保持一致 ③ 隔离性：事务与事务之间相互独立，互不干涉 ④ 持久性：事务执行后，对数据库的影响是永久的 6.数据库的事务隔离级别 ① 脏读：读未提交（事务锁） 一个事务读取了另一个事务没有提交的临时数据 ② 不可重复读：读提交（行级锁） 一个事务中对相同的数据，进行多次读取，读取结果不一致 ③ 幻影读：重复读（表级锁） 一个事务中对同一张表的多次查询，但结果不一致 ④ 序列化 序列化是事务隔离级别最高的级别，在该模式下所有事务按顺序执行，可以避免以上三种情况 二。Redis 1.简介 redis是一个基于内存的高性能的key value的NoSQL数据库,其中内部数据类型丰富，包括以下五种： ① List：有序可重复 ② Set：无序不可重复 ③ hash：存储键值对 ④ String：所有key的类型 ⑤ ZSet：可排序 按分排序 可做排行榜使用，使用ArrayList实现 2.什么是NoSQL（Not Only SQL） ① 关系型数据库（RDB）：以表格的形式存储数据 ② NoSQL类型数据库：不易表格的形式存储数据 3.NoSQL类型数据库的特点： ① Schemaless：弱结构化（忽略表格 行 列），表格式存储数据结构太过僵化 ② In-Memory 内存型产品 ③ 弱化事务（没有事务 事务简单）：有事务会影响并发，弱化事务会提高系统运行效率 ④ 适用于Cluster（集群）环境 ⑤ 没有复杂的表连接查询操作：表连接的工作原理：拿a表的一条数据与b表的所有数据一一进行比对，运行效率低 ⑥ 支持脚本编程语言（JavaScript lua ...） ⑦ 常见的NOSQL产品 [1]Redis（key value） [2]MongoDB（JSON） [3]HBASE 存储列类型 [4]Cassandra 存储列类型 永远没有单节点故障 4.Redis的特点 ① Redis是一个高性能的key value数据库 ② 是一个基于内存的数据库产品 ③ 内部数据类型丰富 ④ 可持久化 ⑤ 可用于订阅/发布 模型 5.Redis 与 Memcache的区别 ① Redis是一个内存行数据库产品，而Mmcache是一个内存型缓存产品，Redis可以对内存中的数据进行持久化，而Memcache不行 ② Redis不仅仅支持简单的key value类型的数据，同时还提供list，set，hash等数据结构存储 ③ Redis支持数据的主从赋值，即集群环境下的数据备份 ④ Redis支持数据的持久化，可以将内存中年的数据保存到磁盘中，重启的时候可以再次加载原有数据 ⑤ Redis单个value最大限制是1GB，memcached只能保存1MB的数据 6.基础命令 ① String类型 [1]flushall ：清空所有数据库中的所有key flushdb ：清空当前数据库中的所有key [2]keys * ：查询所有key [3]get key ：取值 [4]set key value ：存值 [5]msetnx key key ：创建多个key [6]mget key key ：一次取多个值 [7]getset key value ：获取原始值并设置新值 [8]strlen key ：获取值长度 [9]append key value ：追加值内容 [10]getrange key a b：截取value值，从a下标开始到b下标结束 [11]setnx key value ：如果key不存在则创建，如果key存在则不创建 [12]decr key ：对数值类型的值-1 [13]decrby key x ：对数值类型的值指定-x [14]incr key ：对数值类型的值+1 [15]incrby key x ：对数值类型的值指定+x [16]incrbyfloat key y ：对数值类型的值指定+y浮点数 [17]setex key x value ：设置一个key的存活时间为x秒 [18]psetex key x value ：设置一个key的存活时间为x毫秒 [19]mset key1 value1 key2 value2 ：一次设置多个key value ② List类型（有序可重复） [1]lpush key value ：存值 每次将value存入下标0位置 类似压栈 [2]lrang key 0 -1 ：从下标0开始获取到返回-1结束 [3]lpushx key value ：向已存在key中存值 [4]rpush key value ：存值 [5]rpushx key value ：向已存在key中存值 [6]lpop key ：返回并移除第一个元素 [7]rpop key ：返回并移除最后一个元素 [8]lrange key a b ：取值从下标a开始到下标b结束 [9]llen key ：获取元素个数 [10]lset key x value：替换下标x位置的值 [12]lindex key x ：获取下标x位置的值 [13]lrem key x value：删除key中重复value的x个元素 [14]lrange key a b ：截取从下标a开始到下标b结束 [15]linsert key before value1 value2：在value1之前插入value2 ③ Set类型（无序不可重复） [1]sadd key value ：存值 [2]smembers key ：显示所有值 [3]scard key ：返回元素个数 [4]spop key ：随机获取一个元素 并移除 [5]sunion key1 key2 ：查询key1和key2 [6]srem key value ：删除指定value [7]srandmember key ：随机返回一个元素 [8]sdiff key1 key2 ：删除key1 key2共有的元素 [9]sinter key1 key2 ：查询key1 key2共有的元素 [10]smove key1 value key2 ：将key1中的一个value移动到key2中 [11]sismember key value ：判断值是否存在 ④ ZSet（可排序 按分排序 可做排行榜） [1]zadd key n value ：添加一个key 分数为n [2]zcard key ：返回元素个数 [3]zrange key a b ：查询从下标a开始到下标b结束 zrange key a -1 ：查询从下标a开始到返回-1结束 [4]zrem key value ：移除 [5]zrank key value ：查询指定value的下标 [6]zscore key value ：查询指定value的分数 [7]zrangebyscore key x y ：查询分数x到y之间的value [8]zrevrank key value ：倒序查询指定value的下标 [9]zincrby key x value ：为指定value分数增加x ⑤ Hash（存储键值对） [1]hset Key key value ：在Key中存储一个键值对 [2]hget Key key ：通过键取值 [3]hgetall Key ：查询所有键值对 [4]hdel Key key ：通过key删除对应的键值对 [5]hexists Key key ：判断Key中key是否存在 [6]hkeys Key ：查询所有key [7]hvals Key ：查询所有value [8]hmset Key key value key value：设置多个键值对 [9]hmget Key key1 key2 ：获取多个key的值 [10]hsetnx key value ：设置一个不存在的key的值 [11]hincrby Key key x ：对数值类型的值指定+x [12]hincrbyfloat Key key y ：对数值类型的值指定+y浮点数 7.Redis的持久化 Redis是一个支持持久化的内存数据库，也就是说Redis需要经常将内存中的数据同步到磁盘来保证持久化。 Redis支持两种持久化方式，一种是Snapshottiong（快照RDB）（默认），另一种是Append-Only File（缩写AOF）的方式 ① RDB 【1】运行机制：在某个时间点保存一个完整的数据快照 【2】运行原理： Redis是一个单进程的服务，通过fork产生子进程，父进程继续处理Client请求，子进程负责将快照写入临时文件中，子进程写完后，用临时文件替换原来的快照文件，然后子进程退出 ② AOF 【1】 运行机制：直接将Redis的执行命令，直接写在log文件中 【2】运行原理： Redis通过fork一个子进程，父进程继续处理Client请求，子进程把AOF内容写入缓冲区，子进程写完退出，父进程接收退出消息，将缓冲区AOF写入临时文件，临时文件重命名为appendonly.aof 原来文件被覆盖，整个过程完成 8.分布式缓存原理 分布式缓存由服务端实现管理和控制，由多个客户端节点存储数据，可以进一步提高数据的读取速度。当我们要读取某个数据的时候会根据一致性哈希算法确定数据的存储和读取节点，通过对应的哈希值找到对应的节点，一致性哈希算法的好处在于节点个数发生变化时无需重新计算哈希值，保证数据存储或读取时间可以正确、快速的找到对应的节点、分布式缓存能够高性能的读取数据、能够动态的扩展缓存节点、能够自动发现和切换故障节点、能够自动均衡数据分区，而且能够为使用者提供图形化的界面管理，部署和维护都十分方便，一致性哈希算法是使用MD5与MurmurHash两种计算方式，计算hash值，通过java的TreeMap来模拟换装结构实现均匀分布 由于Redis的数据都存放在内存中，如果没有配置持久化，redis重启后数据就全丢失了，于是需要开启redis的持久化功能，将数据保存到磁盘上，当redis重启后，可以从磁盘中恢复数据。redis提供两种方式进行持久化，一种是RDB持久化（原理是将Reids在内存中的数据库记录定时dump到磁盘上的RDB持久化），另外一种是AOF（append only file）持久化（原理是将Reids的操作日志以追加的方式写入文件）。那么这两种持久化方式有什么区别呢，改如何选择呢？网上看了大多数都是介绍这两种方式怎么配置，怎么使用，就是没有介绍二者的区别，在什么应用场景下使用。 2、二者的区别 RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘，实际操作过程是fork一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。 AOF持久化以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录。 3、二者优缺点 RDB存在哪些优势呢？ 1). 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 2). 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 3). 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。 4). 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 RDB又存在哪些劣势呢？ 1). 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。 2). 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。 AOF的优势有哪些呢？ 1). 该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。 2). 由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。 3). 如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。 4). AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。 AOF的劣势有哪些呢？ 1). 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 2). 根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。 二者选择的标准，就是看系统是愿意牺牲一些性能，换取更高的缓存一致性（aof），还是愿意写操作频繁的时候，不启用备份来换取更高的性能，待手动运行save的时候，再做备份（rdb）。rdb这个就更有些 eventually consistent的意思了。 4、常用配置 RDB持久化配置 Redis会将数据集的快照dump到dump.rdb文件中。此外，我们也可以通过配置文件来修改Redis服务器dump快照的频率，在打开6379.conf文件之后，我们搜索save，可以看到下面的配置信息： save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，则dump内存快照。 save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，则dump内存快照。 save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，则dump内存快照。 AOF持久化配置 在Redis的配置文件中存在三种同步方式，它们分别是： appendfsync always #每次有数据修改发生时都会写入AOF文件。 appendfsync everysec #每秒钟同步一次，该策略为AOF的缺省策略。 appendfsync no #从不同步。高效但是数据不会被持久化。 MySql+Memcached架构的问题 　　实际MySQL是适合进行海量数据存储的，通过Memcached将热点数据加载到cache，加速访问，很多公司都曾经使用过这样的架构，但随着业务数据量的不断增加，和访问量的持续增长，我们遇到了很多问题： 　　1.MySQL需要不断进行拆库拆表，Memcached也需不断跟着扩容，扩容和维护工作占据大量开发时间。 　　2.Memcached与MySQL数据库数据一致性问题。 　　3.Memcached数据命中率低或down机，大量访问直接穿透到DB，MySQL无法支撑。 　　4.跨机房cache同步问题。 　　众多NoSQL百花齐放，如何选择 　　最近几年，业界不断涌现出很多各种各样的NoSQL产品，那么如何才能正确地使用好这些产品，最大化地发挥其长处，是我们需要深入研究和思考的问题，实际归根结底最重要的是了解这些产品的定位，并且了解到每款产品的tradeoffs，在实际应用中做到扬长避短，总体上这些NoSQL主要用于解决以下几种问题 　　1.少量数据存储，高速读写访问。此类产品通过数据全部in-momery 的方式来保证高速访问，同时提供数据落地的功能，实际这正是Redis最主要的适用场景。 　　2.海量数据存储，分布式系统支持，数据一致性保证，方便的集群节点添加/删除。 　　3.这方面最具代表性的是dynamo和bigtable 2篇论文所阐述的思路。前者是一个完全无中心的设计，节点之间通过gossip方式传递集群信息，数据保证最终一致性，后者是一个中心化的方案设计，通过类似一个分布式锁服务来保证强一致性,数据写入先写内存和redo log，然后定期compat归并到磁盘上，将随机写优化为顺序写，提高写入性能。 　　4.Schema free，auto-sharding等。比如目前常见的一些文档数据库都是支持schema-free的，直接存储json格式数据，并且支持auto-sharding等功能，比如mongodb。 ​ Redis最适合所有数据in-momory的场景，虽然Redis也提供持久化功能，但实际更多的是一个disk-backed的功能，跟传统意义上的持久化有比较大的差别，那么可能大家就会有疑问，似乎Redis更像一个加强版的Memcached，那么何时使用Memcached,何时使用Redis呢? ​ 如果简单地比较Redis与Memcached的区别，大多数都会得到以下观点： 1 、Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 2 、Redis支持数据的备份，即master-slave模式的数据备份。 3 、Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 　　通过一张图了解下Redis内部内存管理中是如何描述这些不同数据类型的： 　　首先Redis内部使用一个redisObject对象来表示所有的key和value，redisObject最主要的信息如上图所示：type代表一个value对象具体是何种数据类型，encoding是不同数据类型在redis内部的存储方式，比如：type=string代表value存储的是一个普通字符串，那么对应的encoding可以是raw或者是int，如果是int则代表实际redis内部是按数值型类存储和表示这个字符串的，当然前提是这个字符串本身可以用数值表示，比如:\"123\" \"456\"这样的字符串。 　　这里需要特殊说明一下vm字段，只有打开了Redis的虚拟内存功能，此字段才会真正的分配内存，该功能默认是关闭状态的。通过上图我们可以发现Redis使用redisObject来表示所有的key/value数据是比较浪费内存的，当然这些内存管理成本的付出主要也是为了给Redis不同数据类型提供一个统一的管理接口，实际作者也提供了多种方法帮助我们尽量节省内存使用，我们随后会具体讨论。 　　Redis支持5种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。 　　① string 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。value其实不仅是String，也可以是数字。string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如jpg图片或者序列化的对象。string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。 　　常用命令：get、set、incr、decr、mget等。 　　应用场景：String是最常用的一种数据类型，普通的key/ value 存储都可以归为此类，即可以完全实现目前 Memcached 的功能，并且效率更高。还可以享受Redis的定时持久化，操作日志及 Replication等功能。除了提供与 Memcached 一样的get、set、incr、decr 等操作外，Redis还提供了下面一些操作： 获取字符串长度 往字符串append内容 设置和获取字符串的某一段内容 设置及获取字符串的某一位（bit） 批量设置一系列字符串的内容 　　使用场景：常规key-value缓存应用。常规计数: 微博数, 粉丝数。 　　实现方式：String在redis内部存储默认就是一个字符串，被redisObject所引用，当遇到incr,decr等操作时会转成数值型进行计算，此时redisObject的encoding字段为int。 redis 127.0.0.1:6379> SET name \"runoob\" \"OK\" redis 127.0.0.1:6379> GET name \"runoob\" 　　在以上实例中我们使用了 Redis 的 SET 和 GET 命令。键为 name，对应的值为 runoob。 　　注意：一个键最大能存储512MB。 　　② Redis hash 是一个键值(key => value)对集合。Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 　　常用命令：hget,hset,hgetall 等。 　　应用场景：我们简单举个实例来描述下Hash的应用场景，比如我们要存储一个用户信息对象数据，包含以下信息： 　　　　用户ID为查找的key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key/value结构来存储，主要有以下2种存储方式： 　　　　第一种方式将用户ID作为查找key,把其他信息封装成一个对象以序列化的方式存储，这种方式的缺点是，增加了序列化/反序列化的开销，并且在需要修改其中一项信息时，需要把整个对象取回，并且修改操作需要对并发进行保护，引入CAS等复杂问题。 　　　　第二种方法是这个用户信息对象有多少成员就存成多少个key-value对儿，用用户ID+对应属性的名称作为唯一标识来取得对应属性的值，虽然省去了序列化开销和并发问题，但是用户ID为重复存储，如果存在大量这样的数据，内存浪费还是非常可观的。 　　　　那么Redis提供的Hash很好的解决了这个问题，Redis的Hash实际是内部存储的Value为一个HashMap，并提供了直接存取这个Map成员的接口，如下图： 　　　　也就是说，Key仍然是用户ID, value是一个Map，这个Map的key是成员的属性名，value是属性值，这样对数据的修改和存取都可以直接通过其内部Map的Key(Redis里称内部Map的key为field), 也就是通过 key(用户ID) + field(属性标签) 就可以操作对应属性数据了，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题，很好的解决了问题。 　　　　这里同时需要注意，Redis提供了接口(hgetall)可以直接取到全部的属性数据，但是如果内部Map的成员很多，那么涉及到遍历整个内部Map的操作，由于Redis单线程模型的缘故，这个遍历操作可能会比较耗时，而另其它客户端的请求完全不响应，这点需要格外注意。 　　使用场景：存储部分变更数据，如用户信息等。 　　实现方式：上面已经说到Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap，当成员数量增大时会自动转成真正的HashMap，此时encoding为ht。 redis> HSET myhash field1 \"Hello\" field2 \"World\" \"OK\" redis> HGET myhash field1 \"Hello\" redis> HGET myhash field2 \"World\" 　　实例中我们使用了 Redis HMSET, HGET 命令，HMSET 设置了两个 field=>value 对, HGET 获取对应 field 对应的 value。每个 hash 可以存储 232 -1 键值对（40多亿）。 　　③ Redis list 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 　　常用命令：lpush（添加左边元素）,rpush,lpop（移除左边第一个元素）,rpop,lrange（获取列表片段，LRANGE key start stop）等。 　　应用场景：Redis list的应用场景非常多，也是Redis最重要的数据结构之一，比如twitter的关注列表，粉丝列表等都可以用Redis的list结构来实现。 　　　　List 就是链表，相信略有数据结构知识的人都应该能理解其结构。使用List结构，我们可以轻松地实现最新消息排行等功能。List的另一个应用就是消息队列， 可以利用List的PUSH操作，将任务存在List中，然后工作线程再用POP操作将任务取出进行执行。Redis还提供了操作List中某一段的api，你可以直接查询，删除List中某一段的元素。 　　实现方式：Redis list的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构。 　　Redis的list是每个子元素都是String类型的双向链表，可以通过push和pop操作从列表的头部或者尾部添加或者删除元素，这样List即可以作为栈，也可以作为队列。 获取越接近两端的元素速度越快，但通过索引访问时会比较慢。 使用场景： 　　消息队列系统：使用list可以构建队列系统，使用sorted set甚至可以构建有优先级的队列系统。比如：将Redis用作日志收集器，实际上还是一个队列，多个端点将日志信息写入Redis，然后一个worker统一将所有日志写到磁盘。 　　取最新N个数据的操作：记录前N个最新登陆的用户Id列表，超出的范围可以从数据库中获得。 //把当前登录人添加到链表里 ret = r.lpush(\"login:last_login_times\", uid) //保持链表只有N位 ret = redis.ltrim(\"login:last_login_times\", 0, N-1) //获得前N个最新登陆的用户Id列表 last_login_list = r.lrange(\"login:last_login_times\", 0, N-1) 比如微博： 　　在Redis中我们的最新微博ID使用了常驻缓存，这是一直更新的。但是我们做了限制不能超过5000个ID，因此我们的获取ID函数会一直询问Redis。只有在start/count参数超出了这个范围的时候，才需要去访问数据库。我们的系统不会像传统方式那样“刷新”缓存，Redis实例中的信息永远是一致的。SQL数据库（或是硬盘上的其他类型数据库）只是在用户需要获取“很远”的数据时才会被触发，而主页或第一个评论页是不会麻烦到硬盘上的数据库了。 ;) redis 127.0.0.1:6379> lpush runoob redis (integer) 1 redis 127.0.0.1:6379> lpush runoob mongodb (integer) 2 redis 127.0.0.1:6379> lpush runoob rabitmq (integer) 3 redis 127.0.0.1:6379> lrange runoob 0 10 1) \"rabitmq\" 2) \"mongodb\" 3) \"redis\" redis 127.0.0.1:6379> ;) 　　列表最多可存储 232 - 1 元素 (4294967295, 每个列表可存储40多亿)。 　　④ Redis set是string类型的无序集合。集合是通过hashtable实现的，概念和数学中个的集合基本类似，可以交集，并集，差集等等，set中的元素是没有顺序的。所以添加，删除，查找的复杂度都是O(1)。 　　sadd 命令：添加一个 string 元素到 key 对应的 set 集合中，成功返回1，如果元素已经在集合中返回 0，如果 key 对应的 set 不存在则返回错误。 　　常用命令：sadd,spop,smembers,sunion 等。 　　应用场景：Redis set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。 　　Set 就是一个集合，集合的概念就是一堆不重复值的组合。利用Redis提供的Set数据结构，可以存储一些集合性的数据。 　　案例：在微博中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis还为集合提供了求交集、并集、差集等操作，可以非常方便的实现如共同关注、共同喜好、二度好友等功能，对上面的所有集合操作，你还可以使用不同的命令选择将结果返回给客户端还是存集到一个新的集合中。 　　实现方式： set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因。 　　使用场景： 　　①交集，并集，差集：(Set) ;) //book表存储book名称 set book:1:name ”The Ruby Programming Language” set book:2:name ”Ruby on rail” set book:3:name ”Programming Erlang” //tag表使用集合来存储数据，因为集合擅长求交集、并集 sadd tag:ruby 1 sadd tag:ruby 2 sadd tag:web 2 sadd tag:erlang 3 //即属于ruby又属于web的书？ inter_list = redis.sinter(\"tag.web\", \"tag:ruby\") //即属于ruby，但不属于web的书？ inter_list = redis.sdiff(\"tag.ruby\", \"tag:web\") //属于ruby和属于web的书的合集？ inter_list = redis.sunion(\"tag.ruby\", \"tag:web\") ;) 　　②获取某段时间所有数据去重值 　　这个使用Redis的set数据结构最合适了，只需要不断地将数据往set中扔就行了，set意为集合，所以会自动排重。 sadd key member ;) redis 127.0.0.1:6379> sadd runoob redis (integer) 1 redis 127.0.0.1:6379> sadd runoob mongodb (integer) 1 redis 127.0.0.1:6379> sadd runoob rabitmq (integer) 1 redis 127.0.0.1:6379> sadd runoob rabitmq (integer) 0 redis 127.0.0.1:6379> smembers runoob 1) \"redis\" 2) \"rabitmq\" 3) \"mongodb\" ;) 　　注意：以上实例中 rabitmq 添加了两次，但根据集合内元素的唯一性，第二次插入的元素将被忽略。集合中最大的成员数为 232 - 1(4294967295, 每个集合可存储40多亿个成员)。 　　⑤ Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 　　zadd 命令：添加元素到集合，元素在集合中存在则更新对应score。 　　常用命令：zadd,zrange,zrem,zcard等 　　使用场景：Redis sorted set的使用场景与set类似，区别是set不是自动有序的，而sorted set可以通过用户额外提供一个优先级(score)的参数来为成员排序，并且是插入有序的，即自动排序。当你需要一个有序的并且不重复的集合列表，那么可以选择sorted set数据结构，比如twitter 的public timeline可以以发表时间作为score来存储，这样获取时就是自动按时间排好序的。和Set相比，Sorted Set关联了一个double类型权重参数score，使得集合中的元素能够按score进行有序排列，redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的,但分数(score)却可以重复。比如一个存储全班同学成绩的Sorted Set，其集合value可以是同学的学号，而score就可以是其考试得分，这样在数据插入集合的时候，就已经进行了天然的排序。另外还可以用Sorted Set来做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行。 　　实现方式：Redis sorted set的内部使用HashMap和跳跃表(SkipList)来保证数据的存储和有序，HashMap里放的是成员到score的映射，而跳跃表里存放的是所有的成员，排序依据是HashMap里存的score,使用跳跃表的结构可以获得比较高的查找效率，并且在实现上比较简单。 zadd key score member ;) redis 127.0.0.1:6379> zadd runoob 0 redis (integer) 1 redis 127.0.0.1:6379> zadd runoob 0 mongodb (integer) 1 redis 127.0.0.1:6379> zadd runoob 0 rabitmq (integer) 1 redis 127.0.0.1:6379> zadd runoob 0 rabitmq (integer) 0 redis 127.0.0.1:6379> > ZRANGEBYSCORE runoob 0 1000 1) \"mongodb\" 2) \"rabitmq\" 3) \"redis\" ;) 各个数据类型应用场景： 类型 简介 特性 场景 String(字符串) 二进制安全 可以包含任何数据,比如jpg图片或者序列化的对象,一个键最大能存储512M --- Hash(字典) 键值对集合,即编程语言中的Map类型 适合存储对象,并且可以像数据库中update一个属性一样只修改某一项属性值(Memcached中需要取出整个字符串反序列化成对象修改完再序列化存回去) 存储、读取、修改用户属性 List(列表) 链表(双向链表) 增删快,提供了操作某一段元素的API 1、最新消息排行等功能(比如朋友圈的时间线) 2、消息队列 Set(集合) 哈希表实现,元素不重复 1、添加、删除、查找的复杂度都是O(1) 2、为集合提供了求交集、并集、差集等操作 1、共同好友 2、利用唯一性,统计访问网站的所有独立ip 3、好友推荐时,根据tag求交集,大于某个阈值就可以推荐 Sorted Set(有序集合) 将Set中的元素增加一个权重参数score,元素按score有序排列 数据插入集合时,已经进行天然排序 1、排行榜 2、带权重的消息队列 Redis实际应用场景 　　Redis在很多方面与其他数据库解决方案不同：它使用内存提供主存储支持，而仅使用硬盘做持久性的存储；它的数据模型非常独特，用的是单线程。另一个大区别在于，你可以在开发环境中使用Redis的功能，但却不需要转到Redis。 　　转向Redis当然也是可取的，许多开发者从一开始就把Redis作为首选数据库；但设想如果你的开发环境已经搭建好，应用已经在上面运行了，那么更换数据库框架显然不那么容易。另外在一些需要大容量数据集的应用，Redis也并不适合，因为它的数据集不会超过系统可用的内存。所以如果你有大数据应用，而且主要是读取访问模式，那么Redis并不是正确的选择。 　　然而我喜欢Redis的一点就是你可以把它融入到你的系统中来，这就能够解决很多问题，比如那些你现有的数据库处理起来感到缓慢的任务。这些你就可以通过Redis来进行优化，或者为应用创建些新的功能。在本文中，我就想探讨一些怎样将Redis加入到现有的环境中，并利用它的原语命令等功能来解决 传统环境中碰到的一些常见问题。在这些例子中，Redis都不是作为首选数据库。 1、显示最新的项目列表 　　下面这个语句常用来显示最新项目，随着数据多了，查询毫无疑问会越来越慢。 SELECT * FROM foo WHERE ... ORDER BY time DESC LIMIT 10 　　在Web应用中，“列出最新的回复”之类的查询非常普遍，这通常会带来可扩展性问题。这令人沮丧，因为项目本来就是按这个顺序被创建的，但要输出这个顺序却不得不进行排序操作。 　　类似的问题就可以用Redis来解决。比如说，我们的一个Web应用想要列出用户贴出的最新20条评论。在最新的评论边上我们有一个“显示全部”的链接，点击后就可以获得更多的评论。 ​ 我们假设数据库中的每条评论都有一个唯一的递增的ID字段。我们可以使用分页来制作主页和评论页，使用Redis的模板，每次新评论发表时，我们会将它的ID添加到一个Redis列表： LPUSH latest.comments 　　我们将列表裁剪为指定长度，因此Redis只需要保存最新的5000条评论： LTRIM latest.comments 0 5000 每次我们需要获取最新评论的项目范围时，我们调用一个函数来完成（使用伪代码）： ;) FUNCTION get_latest_comments(start, num_items): id_list = redis.lrange(\"latest.comments\",start,start+num_items - 1) IF id_list.length ;) 　　这里我们做的很简单。在Redis中我们的最新ID使用了常驻缓存，这是一直更新的。但是我们做了限制不能超过5000个ID，因此我们的获取ID函数会一直询问Redis。只有在start/count参数超出了这个范围的时候，才需要去访问数据库。我们的系统不会像传统方式那样“刷新”缓存，Redis实例中的信息永远是一致的。SQL数据库（或是硬盘上的其他类型数据库）只是在用户需要获取“很远”的数据时才会被触发，而主页或第一个评论页是不会麻烦到硬盘上的数据库了。 2、删除与过滤 　　我们可以使用LREM来删除评论。如果删除操作非常少，另一个选择是直接跳过评论条目的入口，报告说该评论已经不存在。 redis 127.0.0.1:6379> LREM KEY_NAME COUNT VALUE 　　有些时候你想要给不同的列表附加上不同的过滤器。如果过滤器的数量受到限制，你可以简单的为每个不同的过滤器使用不同的Redis列表。毕竟每个列表只有5000条项目，但Redis却能够使用非常少的内存来处理几百万条项目。 3、排行榜相关 　　另一个很普遍的需求是各种数据库的数据并非存储在内存中，因此在按得分排序以及实时更新这些几乎每秒钟都需要更新的功能上数据库的性能不够理想。 　　典型的比如那些在线游戏的排行榜，比如一个Facebook的游戏，根据得分你通常想要： 　　　　 - 列出前100名高分选手 　　　　 - 列出某用户当前的全球排名 　　这些操作对于Redis来说小菜一碟，即使你有几百万个用户，每分钟都会有几百万个新的得分。 　　模式是这样的，每次获得新得分时，我们用这样的代码： ZADD leaderboard 　　你可能用userID来取代username，这取决于你是怎么设计的。 　　得到前100名高分用户很简单：ZREVRANGE leaderboard 0 99。 　　用户的全球排名也相似，只需要：ZRANK leaderboard 。 4、按照用户投票和时间排序 　　排行榜的一种常见变体模式就像Reddit或Hacker News用的那样，新闻按照类似下面的公式根据得分来排序： 　　score = points / time^alpha 　　因此用户的投票会相应的把新闻挖出来，但时间会按照一定的指数将新闻埋下去。下面是我们的模式，当然算法由你决定。 　　模式是这样的，开始时先观察那些可能是最新的项目，例如首页上的1000条新闻都是候选者，因此我们先忽视掉其他的，这实现起来很简单。 　　每次新的新闻贴上来后，我们将ID添加到列表中，使用LPUSH + LTRIM，确保只取出最新的1000条项目。 　　有一项后台任务获取这个列表，并且持续的计算这1000条新闻中每条新闻的最终得分。计算结果由ZADD命令按照新的顺序填充生成列表，老新闻则被清除。这里的关键思路是排序工作是由后台任务来完成的。 5、处理过期项目 　　另一种常用的项目排序是按照时间排序。我们使用unix时间作为得分即可。 　　模式如下： 　　　　- 每次有新项目添加到我们的非Redis数据库时，我们把它加入到排序集合中。这时我们用的是时间属性，current_time和time_to_live。 　　　　- 另一项后台任务使用ZRANGE…SCORES查询排序集合，取出最新的10个项目。如果发现unix时间已经过期，则在数据库中删除条目。 6、计数 　　Redis是一个很好的计数器，这要感谢INCRBY和其他相似命令。 　　我相信你曾许多次想要给数据库加上新的计数器，用来获取统计或显示新信息，但是最后却由于写入敏感而不得不放弃它们。 　　好了，现在使用Redis就不需要再担心了。有了原子递增（atomic increment），你可以放心的加上各种计数，用GETSET重置，或者是让它们过期。 　　例如这样操作： INCR user: EXPIRE user: 60 　　你可以计算出最近用户在页面间停顿不超过60秒的页面浏览量，当计数达到比如20时，就可以显示出某些条幅提示，或是其它你想显示的东西。 7、特定时间内的特定项目 　　另一项对于其他数据库很难，但Redis做起来却轻而易举的事就是统计在某段特点时间里有多少特定用户访问了某个特定资源。比如我想要知道某些特定的注册用户或IP地址，他们到底有多少访问了某篇文章。 　　每次我获得一次新的页面浏览时我只需要这样做： SADD page:day1: 　　当然你可能想用unix时间替换day1，比如time()-(time()%3600*24)等等。 　　想知道特定用户的数量吗？只需要使用 SCARD page:day1: 　　需要测试某个特定用户是否访问了这个页面？ SISMEMBER page:day1: 8、实时分析正在发生的情况，用于数据统计与防止垃圾邮件等 　　我们只做了几个例子，但如果你研究Redis的命令集，并且组合一下，就能获得大量的实时分析方法，有效而且非常省力。使用Redis原语命令，更容易实施垃圾邮件过滤系统或其他实时跟踪系统。 9、Pub/Sub 　　Redis的Pub/Sub非常非常简单，运行稳定并且快速。支持模式匹配，能够实时订阅与取消频道。 10、队列 　　你应该已经注意到像list push和list pop这样的Redis命令能够很方便的执行队列操作了，但能做的可不止这些：比如Redis还有list pop的变体命令，能够在列表为空时阻塞队列。 　　现代的互联网应用大量地使用了消息队列（Messaging）。消息队列不仅被用于系统内部组件之间的通信，同时也被用于系统跟其它服务之间的交互。消息队列的使用可以增加系统的可扩展性、灵活性和用户体验。非基于消息队列的系统，其运行速度取决于系统中最慢的组件的速度（注：短板效应）。而基于消息队列可以将系统中各组件解除耦合，这样系统就不再受最慢组件的束缚，各组件可以异步运行从而得以更快的速度完成各自的工作。 　　此外，当服务器处在高并发操作的时候，比如频繁地写入日志文件。可以利用消息队列实现异步处理。从而实现高性能的并发操作。 11、缓存 　　Redis的缓存部分值得写一篇新文章，我这里只是简单的说一下。Redis能够替代memcached，让你的缓存从只能存储数据变得能够更新数据，因此你不再需要每次都重新生成数据了。 powered by Gitbook该文件最后修改时间： 2020-07-04 13:54:39 "},"数据库/mybatis.html":{"url":"数据库/mybatis.html","title":"Mybatis","keywords":"","body":"mybatis 的优缺点 MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以对配置和原生Map使用简单的 XML 或注解（实体和数据库的映射可以在XML中间中，也可以使用注解），将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录。 一、MyBatis框架的优点： 　　1. 与JDBC相比，减少了50%以上的代码量。（Mybatis中Connection的commit等具体实现也是jdbc实现的） 　　2. MyBatis语法较为简单，小巧并且简单易学。 　　3. sql实现灵活，SQL写在XML里，从程序代码中彻底分离，降低耦合度，便于统一管理和优化，可重用。 　　4. 提供XML标签，支持编写动态SQL语句（XML中使用if, else）。 　　5. 提供映射标签，支持对象与数据库的ORM字段关系映射（在XML中配置映射关系，也可以使用注解）。 二、MyBatis框架的缺点： 　　1. SQL语句的编写工作量较大，尤其是字段多、关联表多时，更是如此，对开发人员编写SQL语句的功底有一定要求。 　　2. SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库。 入门案例 实体类属性和表的列名需要保持一致的原因是excutor中执行完sql语句后返回的列名需要和你接收的实体类的属性一致才能成功给实体类赋值。 由于在 Windows 环境下 MySQL 不区分大小，所以 userName 等同 username ，不过要注意在 Linux 环境下 MySQL 严格区别大小写。 为了解决实体类属性名与数据库表列名不一致，有以下解决方法： 在 SQL 语句中为所有列起别名，使别名与实体类属性名一致（执行效率相对高，开发效率低） 使用 resultMap 完成结果集到实体类的映射（执行效率相对低，开发效率高） SELECT * FROM user 所用模式 Mybatis 在使用代理 Mapper 的方式（session.getMapper(xxx.class)）实现增删查改的时候只做了以下两件事（划重点） 创建代理对象 在代理对象中调用 selectList 自定义mybatis流程 https://blog.csdn.net/a1092882580/article/details/104086181 day01_eesy_04mybatis sqlsessionfactoryBuilder的作用是将xml的解析部分与factory的构造函数拿出来，便于xml解析器维护和使得factory别太臃肿 编写读取配置文件Resource类，获得配置文件字节流（XMLConfigBuilder形参） 编写SqlsessionFactoryBuilder类 准备工具类XMLConfigBuilder,用于解析XML文件（SqlsessionfactoryBuilder使用） 准备自定义注解接口、配置类Configuration、封装Sql语句和结果类型的Mapper类（XMLConfigBuilder保存解析信息） 编写SqlsessionFactory接口、实现SqlsessionFacatory（SqlsessionFactoryBuilder生成对象） 编写Sqlsession接口，实现Sqlsession（SqlsessionFactory生成对象） 编写链接数据库的工具类DataSourceUtil，编写代理类MapperProxy(Implements InvocationHandler，其实就是调用Executor的selectList方法)（sqlSession类使用） 编写Exector类，用于执行jdbc操作（MapperProxy使用） CURD 获取参数时 parameterType = \"与表对应的实体类\" {实体类属性}： 读取实体类的属性 模糊查询 {} 需要在测试类中自己家%等通配符，像PreparedStatement ，#{}可以有效防止sql注入，${}则可能导致sql注入成功。 public void testListUsersByName() { List users = mapper.listUsersByName(\"%王%\"); // 使用 Stream 流 + 方法引用，需要至少jdk8 users.forEach(System.out::println); } 由于映射文件中的 SQL 语句并没有对参数进行模糊查询处理，所以在调用方法的时候我们必须手动为查询的关键字进行%拼接，这样很不方便。如果想调用方法查询时，只传入查询的关键字，那么可以采用以下方法 ： SELECT * FROM user WHERE username LIKE concat('%',#{name},'%'); SELECT * FROM user WHERE username LIKE \"%\"#{name}\"%\"； parameterType 简单对象 传递pojo对象 首先介绍一下 OGNL 表达式 全称 Object Graphic Navigation Language ，即对象图导航语言 它是通过对象的取值方法来获取数据。在写法上把get给省略了。 传递pojo包装对象 在开发中如果想实现复杂查询 ，查询条件不仅包括用户查询条件，还包括其它的查询条件（比如将用户购买商品信息也作为查询条件），这时可以使用 pojo 包装对象传递输入参数，即将多个需要操作的类放到一个总体类中 实体类属性名与数据库表列名不一致 在某些情况下，实体类的属性和数据库表的列名并不一致，那么就会出现查询结果无法封装进实体类。因为select的返回的数据是依据列名与实体类属性的对应关系进行赋值的，失败便为空，修改实体类进行演示 在 SQL 语句中为所有列起别名，使别名与实体类属性名一致（执行效率相对高，开发效率低） SELECT id AS userId, username AS userName, birthday AS userBirthday, sex AS userSex, address AS userAddress FROM user 使用 resultMap 完成结果集到实体类的映射（执行效率相对低，开发效率高） SELECT * FROM user 使用Dao实现类（划重点->对比源码） 若使用sqlsession（defaultSqlSession）的selectList等方法，就是对传入的单个方法（“statement”，arg）进行相应executor(CachingExecutor->SimpleExecutor(extends BaseExecutor))操作 接着进入执行SQL语句处理（MyBatis在第一次执行SQL操作时，在获取Statement时，会去获取数据库链接，如果我们配置的数据源为POOLED，这里会使用PooledDataSource来获取connection），statementHandler（RoutingStatementHandler->PreparedStatementHandler(extends BaseStatementHandler)） 接着封装处理ResultSetHandler（DefaultResultSetHandler），接着调用excute方法进行JDBC的操作并进行封装，返回结果集。 使用Dao代理类（划重点->对比源码、自定义mybatis） 若是用getmapper则是会在代理对象调用方法时调用动态代理的proxyhadler对像中的invoke方法进行增强，增强时对调用的方法进行switch case 选择，继而调用sqlsession的相应CRUD方法（即上面的流程），返回了一个整个增强的代理类实例，再调用实例方法时增强返回增强后的结果集。 propeties 全局变量 第一种，采用全局的内部配置。采用这种方式的话，如果需要配置多个数据库环境，那么像 username、password 等属性就可以复用，提高开发效率。 ```xml PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"> * resource： ![image-20200720212414351](F:\\Typora数据储存\\数据库\\mybatis.assets\\image-20200720212414351.png) * url ![image-20200720212823840](F:\\Typora数据储存\\数据库\\mybatis.assets\\image-20200720212823840.png) #### typeAliases 标签 之前在编写映射文件的时候， resultType 这个属性可以写 int、INT 等，就是因为 Mybatis 给这些类型起了别名。Mybatis 内置的别名如表格所示： 如果我们也想给某个实体类指定别名的时候，就可以采用 typeAliases 标签。用法如下： ``` typeAlias 子标签用于配置别名。其中 type 属性用于指定要配置的类的全限定类名（该类只能是某个domain实体类）, alias 属性指定别名。一旦指定了别名，那么别名就不再区分大小写。 也就是说，此时我们可以在映射文件中这样写 resultType=\"user\" ，也可以写 resultType=\"USER\"。 当我们有多个实体类需要起别名的时候，那么我们就可以使用 package 标签。 当我们有多个实体类需要起别名的时候，那么我们就可以使用 package 标签。 ``` ``` package 标签指定要配置别名的包，当指定之后，该包下的**所有实体类都会注册别名**，并且**类名就是别名**，不再区分大小写 package 标签还可以将**某个包内的映射器接口实现全部注册为映射器**，如下所示 ``` ``` 这样配置后，我们就无需一个一个地配置 Mapper 接口了。不过，**这种配置方式的前提是映射配置文件位置必须和dao接口的包结构相同** > * mapper的recourse加载，扫描映射文件来查找接口类，映射文件命名为**接口全类名**（接口类名可以和映射文件名不同，因为映射文件有namespace），但可以dao接口和mapper文件不在同一路径下,**mybatis原始开发Dao.xml文件与接口文件不在同一路径下,仅能用resource加载映射文件** > * mapper的class加载，仅适用于类路径下,**接口文件与映射文件在同一路径下**,且**接口名与映射文件名相同**的情况.（扫描接口类来查找映射文件，注解的时候用的多） > * package,适用于类路径下,接口文件与映射文件在**同一路径**下,且接口名与映射**文件名相同**的情况.(package 的优势是若在同一路径且接口名与映射**文件名相同**，则可以定位到包名就行，不要定位到每个类名) > > > package定位时，若包不同且写的为xml所在的包，会报Type interface com.itheima.dao.IUserDao is not known to the MapperRegistry. > > 若写的为接口的包，则直接找不到statement > > Mybatis中接口和对应的mapper文件**不一定要放在同一个包下**，放在一起的目的是为了Mybatis进行自动扫描，并且要注意此时java接口类的名称和mapper文件的名称要相同，否则会报异常，由于此时**Mybatis会自动解析对应的接口和相应的配置文件**（class和package），所以就不需要配置mapper文件的位置了。**因为通常的xml文件的namespace、id需要到接口类的信息，而此时mybatis最先开始的是 XML的解析，若是不同包，config.xml调用的将是mappers中mapper的包（和接口类不在同一级），尚未将其他的为用到的接口类的结构信息通过类加载器加载到JVM的方法区中，因此会缺乏相关的信息组成mappers的成员对象，调用失败导致报错。而若是在同一个包，编译成.class到target包中时，recourse和java包会合并，则可以自动寻找同一级中是否有对应的同名接口信息，便可省去配置的麻烦**，resouce不用受此拘束肯定额外调用了别的方法，会造成更多的资源浪费，为了特殊需求设计的。 > > 1. 接口和文件在同一个包中 > 1.1 默认maven构建 > 如果在工程中使用了maven构建工具，那么就会出现一个问题：我们知道在典型的maven工程中，目录结构有：src/main/java和src/main/resources，前者是用来存放java源代码的，后者则是存放一些资源文件，比如配置文件等，**在默认的情况下maven打包的时候，对于src/main/java目录只打包源代码，而不会打包其他文件。所以此时如果把对应的mapper文件放到src/main/java目录下时，不会打包到最终的jar文件夹中，也不会输出到target文件夹中，**由于在进行单元测试的时候执行的是/target目录下/test-classes下的代码，所以在测试的时候也不会成功。 > > 为了实现在maven默认环境下打包时，Mybatis的接口和mapper文件在同一包中，可以通过将接口文件放在src/main/java某个包中，而在src/main/resources目录中建立同样的包，这是一种约定优于配置的方式，这样在maven打包的时候就会将src/main/java和src/main/resources相同包下的文件合并到同一包中。 > > 在默认maven打包的环境下，不要将接口文件和mapper文件全部放到src/main/java，这样也不会把mapper文件打包进去 > > 简要的过程如下（与如下的例子无关）： > > ``` > src/main/java > edu.zju.mapper > UserMapper.java > src/main/resources > config.xml > edu.zju.mapper > UserMapper.xml > ``` > > 如上这种方式在maven打包之后的目录如下： > > ``` > -src/main/java > -config.xml > -edu.zju.mapper > -UserMapper.java > -UserMapper.xml > ``` > > 2.1 更改maven构建配置 > > 如果不想将接口和mapper文件分别放到`src/main/java`和`src/main/resources`中，而是全部放到`src/main/java`，那么在构建的时候需要指定maven打包需要包括xml文件，具体配置如下： > > ``` > > > > src/main/java > > **/*.xml > > false > > > > ``` > > #### 动态SQL ###### if ``` SELECT * FROM user WHERE 1 = 1 AND username LIKE CONCAT('%',#{username},'%') AND sex = #{sex} AND address = #{address} AND birthday = #{birthday} 这里加上 WHERE 1 =1 是防止所有条件都为空时拼接 SQL 语句出错。因为不加上 1 = 1 这个恒等条件的话，如果后面查询条件都没拼接成功，那么 SQL 语句最后会带有一个 WHERE 关键字而没有条件，不符合 SQL 语法。 标签中 test 属性是必须的，表示判断的条件。其中有几点需要注意： 如果 test 有多个条件，那么必须使用 and 进行连接，而不能使用 Java 中的 && 运算符,mybatis自己设立的，需要符合他的XML解析规则。 test 中的参数名称必须与实体类的属性保持一致，也就是和 #{参数符号} 保持一致。 如果判断条件为字符串，那么除了判断是否为 null 外，最好也判断一下是否为空字符串，'' ，防止 SQL语句将其作为条件查询。 ``` ###### where ``` SELECT * FROM user AND username LIKE CONCAT('%',#{username},'%') AND sex = #{sex} AND address = #{address} AND birthday = #{birthday} 可以发现，相比之前的 SQL 语句，我们少写了 WHERE 1 = 1，而是使用 标签来代替它。 标签只会在至少有一个子元素的条件返回 SQL 子句的情况下才去插入 WHERE 子句。而且，若语句的开头为 AND 或 OR， 标签也会将它们去除。 简单来说，就是该标签可以动态添加 WHERE 关键字，并且剔除掉 SQL 语句中多余的 AND 或者 OR。 ``` ###### foreach 假如我们现在有一个新的需求，就是根据一个 id 集合，来查询出 id 在该集合中的所有用户，那么又该怎么实现呢？ 如果使用普通 SQL 语句的话，那么查询语句应该这样写：SELECT * FROM user WHERE id IN(41,42,43); 因此，如果想使用动态 SQL 来完成的话，那么我们就应该考虑如何拼接上 id IN(41,42,43) 这一串内容，这时候，我们的 标签就出场了。 **foreach元素的属性主要有 item，index，collection，open，separator，close。** ``` item表示集合中每一个元素进行迭代时的别名， index指 定一个名字，用于表示在迭代过程中，每次迭代到的位置， open表示该语句以什么开始， separator表示在每次进行迭代之间以什么符号作为分隔 符， close表示以什么结束。12345 ``` 在使用foreach的时候最关键的也是最容易出错的就是collection属性，该属性是必须指定的，但是在不同情况 下，该属性的值是不一样的，主要有一下3种情况： ``` 1. 如果传入的的是一个普通对象（没有特殊的数据结构），而该类中有list等collection属性的，需要用到该属性并需要foreach，collection则为其中属性的值，即用的为包装类Vo时。 2. 如果传入的是单参数且参数类型是一个List的时候，collection属性值为list 3. 如果传入的是单参数且参数类型是一个array数组的时候，collection的属性值为array 4. 如果传入的参数是多个的时候，我们就需要把它们封装成一个Map了，collection的属性值为map的key属性 ``` ``` SELECT * FROM user 0\"> #{id} 标签用于遍历集合，每个属性的作用如下所示： collection ： 代表要遍历的集合或数组，这个属性是必须的。如果是遍历数组，那么该值只能为 array open ： 代表语句的开始部份。 close ： 代表语句的结束部份。 item ： 代表遍历集合时的每个元素，相当于一个临时变量。 separator ： 代表拼接每个元素之间的分隔符。 你可以将任何可迭代对象（如 List、Set 等）、Map 对象或者数组对象传递给 foreach 作为集合参数。当使用可迭代对象或者数组时，index 是当前迭代的次数，item 的值是本次迭代获取的元素。当使用 Map 对象（或者 Map.Entry 对象的集合）时，index 是键，item 是值。 注意，SQL 语句中的参数符号 #{id} 应该与 item=\"id\" 保持一致，也就是说，item 属性如果把临时变量声明为 uid 的话，那么使用时就必须写成 #{uid}。 ``` ###### 定义 SQL 片段 - **在上面的例子中，我们在每条 SQL 中都用到了 `SELECT \\* FROM user` ，因此，我们可以把该语句定义为 SQL 片段，以供复用，减少工作量。** - **首先在映射文件中定义代码片段** ``` SELECT * FROM user ``` 接着就可以在需要的地方引用该片段了 ``` 0\"> #{id} ``` ## 连接池 我们在配置数据源的时候一般都==已经配置好数据库连接池的参数==，然后数据源会自动帮我们维护这些。所有这些对开发者而言是透明的。 在 Mybatis 中，数据源 dataSource 共有三类，分别是： UNPOOLED ： 不使用连接池的数据源。采用传统的 javax.sql.DataSource 规范中的连接池，Mybatis 中有针对规范的实现 POOLED ： 使用连接池的数据源。采用池的思想 JNDI ： 使用 JNDI 实现的数据源，采用服务器提供的 JNDI 技术实现，来获取 DataSource 对象，不同的服务器所能拿到的 DataSource 是不一样的。（如果不是web或者maven的war工程，是不能使用的，tomcat-dbcp连接池：**因为需要相应的代码需要在服务器端才能启动，连接池连接的数据源是部署在服务器上的，如tomcat你可以在servlet或者jsp（最终还是会转换成servlet）上进行sqlsession的创建才能在jndi上getConnection到相应的connection，通过浏览器或客户端对服务器的访问调用了服务器上部署的有相应需要连接数据库的方法，才能调用服务器连接的数据库源**） > 连接池和线程池： > > 连接池：（降低物理连接损耗） > 1、连接池是面向数据库连接的 > 2、连接池是为了优化数据库连接资源 > 3、连接池有点类似在客户端做优化 > > 数据库连接是一项有限的昂贵资源，一个数据库连接对象均对应一个物理数据库连接，每次操作都打开一个物理连接，使用完都关闭连接，这样造成系统的性能低下。 数据库连接池的解决方案是在应用程序启动时建立足够的数据库连接，并将这些连接组成一个连接池，由应用程序动态地对池中的连接进行申请、使用和释放。**对于多于连接池中连接数的并发请求，应该在请求队列中排队等待。并且应用程序可以根据池中连接的使用率，动态增加或减少池中的连接数**。 > > 线程池：（降低线程创建销毁损耗） > 1.、线程池是面向后台程序的 > 2、线程池是是为了提高内存和CPU效率 > 3、线程池有点类似于在服务端做优化 > > 线程池是一次性创建一定数量的线程（应该可以配置初始线程数量的），当用请求过来不用去创建新的线程，直接使用已创建的线程，使用后又放回到线程池中。 > 避免了频繁创建线程，及销毁线程的系统开销，提高是内存和CPU效率。 > > 相同点： > 都是事先准备好资源，避免频繁创建和销毁的代价。 > > 扩展： > 对象池： > 对象池技术基本原理的核心有两点：缓存和共享，即对于那些被频繁使用的对象，在使用完后，不立即将它们释放，而是将它们缓存起来，以供后续的应用程序重复使用，从而减少创建对象和释放对象的次数，进而改善应用程序的性能。事实上，由于对象池技术将对象限制在一定的数量，也有效地减少了应用程序内存上的开销。 > > 对于两者是否有联系： > > 1. 首先，每个连接要启动，都需要依赖于一个线程的调用，否则，即使连接在连接池里，没有断开连接，也无法做出行为 > 2. 当线程使用完时，会将连接与线程解绑交还于连接池 > 3. 一般会是多个连接并发执行，即需要多个线程，因此前面也有线程池的管理 > 4. 每个连接都是线程安全的，用synchronized锁住 > > ``` > while(conn == null) { > synchronized(this.state) { > PoolState var10000; > if (!this.state.idleConnections.isEmpty()) { > conn = (PooledConnection)this.state.idleConnections.remove(0); > if (log.isDebugEnabled()) { > log.debug(\"Checked out connection \" + conn.getRealHashCode() + \" from pool.\"); > } > } else if (this.state.activeConnections.size() conn = new PooledConnection(this.dataSource.getConnection(), this); > if (log.isDebugEnabled()) { > log.debug(\"Created connection \" + conn.getRealHashCode() + \".\"); > } > } else { > PooledConnection oldestActiveConnection = (PooledConnection)this.state.activeConnections.get(0); > long longestCheckoutTime = oldestActiveConnection.getCheckoutTime(); > if (longestCheckoutTime > (long)this.poolMaximumCheckoutTime) { > ++this.state.claimedOverdueConnectionCount; > var10000 = this.state; > var10000.accumulatedCheckoutTimeOfOverdueConnections += longestCheckoutTime; > var10000 = this.state; > var10000.accumulatedCheckoutTime += longestCheckoutTime; > this.state.activeConnections.remove(oldestActiveConnection); > if (!oldestActiveConnection.getRealConnection().getAutoCommit()) { > try { > oldestActiveConnection.getRealConnection().rollback(); > } catch (SQLException var16) { > log.debug(\"Bad connection. Could not roll back\"); > } > } > > conn = new PooledConnection(oldestActiveConnection.getRealConnection(), this); > conn.setCreatedTimestamp(oldestActiveConnection.getCreatedTimestamp()); > conn.setLastUsedTimestamp(oldestActiveConnection.getLastUsedTimestamp()); > oldestActiveConnection.invalidate(); > if (log.isDebugEnabled()) { > log.debug(\"Claimed overdue connection \" + conn.getRealHashCode() + \".\"); > } > } else { > try { > if (!countedWait) { > ++this.state.hadToWaitCount; > countedWait = true; > } > > if (log.isDebugEnabled()) { > log.debug(\"Waiting as long as \" + this.poolTimeToWait + \" milliseconds for connection.\"); > } > > long wt = System.currentTimeMillis(); > this.state.wait((long)this.poolTimeToWait); > var10000 = this.state; > var10000.accumulatedWaitTime += System.currentTimeMillis() - wt; > } catch (InterruptedException var17) { > break; > } > } > } > ``` - **如果想要修改 Mybatis 使用的数据源，那么就可以在 Mybatis 配置文件中修改：**（这里 `type` 属性的取值就是为`POOLED、UNPOOLED、JNDI` 。） ``` ``` **查看 `POOLED` 的实现 `PooledDataSource` ，可以看出获取连接时采用了池的思想，大概流程如下图（只是一个简单介绍，不全面）** ```java protected DataSource createDataSource() throws SQLException { if (this.closed) { throw new SQLException(\"Data source is closed\"); } else if (this.dataSource != null) { return this.dataSource; } else { synchronized(this) {// 线程安全 if (this.dataSource != null) { return this.dataSource; } else { this.jmxRegister(); ConnectionFactory driverConnectionFactory = this.createConnectionFactory();// 线创建数据库的连接工厂 boolean success = false; PoolableConnectionFactory poolableConnectionFactory; try { poolableConnectionFactory = this.createPoolableConnectionFactory(driverConnectionFactory); poolableConnectionFactory.setPoolStatements(this.poolPreparedStatements); poolableConnectionFactory.setMaxOpenPreparedStatements(this.maxOpenPreparedStatements); success = true; } catch (SQLException var21) { throw var21; } catch (RuntimeException var22) { throw var22; } catch (Exception var23) { throw new SQLException(\"Error creating connection factory\", var23); } if (success) { this.createConnectionPool(poolableConnectionFactory);// 真正的创建连接池的部分 } success = false; DataSource newDataSource; try { newDataSource = this.createDataSourceInstance();// 再创建数据源 newDataSource.setLogWriter(this.logWriter); success = true; } catch (SQLException var18) { throw var18; } catch (RuntimeException var19) { throw var19; } catch (Exception var20) { throw new SQLException(\"Error creating datasource\", var20); } finally { if (!success) { this.closeConnectionPool();// 创建失败，回滚 } } try { for(int i = 0; i List selectList(String statement, Object parameter, RowBounds rowBounds) { List var5; try { MappedStatement ms = this.configuration.getMappedStatement(statement); var5 = this.executor.query(ms, this.wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); } catch (Exception var9) { throw ExceptionFactory.wrapException(\"Error querying database. Cause: \" + var9, var9); } finally { ErrorContext.instance().reset(); } return var5; } public int update(String statement, Object parameter) { int var4; try { this.dirty = true; MappedStatement ms = this.configuration.getMappedStatement(statement); var4 = this.executor.update(ms, this.wrapCollection(parameter)); } catch (Exception var8) { throw ExceptionFactory.wrapException(\"Error updating database. Cause: \" + var8, var8); } finally { ErrorContext.instance().reset(); } return var4; } public void commit(boolean force) { try { this.executor.commit(this.isCommitOrRollbackRequired(force)); this.dirty = false; } catch (Exception var6) { throw ExceptionFactory.wrapException(\"Error committing transaction. Cause: \" + var6, var6); } finally { ErrorContext.instance().reset(); } } public void rollback(boolean force) { try { this.executor.rollback(this.isCommitOrRollbackRequired(force)); this.dirty = false; } catch (Exception var6) { throw ExceptionFactory.wrapException(\"Error rolling back transaction. Cause: \" + var6, var6); } finally { ErrorContext.instance().reset(); } } ``` * sqlsession无论执行sql语句还是对事物的管理，都会转由executor执行 #### executor对sql的执行 * （simpleExecutor extends baseExecutor） sqlsession->executor->connection * 由configuration以及参数生成语句处理对象 handler，再调用preaparement方法对handler进行connection的获取与连接（当然也通过了transaction，用的是父类baseEexcutor的方法） * 之后将操作都交给了handler，经过实现了statementhandler接口的RoutingStatementHandler->PreparedStatementHandler(extends BaseStatementHandler)之后的preparedstatement等statement后，便是jdbc的excute操作与result的封装 * 一级缓存的发生也在处理后发生 ``` public class SimpleExecutor extends BaseExecutor { public SimpleExecutor(Configuration configuration, Transaction transaction) { super(configuration, transaction); } public int doUpdate(MappedStatement ms, Object parameter) throws SQLException { Statement stmt = null; int var6; try { Configuration configuration = ms.getConfiguration(); StatementHandler handler = configuration.newStatementHandler(this, ms, parameter, RowBounds.DEFAULT, (ResultHandler)null, (BoundSql)null); stmt = this.prepareStatement(handler, ms.getStatementLog()); var6 = handler.update(stmt); } finally { this.closeStatement(stmt); } return var6; } public List doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException { Statement stmt = null; List var9; try { Configuration configuration = ms.getConfiguration(); StatementHandler handler = configuration.newStatementHandler(this.wrapper, ms, parameter, rowBounds, resultHandler, boundSql); stmt = this.prepareStatement(handler, ms.getStatementLog()); var9 = handler.query(stmt, resultHandler); } finally { this.closeStatement(stmt); } return var9; } private Statement prepareStatement(StatementHandler handler, Log statementLog) throws SQLException { Connection connection = this.getConnection(statementLog); Statement stmt = handler.prepare(connection, this.transaction.getTimeout()); handler.parameterize(stmt); return stmt; } ``` ``` public abstract class BaseExecutor implements Executor { protected Connection getConnection(Log statementLog) throws SQLException { Connection connection = this.transaction.getConnection(); return statementLog.isDebugEnabled() ? ConnectionLogger.newInstance(connection, statementLog, this.queryStack) : connection; } } ``` #### executor对事务的处理 * 由simpleExecutor对象处理，但用的方法都继承自baseExecutor * sqlsession(带着在sqlsessionfactory创建的transaction)->executor(transaction)->transaction.commit/close····· * executor 将事务管理交给了 transaction，commit/rollback等对cache和statement的清空（clearLocalCache，flushStatements）也在这里开始进行。 * **在sqlsession的close前，对sql语句的执行都会用getConnection创建的连接进行sql语句的执行，commit也并不会说新建一个事务（transaction），而是清空cache和statement并提交jdbc执行后的结果到mysql。此时仍可以用commit后的sqlsession和transaction进行getMapper代理并调用接口方法，只有close后两者才会消失** ``` public class SimpleExecutor extends BaseExecutor { public Transaction getTransaction() { if (this.closed) { throw new ExecutorException(\"Executor was closed.\"); } else { return this.transaction; } } public void close(boolean forceRollback) { try { try { this.rollback(forceRollback); } finally { if (this.transaction != null) { this.transaction.close(); } } } catch (SQLException var11) { log.warn(\"Unexpected exception on closing transaction. Cause: \" + var11); } finally { this.transaction = null; this.deferredLoads = null; this.localCache = null; this.localOutputParameterCache = null; this.closed = true; } } public void commit(boolean required) throws SQLException { if (this.closed) { throw new ExecutorException(\"Cannot commit, transaction is already closed\"); } else { this.clearLocalCache(); this.flushStatements(); if (required) { this.transaction.commit(); } } } public void rollback(boolean required) throws SQLException { if (!this.closed) { try { this.clearLocalCache(); this.flushStatements(true); } finally { if (required) { this.transaction.rollback(); } } } } public void clearLocalCache() { if (!this.closed) { this.localCache.clear(); this.localOutputParameterCache.clear(); } } ``` #### transaction到connection * JdbcTransaction * 由transaction对与connection 的commit/colse进行管理。 ```java public class JdbcTransaction implements Transaction { public JdbcTransaction(Connection connection) { this.connection = connection; } public Connection getConnection() throws SQLException { if (this.connection == null) { this.openConnection(); } return this.connection; } public void commit() throws SQLException { if (this.connection != null && !this.connection.getAutoCommit()) { if (log.isDebugEnabled()) { log.debug(\"Committing JDBC Connection [\" + this.connection + \"]\"); } this.connection.commit(); } } public void rollback() throws SQLException { if (this.connection != null && !this.connection.getAutoCommit()) { if (log.isDebugEnabled()) { log.debug(\"Rolling back JDBC Connection [\" + this.connection + \"]\"); } this.connection.rollback(); } } public void close() throws SQLException { if (this.connection != null) { this.resetAutoCommit(); if (log.isDebugEnabled()) { log.debug(\"Closing JDBC Connection [\" + this.connection + \"]\"); } this.connection.close(); } } } ``` #### connection * connectionImp类 * 从这开始便是com.mysql.jdbc包的东西了，就是与数据库直接进行交接的部分了 ```java public class ConnectionImpl extends ConnectionPropertiesImpl implements Connection { public void commit() throws SQLException { synchronized(this.getMutex()) { this.checkClosed(); try { if (this.connectionLifecycleInterceptors != null) { IterateBlock iter = new IterateBlock(this.connectionLifecycleInterceptors.iterator()) { void forEach(Object each) throws SQLException { if (!((ConnectionLifecycleInterceptor)each).commit()) { this.stopIterating = true; } } }; iter.doForAll(); if (!iter.fullIteration()) { return; } } if (this.autoCommit && !this.getRelaxAutoCommit()) { throw SQLError.createSQLException(\"Can't call commit when autocommit=true\"); } if (this.transactionsSupported) { if (this.getUseLocalSessionState() && this.versionMeetsMinimum(5, 0, 0) && !this.io.inTransactionOnServer()) { return; } this.execSQL((StatementImpl)null, \"commit\", -1, (Buffer)null, 1003, 1007, false, this.database, (Field[])null, false); } } catch (SQLException var8) { if (\"08S01\".equals(var8.getSQLState())) { throw SQLError.createSQLException(\"Communications link failure during commit(). Transaction resolution unknown.\", \"08007\"); } throw var8; } finally { this.needsPing = this.getReconnectAtTxEnd(); } } } } ``` ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621235803927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjQ5MDM4Mw==,size_16,color_FFFFFF,t_70) ## 多表查询 1. 新建一个接收多表消息的实体类 2. 一对一：建立两者的表，在**一**中的实体类添加**一**的类为成员变量，使用resultmap映射。 一对多： 建立两者的表，实体类上转换成相应的一对多的关系，在**一**中的实体类添加**多**的类为成员变量，使用resultmap映射。 多对多： 建立两者的表并建立中间表，实体类(与一对多一致)转换成相应的一对多的关系，在**一**中的实体类添加**多**的类为成员变量，使用resultmap映射(与一对多一致)，中间表被sql语句中用来与其他sql关键字和函数进行表的拼接 ###### 一对一 **首先编写 SQL 语句，如果想要达到上述效果，那么 SQL 语句可以这样写：** ``` SELECT U.*, a.id AS aid, a.uid, a.money from account a, user u WHERE a.uid = u.id; ``` - **为了可以接收这条语句的查询结果，那么我们应该对账户实体类 `Account` 进行修改，为其增加成员变量 `User` 来接收查询出的用户信息（从表实体应该包含一个主表实体的对象引用）：** ``` public class Account implements Serializable { // 其他成员不展示... /** * 账户对应的用户信息,从表实体应该包含一个主表实体的对象引用 */ private User user; public User getUser() { return user; } public void setUser(User user) { this.user = user; } // 其他setter()/getter()不展示... @Override public String toString() { return \"Account{\" + \"id=\" + id + \", uid=\" + uid + \", money=\" + money + \", user=\" + user + '}'; } ``` * 因为此时账户实体类 Account 已经发生了变化，所以我们应该在映射文件定义用于封装带有 User 信息的 Account 的映射集合resultMap ```xml SELECT U.*, a.id AS aid, a.uid, a.money from account a, user u WHERE a.uid = u.id 用于一对一映射，其中的 property 属性表示要关联的属性，javaType 表示待关联的实体类的全限定类名。 注意，因为 SQL 语句中为 account 表的 id 字段起了别名 aid ，所以在定义 resultMap 的时候，记得主字段写 column=\"aid\"，而不是 column=\"id\"。 ``` ###### 一对多 - **我们应该采用外连接的方式，使得没有账户的用户也可以查询出来：** ``` SELECT u.*, a.id AS aid, a.uid, a.money FROM user u LEFT OUTER JOIN account a ON u.id = a.uid; ``` - **为了可以接收这条语句的查询结果，那么我们应该对用户实体类 `User` 进行修改，为其增加成员变量 `accounts` 来接收查询出的账户信息（主表实体应该包含从表实体的集合引用）：** ```java public class User implements Serializable { // 其他成员省略不展示... /** * 一对多关系映射：主表实体应该包含从表实体的集合引用 */ private List accounts; public List getAccounts() { return accounts; } public void setAccounts(List accounts) { this.accounts = accounts; } // 其他setter()/getter()省略不展示... @Override public String toString() { return \"User{\" + \"id=\" + id + \", username='\" + username + '\\'' + \", birthday=\" + birthday + \", sex='\" + sex + '\\'' + \", address='\" + address + '\\'' + \", accounts=\" + accounts + '}'; } ``` - **修改映射文件** ``` SELECT u.*, a.id AS aid, a.uid, a.money FROM user u LEFT OUTER JOIN account a ON u.id = a.uid; 用于一对多映射，其中的 property 属性表示要关联的集合，ofType 表示集合中的实体类的全限定类名。 ``` ###### 多对多 - **修改用户实体类 `User` ，并添加角色实体类 `Role`，为了能体现出多对多的关系，两个实体类都必须包含对方的一个集合引用（多对多关系其实我们看成是双向的一对多关系）** ```java public class User implements Serializable { /** * 多对多关系映射：包含对方的集合引用 */ private List roles; public List getRoles() { return roles; } public void setRoles(List roles) { this.roles = roles; } @Override public String toString() { return \"User{\" + \"id=\" + id + \", username='\" + username + '\\'' + \", birthday=\" + birthday + \", sex='\" + sex + '\\'' + \", address='\" + address + '\\'' + \", accounts=\" + accounts + \", roles=\" + roles + '}'; } ``` ``` public class Role implements Serializable { private Integer roleId; private String roleName; private String roleDesc; /** * 多对多关系映射：持有对方集合引用 */ private List users; public List getUsers() { return users; } public void setUsers(List users) { this.users = users; } public Integer getRoleId() { return roleId; } public void setRoleId(Integer roleId) { this.roleId = roleId; } public String getRoleName() { return roleName; } public void setRoleName(String roleName) { this.roleName = roleName; } public String getRoleDesc() { return roleDesc; } public void setRoleDesc(String roleDesc) { this.roleDesc = roleDesc; } @Override public String toString() { return \"Role{\" + \"roleId=\" + roleId + \", roleName='\" + roleName + '\\'' + \", roleDesc='\" + roleDesc + '\\'' + \", users=\" + users + '}'; } } ``` - **为了确保所有用户（无论是否具有角色）都可以被查询出来，应该使用左连接** ```SELECT u.*, r.id as rid, r.role_name, r.role_desc FROM user u SELECT u.*, r.id as rid, r.role_name, r.role_desc FROM user u LEFT OUTER JOIN user_role ur ON u.id = ur.uid LEFT OUTER JOIN role r ON ur.rid = r.id; ``` - **开始编写接口层和映射文件** ``` /** * 查询所有用户信息，包含用户所拥有的角色信息 * @return */ List listUsersWithRoles(); ``` ``` SELECT u.*, r.id as rid, r.role_name, r.role_desc FROM user u LEFT OUTER JOIN user_role ur ON u.id = ur.uid LEFT OUTER JOIN role r ON ur.rid = r.id ``` ## 加载 ### 延迟加载和立即加载 延迟加载针对级联使用的，延迟加载的目的是减少内存的浪费和减轻系统负担。 **延迟加载需要的配置：** | 设置参数 | 描述 | 有效值 | 默认值 | | ---------------------- | ------------------------------------------------------------ | ---------------------- | ------------------------------ | | lazyLoadingEnabled | 延迟加载的全局开关。当开启时，所有关联对象都会延迟加载。 特定关联关系中可通过设置fetchType属性来覆盖该项的开关状态。 | true、false | false | | aggressiveLazyLoading | 当开启时，任何方法的调用都会加载该对象的所有属性。否则，每个属性会按需加载（参考lazyLoadTriggerMethods). | true、false | false (true in ≤3.4.1) | | lazyLoadTriggerMethods | 指定哪个对象的方法触发一次全部属性加载。 | 用逗号分隔的方法列表。 | equals,clone,hashCode,toString | ```xml **开启懒加载一定要两个属性都配置吗？** 不是的，真正开启懒加载标签的只是lazyLoadingEnabled：true表示开启，false表示关闭，默认为false。 **如果我不想将全部的级联都设计懒加载怎么办？** association和collection有个fetchType属性可以覆盖全局的懒加载状态：eager表示这个级联不使用懒加载要立即加载，lazy表示使用懒加载。 **既然fetchType可以控制懒加载那么我仅仅配置fetchType不配置全局的可以吗？** 可以的。 **aggressiveLazyLoading是做什么么的？** 它是控制具有懒加载特性的对象的属性的加载情况的。(懒加载对象在被调用方法或属性时，) true表示如果对具有懒加载特性的对象的任意调用会导致这个**对象的完整加载**，false表示每种属性按照需要加载。 **如果我仅仅使用aggressiveLazyLoading不使用lazyLoadingEnabled还有能按需加载吗？即调用getName不执行role的sql语句，就像第一种情况那样** 如果不开启懒加载，在执行user的sql语句时也将执行role的sql语句。因此不能实现需求。 ```xml select * from users where id = #{id} 调用getUser查询数据，从查询结果集解析数据到User对象，当数据解析到lazy1，lazy2，lazy3判断需要执行关联查询 lazyLoadingEnabled=true，将创建lazy1，lazy2，lazy3对应的Proxy延迟执行对象lazyLoader，并保存 当逻辑触发lazyLoadTriggerMethods 对应的方法（equals,clone,hashCode,toString）则执行全部属性加载 如果aggressiveLazyLoading=true，只要触发到对象任何的方法，就会立即加载所有属性的加载 实现延迟加载(一对一、一对多) 实现以下需求： 当查询账户信息时使用延迟加载。也就是说，如果不需要使用用户信息的话，那么只查询账户信息；只有当需要使用用户信息时，才去关联查询 dao： package com.itheima.dao; import com.itheima.doamin.Account; import com.itheima.doamin.User; import java.util.List; public interface IAccountDao { List findAll(); List selectUserById (Integer uid); } package com.itheima.dao; import com.itheima.doamin.User; import sun.nio.cs.US_ASCII; import javax.jws.soap.SOAPBinding; import java.util.List; public interface IUserDao { List findAll(); User findById(Integer userId); } domain: package com.itheima.doamin; import javax.jws.soap.SOAPBinding; import java.io.Serializable; public class Account implements Serializable { private Integer id; private Integer uid; private Double money; private User user; public User getUser() { return user; } public void setUser(User user) { this.user = user; } public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public Integer getUid() { return uid; } public void setUid(Integer uid) { this.uid = uid; } public Double getMoney() { return money; } public void setMoney(Double money) { this.money = money; } @Override public String toString() { return \"Account{\" + \"id=\" + id + \", uid=\" + uid + \", money=\" + money + '}'; } } package com.itheima.doamin; import java.io.Serializable; import java.util.Date; import java.util.List; public class User implements Serializable { private Integer id; private String username; private String address; private String sex; private Date birthday; private List accounts; public List getAccounts() { return accounts; } public void setAccounts(List accounts) { this.accounts = accounts; } public Integer getId() { return id; } public String getUsername() { return username; } public String getAddress() { return address; } public String getSex() { return sex; } public Date getBirthday() { return birthday; } public void setId(Integer id) { this.id = id; } public void setUsername(String username) { this.username = username; } public void setAddress(String address) { this.address = address; } public void setSex(String sex) { this.sex = sex; } public void setBirthday(Date birthday) { this.birthday = birthday; } @Override public String toString() { return \"User{\" + \"id=\" + id + \", username='\" + username + '\\'' + \", address='\" + address + '\\'' + \", sex='\" + sex + '\\'' + \", birthday=\" + birthday + '}'; } } 配置xml： IAccountDao.xml： SELECT * from account; SELECT * from account where uid = #{uid}; IAccountDao.xml： SELECT * from user; select * from user where id = #{id} test: package com.itheima.test; import com.itheima.dao.IAccountDao; import com.itheima.dao.IUserDao; import com.itheima.doamin.Account; import com.itheima.doamin.User; import org.apache.ibatis.io.Resources; import org.apache.ibatis.session.SqlSession; import org.apache.ibatis.session.SqlSessionFactory; import org.apache.ibatis.session.SqlSessionFactoryBuilder; import org.junit.After; import org.junit.Before; import org.junit.Test; import java.io.InputStream; import java.util.List; public class AccountTest { private InputStream in; private SqlSession sqlSession; private IAccountDao accountDao; @Before public void init()throws Exception{ in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(in); sqlSession = factory.openSession(); accountDao = sqlSession.getMapper(IAccountDao.class); } @After public void destory() throws Exception{ sqlSession.commit(); sqlSession.close(); in.close(); } @Test public void testFindAll(){ List accounts = accountDao.findAll(); // for(Account account:accounts){ // // System.out.println(account); // System.out.println(account.getUser()); // } } } package com.itheima.test; import com.itheima.dao.IAccountDao; import com.itheima.dao.IUserDao; import com.itheima.doamin.Account; import com.itheima.doamin.User; import org.apache.ibatis.io.Resources; import org.apache.ibatis.session.SqlSession; import org.apache.ibatis.session.SqlSessionFactory; import org.apache.ibatis.session.SqlSessionFactoryBuilder; import org.junit.After; import org.junit.Before; import org.junit.Test; import java.io.InputStream; import java.util.List; public class UserTest { private InputStream in; private SqlSession sqlSession; private IUserDao userDao; @Before public void init()throws Exception{ in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(in); sqlSession = factory.openSession(); userDao = sqlSession.getMapper(IUserDao.class); } @After public void destory() throws Exception{ sqlSession.commit(); sqlSession.close(); in.close(); } @Test public void testFindAll(){ List users = userDao.findAll(); for(User user:users){ System.out.println(user); System.out.println(user.getAccounts()); } } } 延迟加载原理 Mybatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载，association 指的就是一对一，collection 指的就是一对多查询。在 Mybatis配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabled=true|false。它的原理是，使用 CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用 a.getB().getName()，拦截器 invoke()方法发现 a.getB()是null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来，然后调用 a.setB(b)，于是 a 的对象 b 属性就有值了，接着完成 a.getB().getName()方法的调用。这就是延迟加载的基本原理。当然了，不光是 Mybatis，几乎所有的包括 Hibernate，支持延迟加载的原理都是一样的。 延迟加载源码解析 Setting 配置加载： public class Configuration { /** aggressiveLazyLoading： * 当开启时，任何方法的调用都会加载该对象的所有属性。否则，每个属性会按需加载（参考lazyLoadTriggerMethods). * 默认为true * */ protected boolean aggressiveLazyLoading; /** * 延迟加载触发方法 */ protected Set lazyLoadTriggerMethods = new HashSet(Arrays.asList(new String[] { \"equals\", \"clone\", \"hashCode\", \"toString\" })); /** 是否开启延迟加载 */ protected boolean lazyLoadingEnabled = false; /** * 默认使用Javassist代理工厂 * @param proxyFactory */ public void setProxyFactory(ProxyFactory proxyFactory) { if (proxyFactory == null) { proxyFactory = new JavassistProxyFactory(); } this.proxyFactory = proxyFactory; } //省略... } 延迟加载代理对象创建 DefaultResultSetHandler //#mark 创建结果对象 private Object createResultObject(ResultSetWrapper rsw, ResultMap resultMap, ResultLoaderMap lazyLoader, String columnPrefix) throws SQLException { this.useConstructorMappings = false; // reset previous mapping result final List> constructorArgTypes = new ArrayList>(); final List constructorArgs = new ArrayList(); //#mark 创建返回的结果映射的真实对象 Object resultObject = createResultObject(rsw, resultMap, constructorArgTypes, constructorArgs, columnPrefix); if (resultObject != null && !hasTypeHandlerForResultObject(rsw, resultMap.getType())) { final List propertyMappings = resultMap.getPropertyResultMappings(); for (ResultMapping propertyMapping : propertyMappings) { // issue gcode #109 && issue #149 判断属性有没配置嵌套查询，如果有就创建代理对象 if (propertyMapping.getNestedQueryId() != null && propertyMapping.isLazy()) { //#mark 创建延迟加载代理对象 resultObject = configuration.getProxyFactory().createProxy(resultObject, lazyLoader, configuration, objectFactory, constructorArgTypes, constructorArgs); break; } } } this.useConstructorMappings = resultObject != null && !constructorArgTypes.isEmpty(); // set current mapping result return resultObject; } 代理功能实现 由于Javasisst和Cglib的代理实现基本相同，这里主要介绍Javasisst ProxyFactory接口定义 public interface ProxyFactory { void setProperties(Properties properties); /** * 创建代理 * @param target 目标结果对象 * @param lazyLoader 延迟加载对象 * @param configuration 配置 * @param objectFactory 对象工厂 * @param constructorArgTypes 构造参数类型 * @param constructorArgs 构造参数值 * @return */ Object createProxy(Object target, ResultLoaderMap lazyLoader, Configuration configuration, ObjectFactory objectFactory, List> constructorArgTypes, List constructorArgs); } JavasisstProxyFactory实现 public class JavassistProxyFactory implements org.apache.ibatis.executor.loader.ProxyFactory { /** * 接口实现 * @param target 目标结果对象 * @param lazyLoader 延迟加载对象 * @param configuration 配置 * @param objectFactory 对象工厂 * @param constructorArgTypes 构造参数类型 * @param constructorArgs 构造参数值 * @return */ @Override public Object createProxy(Object target, ResultLoaderMap lazyLoader, Configuration configuration, ObjectFactory objectFactory, List> constructorArgTypes, List constructorArgs) { return EnhancedResultObjectProxyImpl.createProxy(target, lazyLoader, configuration, objectFactory, constructorArgTypes, constructorArgs); } //省略... /** * 代理对象实现，核心逻辑执行 */ private static class EnhancedResultObjectProxyImpl implements MethodHandler { /** * 创建代理对象 * @param type * @param callback * @param constructorArgTypes * @param constructorArgs * @return */ static Object crateProxy(Class type, MethodHandler callback, List> constructorArgTypes, List constructorArgs) { ProxyFactory enhancer = new ProxyFactory(); enhancer.setSuperclass(type); try { //通过获取对象方法，判断是否存在该方法 type.getDeclaredMethod(WRITE_REPLACE_METHOD); // ObjectOutputStream will call writeReplace of objects returned by writeReplace if (log.isDebugEnabled()) { log.debug(WRITE_REPLACE_METHOD + \" method was found on bean \" + type + \", make sure it returns this\"); } } catch (NoSuchMethodException e) { //没找到该方法，实现接口 enhancer.setInterfaces(new Class[]{WriteReplaceInterface.class}); } catch (SecurityException e) { // nothing to do here } Object enhanced; Class[] typesArray = constructorArgTypes.toArray(new Class[constructorArgTypes.size()]); Object[] valuesArray = constructorArgs.toArray(new Object[constructorArgs.size()]); try { //创建新的代理对象 enhanced = enhancer.create(typesArray, valuesArray); } catch (Exception e) { throw new ExecutorException(\"Error creating lazy proxy. Cause: \" + e, e); } //设置代理执行器 ((Proxy) enhanced).setHandler(callback); return enhanced; } /** * 代理对象执行 * @param enhanced 原对象 * @param method 原对象方法 * @param methodProxy 代理方法 * @param args 方法参数 * @return * @throws Throwable */ @Override public Object invoke(Object enhanced, Method method, Method methodProxy, Object[] args) throws Throwable { final String methodName = method.getName(); try { synchronized (lazyLoader) { if (WRITE_REPLACE_METHOD.equals(methodName)) { //忽略暂未找到具体作用 Object original; if (constructorArgTypes.isEmpty()) { original = objectFactory.create(type); } else { original = objectFactory.create(type, constructorArgTypes, constructorArgs); } PropertyCopier.copyBeanProperties(type, enhanced, original); if (lazyLoader.size() > 0) { return new JavassistSerialStateHolder(original, lazyLoader.getProperties(), objectFactory, constructorArgTypes, constructorArgs); } else { return original; } } else { //延迟加载数量大于0 if (lazyLoader.size() > 0 && !FINALIZE_METHOD.equals(methodName)) { //aggressive 一次加载性所有需要要延迟加载属性或者包含触发延迟对象全部加载方法 if (aggressive || lazyLoadTriggerMethods.contains(methodName)) { log.debug(\"==> laze lod trigger method:\" + methodName + \",proxy method:\" + methodProxy.getName() + \" class:\" + enhanced.getClass()); //一次全部加载 lazyLoader.loadAll(); } else if (PropertyNamer.isSetter(methodName)) { //判断是否为set方法，set方法不需要延迟加载 final String property = PropertyNamer.methodToProperty(methodName); lazyLoader.remove(property); } else if (PropertyNamer.isGetter(methodName)) { final String property = PropertyNamer.methodToProperty(methodName); if (lazyLoader.hasLoader(property)) { //延迟加载单个属性 lazyLoader.load(property); log.debug(\"load one :\" + methodName); } } } } } return methodProxy.invoke(enhanced, args); } catch (Throwable t) { throw ExceptionUtil.unwrapThrowable(t); } } } 缓存 什么是缓存？ 缓存就是存在于内存中的临时数据。 为什么要使用缓存？ 为了减少和数据库交互的次数，提高执行效率。 适用于缓存的数据 经常查询并且不经常改变的数据。 数据的正确与否对最终结果影响不大的。 不适用于缓存的数据 经常改变的数据。 数据的正确与否对最终结果影响很大的。例如：商品的库存、银行的汇率、股市的牌价等。 一级缓存 一级缓存是 SqlSession 级别的缓存，只要 SqlSession 没有 flush 或 close，它就会存在。当调用 SqlSession 的修改、添加、删除、commit()、close()、clearCache() 等方法时，就会清空一级缓存。 一级缓存流程如下图： 第一次发起查询用户 id 为 1 的用户信息，Mybatis 会先去找缓存中是否有 id 为 1 的用户信息，如果没有，从数据库查询用户信息。 得到用户信息，将用户信息存储到一级缓存中。 如果 sqlSession 去执行 commit 操作（执行插入、更新、删除），那么 Mybatis 就会清空 SqlSession 中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。 第二次发起查询用户 id 为 1 的用户信息，先去找缓存中是否有 id 为 1 的用户信息，缓存中有，直接从缓存中获取用户信息。 Mybatis 默认就是使用一次缓存的，不需要配置。 一级缓存中存放的是对象。（一级缓存其实就是 Map 结构，直接存放对象） mybatis一级缓存，缓存在SqlSession中。 一级缓存是默认的，不需要配置，localCache中没有缓存时，才去执行queryFromDatabase方法，去查询数据库，并将结果缓存到localCache中。 MyBatis的一级缓存最大范围是SqlSession内部，有多个SqlSession或者分布式的环境下，数据库写操作会引起脏数据，建议设定缓存级别为Statement 源码 public abstract class BaseExecutor implements Executor { public List query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { ErrorContext.instance().resource(ms.getResource()).activity(\"executing a query\").object(ms.getId()); if (this.closed) { throw new ExecutorException(\"Executor was closed.\"); } else { if (this.queryStack == 0 && ms.isFlushCacheRequired()) { this.clearLocalCache(); } List list; try { ++this.queryStack; //在缓存中查找是否有 list = resultHandler == null ? (List)this.localCache.getObject(key) : null; if (list != null) { this.handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); } else { list = this.queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql); } } finally { --this.queryStack; } if (this.queryStack == 0) { Iterator var8 = this.deferredLoads.iterator(); while(var8.hasNext()) { BaseExecutor.DeferredLoad deferredLoad = (BaseExecutor.DeferredLoad)var8.next(); deferredLoad.load(); } this.deferredLoads.clear(); if (this.configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) { this.clearLocalCache(); } } return list; } } 二级缓存 二级缓存是 Mapper 映射级别的缓存，多个 SqlSession 去操作同一个 Mapper 映射的 SQL 语句，多个SqlSession 可以共用二级缓存，二级缓存是跨 SqlSession 的。 二级缓存流程如下图： 当 sqlSession1 去查询用户信息的时候，Mybatis 会将查询数据存储到二级缓存中。 如果 sqlSession3 去执行相同 Mapper 映射下的 SQL 语句，并且执行 commit 提交，那么 Mybatis 将会清空该 Mapper 映射下的二级缓存区域的数据。 sqlSession2 去查询与 sqlSession1 相同的用户信息，Mybatis 首先会去缓存中找是否存在数据，如果存在直接从缓存中取出数据。 如果想使用 Mybatis 的二级缓存，那么应该做以下配置 首先在 Mybatis 配置文件中添加配置 （这一步其实可以忽略，因为默认值为 true） 接着在映射文件中配置 最后在需要使用二级缓存的操作上配置 （针对每次查询都需要最新数据的操作，要设置成 useCache=\"false\"，禁用二级缓存） ``` SELECT * FROM user > - 当我们使用二级缓存的时候，所缓存的类一定要**实现 `java.io.Serializable` 接口**，这样才可以使用序列化的方式来保存对象。 > - 由于是序列化保存对象，所以二级缓存中存放的是数据，而不是整个对象。 ###### 源码 * 首先，excutor是在sqlsessionfactory创建sqlsession时创建的，用以作为sqlsession的参数传入 ```java public class DefaultSqlSessionFactory implements SqlSessionFactory { private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) { Transaction tx = null; DefaultSqlSession var8; try { Environment environment = this.configuration.getEnvironment(); TransactionFactory transactionFactory = this.getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); Executor executor = this.configuration.newExecutor(tx, execType); var8 = new DefaultSqlSession(this.configuration, executor, autoCommit); } catch (Exception var12) { this.closeTransaction(tx); throw ExceptionFactory.wrapException(\"Error opening session. Cause: \" + var12, var12); } finally { ErrorContext.instance().reset(); } return var8; } 接着看下excutor有什么类型 public class Configuration { public Executor newExecutor(Transaction transaction, ExecutorType executorType) { executorType = executorType == null ? this.defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Object executor; if (ExecutorType.BATCH == executorType) { executor = new BatchExecutor(this, transaction); } else if (ExecutorType.REUSE == executorType) { executor = new ReuseExecutor(this, transaction); } else { executor = new SimpleExecutor(this, transaction); } if (this.cacheEnabled) { executor = new CachingExecutor((Executor)executor);//装饰前面的三个executor } Executor executor = (Executor)this.interceptorChain.pluginAll(executor); return executor; } } 可以看出如果开启二级缓存，cachingExecutor会被作为装饰类，为什么说是装饰类呢 public class CachingExecutor implements Executor { private final Executor delegate; private final TransactionalCacheManager tcm = new TransactionalCacheManager(); public CachingExecutor(Executor delegate) { this.delegate = delegate; delegate.setExecutorWrapper(this); } public Transaction getTransaction() { return this.delegate.getTransaction(); } public void close(boolean forceRollback) { try { if (forceRollback) { this.tcm.rollback(); } else { this.tcm.commit(); } } finally { this.delegate.close(forceRollback); } } public boolean isClosed() { return this.delegate.isClosed(); } public int update(MappedStatement ms, Object parameterObject) throws SQLException { this.flushCacheIfRequired(ms);//刷新二级缓存 return this.delegate.update(ms, parameterObject); } public List query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException { BoundSql boundSql = ms.getBoundSql(parameterObject); CacheKey key = this.createCacheKey(ms, parameterObject, rowBounds, boundSql); return this.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); } public List query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { Cache cache = ms.getCache(); if (cache != null) { this.flushCacheIfRequired(ms); if (ms.isUseCache() && resultHandler == null) { this.ensureNoOutParams(ms, parameterObject, boundSql); List list = (List)this.tcm.getObject(cache, key);二级缓存中存在则在二级缓存中读取 if (list == null) { //若是二级缓存没有，则到一级缓存中查询 list = this.delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); this.tcm.putObject(cache, key, list);//保存到二级缓存中 } return list;//返回 } } return this.delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); } public List flushStatements() throws SQLException { return this.delegate.flushStatements(); } public void commit(boolean required) throws SQLException { this.delegate.commit(required); this.tcm.commit(); } public void rollback(boolean required) throws SQLException { try { this.delegate.rollback(required); } finally { if (required) { this.tcm.rollback(); } } } } 可以从commit等事务可以看出来是由传入的excutor（delegate）实现的，并且与TransactionalCacheManager tcm辅助完成功能，只有query自己实现了功能，即二级缓存，从中可以看出他是先读取二级缓存有无内容，没有再到一级缓存中查看，没有再去数据库查询，再保存到一级缓存（若commit或），最后返还给用户。 因为一个二级缓存对应多个一级缓存，所以在其中一个sqlsession需要刷新导致二级缓存刷新时，其他sqlsession的与该sqlsession无关联的数据可以在一级缓存中获取就行，但是如果是相关联的数据，此时又用其他的sqlsession来查找，其他sqlsession的一级缓存数据是否存在过期现象。（不同命名空间？）这就涉及到一级缓存什么时候提交数据到二级缓存中的问题了： 只有会话关闭或提交后，一级缓存中的数据才会转移到二级缓存中,并且只有是同一个namespace所以可以获取到数据；因为mybatis的缓存会被一个transactioncache类包装住，所有的cache.putObject全部都会被暂时存到一个map里，等会话commit/close以后，这个map里的缓存对象才会被真正的cache类执行putObject操作。这么设计的原因是为了防止事务执行过程中出异常导致回滚，如果get到object后直接put进缓存，万一发生回滚，就很容易导致mybatis缓存被脏读 这就保证了select就不会到达同的其他的sqlsession的cache中了，若是update成功，则commit成功，数据上传到二级缓存中，对该sqlsession的数据访问会直接在二级内存读取，若是失败，则不会commit，则不会上传到二级缓存中，数据因为失败仍应是原来的数据才是对的，二级缓存数据没有收到commit或close后传来的数据而不变，继续等待相应的sqlsession 进行commit/close UserMapper有一个二级缓存区域（按namespace分，如果namespace相同则使用同一个相同的二级缓存区），其它mapper也有自己的二级缓存区域（按namespace分） 每一个namespace的mapper都有一个二缓存区域，两个mapper的namespace如果相同，这两个mapper执行sql查询到数据将存在相同的二级缓存区域中。 mybatis二级缓存的局限性： 细粒度缓存就是，针对某个商品，如果要修改其信息，只修改该商品的缓存数据，缓存区的其他数据不动（不会因为某个商品信息的修改就直接清空整个缓存区） mybatis二级缓存对细粒度级别的缓存实现不好，比如如下需求： 对商品信息进行缓存， 由于商品信息查询访问量大，但是要求用户每次都能查询到最新的商品信息，此时如果使用mybatis的二级缓存，就无法实现当一个商品信息变化时，只刷新该商品的缓存信息，而不刷新其它商品的信息。这是因为mybatis的二级缓存区域以mapper为单位进行划分，当一个商品信息变化时（使用这个mapper的任意一个sqlSession执行commt操作），会将所有商品信息的缓存数据全部清空。解决此问题，需要在业务层根据需求对数据进行针对性的缓存（三级缓存）。 注解开发 Mybatis 中的注解开发 在 Mybatis 的注解开发中，常用的注解如下表所示： 注解 作用 @Intsert 实现新增 @Update 实现更新 @Delete 实现删除 @Select 实现查询 @Results 实现结果集封装 @ResultMap 实现引用 @Results 定义的封装 @One 实现一对一结果集封装 @Many 实现一对多结果集封装 @SelectProvider 实现动态 SQL 映射 Mybatis 使用注解实现单表 CURD 用户实体类接口层 UserMapper 代码如下 public interface UserMapper { /** * 查询所有用户 * * @return */ @Select(\"SELECT * FROM user\") List listAllUsers(); /** * 添加用户 * * @param user * @return 成功返回1，失败返回0 */ @Insert(\"INSERT INTO user(username,birthday,sex,address) VALUES(#{username},#{birthday},#{sex},#{address})\") @SelectKey(keyProperty = \"id\", keyColumn = \"id\", statement = \"SELECT LAST_INSERT_ID()\", resultType = Integer.class, before = false) int saveUser(User user); /** * 根据id删除用户 * * @param userId * @return 成功返回1，失败返回0 */ @Delete(\"DELETE FROM user WHERE id = #{id}\") int removeUserById(Integer userId); /** * 修改用户 * * @param user * @return 成功返回1，失败返回0 */ @Update(\"UPDATE user SET username = #{username}, birthday = #{birthday}, sex = #{sex}, address = #{address} WHERE id = #{id}\") int updateUser(User user); /** * 根据id查询单个用户 * * @param userId * @return */ @Select(\"SELECT * FROM user WHERE id = #{id}\") User getUserById(Integer userId); /** * 根据姓名模糊查询多个用户 * * @param username * @return */ @Select(\"SELECT * FROM user WHERE username LIKE CONCAT('%',#{username},'%')\") List listUsersByName(String username); /** * 查询用户总数 * * @return */ @Select(\"SELECT COUNT(id) FROM user\") int countUser(); } 注意，如果此时实体类的属性与数据库表列名不一致，那么我们应该使用@Results、@Result、@ResultMap 等注解，如下 public interface UserMapper { /** * 查询所有用户 * * @return */ @Select(\"SELECT * FROM user\") @Results(id = \"UserMap\",value = { @Result(id = true,property = \"userId\",column = \"id\"), @Result(property = \"userName\",column = \"username\"), @Result(property = \"userBirthday\",column = \"birthday\"), @Result(property = \"userSex\",column = \"sex\"), @Result(property = \"userAddress\",column = \"address\"), }) List listAllUsers(); /** * 添加用户 * * @param user * @return 成功返回1，失败返回0 */ @Insert(\"INSERT INTO user(username,birthday,sex,address) VALUES(#{username},#{birthday},#{sex},#{address})\") @ResultMap(\"UserMap\") int saveUser(User user); @Results 注解用于定义映射结果集，相当于标签 。 其中，id 属性为唯一标识。 value 属性用于接收 @Result[] 注解类型的数组。 @Result 注解用于定义映射关系，相当于标签 和 其中，id 属性指定主键。 property 属性指定实体类的属性名，column 属性指定数据库表中对应的列。 @ResultMap 注解用于引用 @Results 定义的映射结果集，避免了重复定义映射结果集。 Mybatis 使用注解实现多对一（一对一） 还是以上一篇笔记的用户和账户为例子。一个用户可以有多个账户，而一个账户只能对应一个用户。 在 Mybatis 中，多对一是作为一对一来进行处理的。也就是说，虽然多个账户可以属于同一个用户（多对一），但是在实体类中，我们是在账户类中添加一个用户类的对象引用，以此来表明所属用户。(此时就相当于一对一) 账户实体类和用户实体类见上篇笔记，接口层代码如下 public interface AccountMapper { /** * 查询所有账户，并查询所属用户，采用立即加载 * * @return */ @Select(\"SELECT * FROM account\") @Results(id = \"AccountMap\", value = { @Result(id = true, property = \"id\", column = \"id\"), @Result(property = \"uid\", column = \"uid\"), @Result(property = \"user\", column = \"uid\", one = @One(select = \"cn.ykf.mapper.UserMapper.getUserById\", fetchType = FetchType.EAGER)) }) List listAllAccounts(); } public interface UserMapper { /** * 根据id查询单个用户 * * @param userId * @return */ @Select(\"SELECT * FROM user WHERE id = #{id}\") User getUserById(Integer userId); } @One 注解相当于标签 ，是多表查询的关键，在注解中用来指定子查询返回单一对象。 其中，select 属性指定用于查询的接口方法，fetchType 属性用于指定立即加载或延迟加载，分别对应 FetchType.EAGER 和 FetchType.LAZY 在包含 @one 注解的 @Result 中，column 属性用于指定将要作为参数进行查询的数据库表列。 Mybatis 使用注解实现一对多 为用户实体类添加包含账户实体类的集合引用，具体代码见上篇笔记。接口层代码如下 public interface UserMapper { /** * 查询所有用户，并且查询拥有账户，采用延迟加载 * * @return */ @Select(\"SELECT * FROM user\") @Results(id = \"UserMap\", value = { @Result(id = true, property = \"userId\", column = \"id\"), @Result(property = \"userName\", column = \"username\"), @Result(property = \"userBirthday\", column = \"birthday\"), @Result(property = \"userSex\", column = \"sex\"), @Result(property = \"userAddress\", column = \"address\"), @Result(property = \"accounts\", column = \"id\", many = @Many(select = \"cn.ykf.mapper.AccountMapper.getAccountByUid\", fetchType = FetchType.LAZY)) }) List listAllUsers(); public interface AccountMapper { /** * 根据用户id查询账户列表 * * @param uid * @return */ @Select(\"SELECT * FROM account WHERE uid = #{uid}\") List listAccountsByUid(Integer uid); } @Many 注解相当于标签 ，是多表查询的关键，在注解中用来指定子查询返回对象集合。 其中，select 属性指定用于查询的接口方法，fetchType 属性用于指定立即加载或延迟加载，分别对应 FetchType.EAGER 和 FetchType.LAZY 在包含 @Many 注解的 @Result 中，column 属性用于指定将要作为参数进行查询的数据库表列。 Mybatis 使用注解实现二级缓存 如果使用注解时想开启二级缓存，那么首先应该在 Mybatis 配置文件中开启全局配置 Mybatis 使用注解实现二级缓存 如果使用注解时想开启二级缓存，那么首先应该在 Mybatis 配置文件中开启全局配置 @CacheNamespace(blocking = true) public interface UserMapper { // ..... } Mybatis分页功能 https://blog.csdn.net/chenbaige/article/details/70846902 一.借助数组进行分页（Service层的框架实现） 原理：进行数据库查询操作时，获取到数据库中所有满足条件的记录，保存在应用的临时数组中，再通过List的subList方法，获取到满足条件的所有记录。 缺点：数据库查询并返回所有的数据，而我们需要的只是极少数符合要求的数据。当数据量少时，还可以接受。当数据库数据量过大时，每次查询对数据库和程序的性能都会产生极大的影响。 二.借助Sql语句进行分页(Dao层的sql limit实现) 实现：通过sql语句实现分页也是非常简单的，只是需要改变我们查询的语句就能实现了，即在sql语句后面添加limit分页语句。 在xxxMapper.xml文件中编写sql语句通过limit关键字进行分页： select * from student limit #{currIndex} , #{pageSize} 三.拦截器分页(PageHelper原理也是如此，其中startPage只是借助localThread先保存分页Page信息，并在Intercept方法中辅以dialect对象进行检测与执行相关操作，sqlsession下拦截器的limit实现) PageHelepr的Intecepter在dao层下面的sqlsession.selectlist下面对statemnehandler等拦截对象进行拦截生成代理对象的，可以说是对dao层（sql语句execute前）的拦截增强 自定义拦截器实现了拦截所有StatementHandler的prepare方法处理的xml中id以ByPage结尾的语句，并且利用获取到的分页相关参数统一在sql语句后面加上limit分页的相关语句 如何使用Interceptor呢？ 实例化后的Interceptor会在ApplicationContext初始化时读取实现了Inteceptor接口的实例按实例顺序自动添加到责任链中 Interceptor会在何时起作用？ 我们需要先通过@Interceptor判断我们的Interceptor是对于在sql的Executor、StatementHandler、ParameterHandler、ResultSetHandler四个处理阶段中进行拦截的与拦截了四个类中的什么方法，以便更好的理解Interceptor在哪一步将Page信息往下传递。（因为在创建sqlSession 的时候就会同步创建Executor，并在query执行sql语句时，新建statementHandler并同时创建ParameterHandle和ResultSetHandler，所以Interceptor可以随心在哪个阶段进行拦截并顺序处理，也因此是分页插件可以实现） https://www.cnblogs.com/tanghaorong/p/14094521.html 分页需要在Dao层进行哪些标记使得Inteceptor能判断哪些方法需要分页呢？ 不同的Interceptor有不同的拦截逻辑，会使用不同的逻辑判断动态增强DAO层， 在拦截到标记的拦截类的拦截方法时，在intercept方法中对Dao方法封装成的statement进行相应条件的判断 如PageHelper会在执行Dao层时判断ThreadLocal上的查看有没有Page 的属性存储，中间会进行判断该sql 的类型是查询，还是修改操作。若是查询有则动态增强，分页处理 PaginationInterceptor则会判断Dao层的第一个参数中有没有实现IPage接口的如Page类，也会判断是否为查询操作，若是且有则读取Page类中存储的属性，动态增强，进行分页处理。 package com.cbg.interceptor; import org.apache.ibatis.executor.Executor; import org.apache.ibatis.executor.parameter.ParameterHandler; import org.apache.ibatis.executor.resultset.ResultSetHandler; import org.apache.ibatis.executor.statement.StatementHandler; import org.apache.ibatis.mapping.MappedStatement; import org.apache.ibatis.plugin.*; import org.apache.ibatis.reflection.MetaObject; import org.apache.ibatis.reflection.SystemMetaObject; import java.sql.Connection; import java.util.Map; import java.util.Properties; /** * Created by chenboge on 2017/5/7. * * Email:baigegechen@gmail.com * * description: */ /** * @Intercepts 说明是一个拦截器 * @Signature 拦截器的签名 * type 拦截的类型 四大对象之一( Executor,ResultSetHandler,ParameterHandler,StatementHandler) * method 拦截的方法 * args 参数 */ @Intercepts({@Signature(type = StatementHandler.class, method = \"prepare\", args = {Connection.class, Integer.class})}) public class MyPageInterceptor implements Interceptor { //每页显示的条目数 private int pageSize; //当前现实的页数 private int currPage; private String dbType; @Override public Object intercept(Invocation invocation) throws Throwable { //获取StatementHandler，默认是RoutingStatementHandler StatementHandler statementHandler = (StatementHandler) invocation.getTarget(); //获取statementHandler包装类 MetaObject MetaObjectHandler = SystemMetaObject.forObject(statementHandler); //分离代理对象链 while (MetaObjectHandler.hasGetter(\"h\")) { Object obj = MetaObjectHandler.getValue(\"h\"); MetaObjectHandler = SystemMetaObject.forObject(obj); } while (MetaObjectHandler.hasGetter(\"target\")) { Object obj = MetaObjectHandler.getValue(\"target\"); MetaObjectHandler = SystemMetaObject.forObject(obj); } //获取连接对象 //Connection connection = (Connection) invocation.getArgs()[0]; //object.getValue(\"delegate\"); 获取StatementHandler的实现类 //获取查询接口映射的相关信息 MappedStatement mappedStatement = (MappedStatement) MetaObjectHandler.getValue(\"delegate.mappedStatement\"); String mapId = mappedStatement.getId(); //statementHandler.getBoundSql().getParameterObject(); //拦截以.ByPage结尾的请求，分页功能的统一实现 if (mapId.matches(\".+ByPage$\")) { //获取进行数据库操作时管理参数的handler ParameterHandler parameterHandler = (ParameterHandler) MetaObjectHandler.getValue(\"delegate.parameterHandler\"); //获取请求时的参数 Map paraObject = (Map) parameterHandler.getParameterObject(); //也可以这样获取 //paraObject = (Map) statementHandler.getBoundSql().getParameterObject(); //参数名称和在service中设置到map中的名称一致 currPage = (int) paraObject.get(\"currPage\"); pageSize = (int) paraObject.get(\"pageSize\"); String sql = (String) MetaObjectHandler.getValue(\"delegate.boundSql.sql\"); //也可以通过statementHandler直接获取 //sql = statementHandler.getBoundSql().getSql(); //构建分页功能的sql语句 String limitSql; sql = sql.trim(); limitSql = sql + \" limit \" + (currPage - 1) * pageSize + \",\" + pageSize; //将构建完成的分页sql语句赋值个体'delegate.boundSql.sql'，偷天换日 MetaObjectHandler.setValue(\"delegate.boundSql.sql\", limitSql); } //调用原对象的方法，进入责任链的下一级 return invocation.proceed(); } //获取代理对象 @Override public Object plugin(Object o) { //生成object对象的动态代理对象 return Plugin.wrap(o, this); } //设置代理对象的参数 @Override public void setProperties(Properties properties) { //如果项目中分页的pageSize是统一的，也可以在这里统一配置和获取，这样就不用每次请求都传递pageSize参数了。参数是在配置拦截器时配置的。 String limit1 = properties.getProperty(\"limit\", \"10\"); this.pageSize = Integer.valueOf(limit1); this.dbType = properties.getProperty(\"dbType\", \"mysql\"); } } 编写好拦截器后，需要注册到项目中，才能发挥它的作用。在mybatis的配置文件中，添加如下代码： 若是自定义的interceptor，则需要用setProperties方法处理我们property标签的参数，PageHelper等已经设置好相应的properties，我们只需在property中写入相应的name和value即可 //读取配置的代理对象的参数 @Override public void setProperties(Properties properties) { String limit1 = properties.getProperty(\"limit\", \"10\"); this.pageSize = Integer.valueOf(limit1); this.dbType = properties.getProperty(\"dbType\", \"mysql\"); } 四.RowBounds实现分页（sqlsession的selectList方法重载后在框架中分页，sqlsession的框架实现） 原理：通过RowBounds实现分页和通过数组方式分页原理差不多，都是一次获取所有符合条件的数据，然后在内存中对大数据进行操作，实现分页效果。只是数组分页需要我们自己去实现分页逻辑，这里更加简化而已。 存在问题：一次性从数据库获取的数据可能会很多，对内存的消耗很大，可能导师性能变差，甚至引发内存溢出。 实现：在Dao层中需要分页的方法的SelectList方法依据Page等Service层传递的实参，转换成RowBounds对象或者DAO接口写成List xxx(RowBounds page)，这种情况下，mybatis也会完成分页查询。(但是Dao接口的形参为Page等自定义类型，却没有实现将Page转换成RowBounds交给selectList方法，则不会实现分页功能) spring与MyBatis 事务管理 https://coderbee.net/index.php/framework/20191025/2002 1. 运行环境 Enviroment 当 MyBatis 与不同的应用结合时，需要不同的事务管理机制。与 Spring 结合时，由 Spring 来管理事务；单独使用时需要自行管理事务，在容器里运行时可能由容器进行管理。 MyBatis 用 Enviroment 来表示运行环境，其封装了三个属性： public class Configuration { // 一个 MyBatis 的配置只对应一个环境 protected Environment environment; // 其他属性 ..... } public final class Environment { private final String id; private final TransactionFactory transactionFactory; private final DataSource dataSource; } 2. 事务抽象 MyBatis 把事务管理抽象出 Transaction 接口，由 TransactionFactory 接口的实现类负责创建。 public interface Transaction { Connection getConnection() throws SQLException; void commit() throws SQLException; void rollback() throws SQLException; void close() throws SQLException; Integer getTimeout() throws SQLException; } public interface TransactionFactory { void setProperties(Properties props); Transaction newTransaction(Connection conn); Transaction newTransaction(DataSource dataSource, TransactionIsolationLevel level, boolean autoCommit); } Executor 的实现持有一个 SqlSession 实现，事务控制是委托给 SqlSession 的方法来实现的。 public abstract class BaseExecutor implements Executor { protected Transaction transaction; public void commit(boolean required) throws SQLException { if (closed) { throw new ExecutorException(\"Cannot commit, transaction is already closed\"); } clearLocalCache(); flushStatements(); if (required) { transaction.commit(); } } public void rollback(boolean required) throws SQLException { if (!closed) { try { clearLocalCache(); flushStatements(true); } finally { if (required) { transaction.rollback(); } } } } // 省略其他方法、属性 } 3. 与 Spring 集成的事务管理 3.1 配置 TransactionFactory 与 Spring 集成时，通过 SqlSessionFactoryBean 来初始化 MyBatis 。 protected SqlSessionFactory buildSqlSessionFactory() throws IOException { Configuration configuration; XMLConfigBuilder xmlConfigBuilder = null; if (this.configuration != null) { configuration = this.configuration; if (configuration.getVariables() == null) { configuration.setVariables(this.configurationProperties); } else if (this.configurationProperties != null) { configuration.getVariables().putAll(this.configurationProperties); } } else if (this.configLocation != null) { xmlConfigBuilder = new XMLConfigBuilder(this.configLocation.getInputStream(), null, this.configurationProperties); configuration = xmlConfigBuilder.getConfiguration(); } else { configuration = new Configuration(); configuration.setVariables(this.configurationProperties); } if (this.objectFactory != null) { configuration.setObjectFactory(this.objectFactory); } if (this.objectWrapperFactory != null) { configuration.setObjectWrapperFactory(this.objectWrapperFactory); } if (this.vfs != null) { configuration.setVfsImpl(this.vfs); } if (hasLength(this.typeAliasesPackage)) { String[] typeAliasPackageArray = tokenizeToStringArray(this.typeAliasesPackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS); for (String packageToScan : typeAliasPackageArray) { configuration.getTypeAliasRegistry().registerAliases(packageToScan, typeAliasesSuperType == null ? Object.class : typeAliasesSuperType); } } if (!isEmpty(this.typeAliases)) { for (Class typeAlias : this.typeAliases) { configuration.getTypeAliasRegistry().registerAlias(typeAlias); } } if (!isEmpty(this.plugins)) { for (Interceptor plugin : this.plugins) { configuration.addInterceptor(plugin); } } if (hasLength(this.typeHandlersPackage)) { String[] typeHandlersPackageArray = tokenizeToStringArray(this.typeHandlersPackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS); for (String packageToScan : typeHandlersPackageArray) { configuration.getTypeHandlerRegistry().register(packageToScan); } } if (!isEmpty(this.typeHandlers)) { for (TypeHandler typeHandler : this.typeHandlers) { configuration.getTypeHandlerRegistry().register(typeHandler); } } if (this.databaseIdProvider != null) {//fix #64 set databaseId before parse mapper xmls try { configuration.setDatabaseId(this.databaseIdProvider.getDatabaseId(this.dataSource)); } catch (SQLException e) { throw new NestedIOException(\"Failed getting a databaseId\", e); } } if (this.cache != null) { configuration.addCache(this.cache); } if (xmlConfigBuilder != null) { try { xmlConfigBuilder.parse(); } catch (Exception ex) { throw new NestedIOException(\"Failed to parse config resource: \" + this.configLocation, ex); } finally { ErrorContext.instance().reset(); } } // 创建 SpringManagedTransactionFactory if (this.transactionFactory == null) { this.transactionFactory = new SpringManagedTransactionFactory(); } // 封装成 Environment configuration.setEnvironment(new Environment(this.environment, this.transactionFactory, this.dataSource)); if (!isEmpty(this.mapperLocations)) { for (Resource mapperLocation : this.mapperLocations) { if (mapperLocation == null) { continue; } try { XMLMapperBuilder xmlMapperBuilder = new XMLMapperBuilder(mapperLocation.getInputStream(), configuration, mapperLocation.toString(), configuration.getSqlFragments()); xmlMapperBuilder.parse(); } catch (Exception e) { throw new NestedIOException(\"Failed to parse mapping resource: '\" + mapperLocation + \"'\", e); } finally { ErrorContext.instance().reset(); } } } else { } return this.sqlSessionFactoryBuilder.build(configuration); } public class SqlSessionFactoryBuilder { public SqlSessionFactory build(Configuration config) { return new DefaultSqlSessionFactory(config); } } 重点是在构建 MyBatis Configuration 对象时，把 transactionFactory 配置成 SpringManagedTransactionFactory，再封装成 Environment 对象。 3.2 运行时事务管理 Mapper 的代理对象持有的是 SqlSessionTemplate，其实现了 SqlSession 接口。 SqlSessionTemplate 的方法并不直接调用具体的 SqlSession 的方法，而是委托给一个动态代理，通过代理 SqlSessionInterceptor 对方法调用进行拦截。 SqlSessionInterceptor 负责获取真实的与数据库关联的 SqlSession 实现，并在方法执行完后决定提交或回滚事务、关闭会话。 public class SqlSessionTemplate implements SqlSession, DisposableBean { private final SqlSessionFactory sqlSessionFactory; private final ExecutorType executorType; private final SqlSession sqlSessionProxy; private final PersistenceExceptionTranslator exceptionTranslator; public SqlSessionTemplate(SqlSessionFactory sqlSessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) { notNull(sqlSessionFactory, \"Property 'sqlSessionFactory' is required\"); notNull(executorType, \"Property 'executorType' is required\"); this.sqlSessionFactory = sqlSessionFactory; this.executorType = executorType; this.exceptionTranslator = exceptionTranslator; // 因为 SqlSession 接口声明的方法也不少， // 在每个方法里添加事务相关的拦截比较麻烦， // 不如创建一个内部的代理对象进行统一处理。 this.sqlSessionProxy = (SqlSession) newProxyInstance( SqlSessionFactory.class.getClassLoader(), new Class[] { SqlSession.class }, new SqlSessionInterceptor()); } public int update(String statement) { // 在代理对象上执行方法调用 return this.sqlSessionProxy.update(statement); } // 对方法调用进行拦截，加入事务控制逻辑 private class SqlSessionInterceptor implements InvocationHandler { public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { // 获取与数据库关联的会话 SqlSession sqlSession = SqlSessionUtils.getSqlSession( SqlSessionTemplate.this.sqlSessionFactory, SqlSessionTemplate.this.executorType, SqlSessionTemplate.this.exceptionTranslator); try { // 执行 SQL 操作 Object result = method.invoke(sqlSession, args); if (!SqlSessionUtils.isSqlSessionTransactional(sqlSession, SqlSessionTemplate.this.sqlSessionFactory)) { // 如果 sqlSession 不是 Spring 管理的，则要自行提交事务 sqlSession.commit(true); } return result; } catch (Throwable t) { Throwable unwrapped = unwrapThrowable(t); if (SqlSessionTemplate.this.exceptionTranslator != null && unwrapped instanceof PersistenceException) { SqlSessionUtils.closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); sqlSession = null; Throwable translated = SqlSessionTemplate.this.exceptionTranslator.translateExceptionIfPossible((PersistenceException) unwrapped); if (translated != null) { unwrapped = translated; } } throw unwrapped; } finally { if (sqlSession != null) { SqlSessionUtils.closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); } } } } } SqlSessionUtils 封装了对 Spring 事务管理机制的访问。 // SqlSessionUtils public static SqlSession getSqlSession(SqlSessionFactory sessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) { // 从 Spring 的事务管理机制那里获取当前事务关联的会话 SqlSessionHolder holder = (SqlSessionHolder) TransactionSynchronizationManager.getResource(sessionFactory); SqlSession session = sessionHolder(executorType, holder); if (session != null) { // 已经有一个会话则复用 return session; } // 创建新的 会话 session = sessionFactory.openSession(executorType); // 注册到 Spring 的事务管理机制里 registerSessionHolder(sessionFactory, executorType, exceptionTranslator, session); return session; } private static void registerSessionHolder(SqlSessionFactory sessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator, SqlSession session) { SqlSessionHolder holder; if (TransactionSynchronizationManager.isSynchronizationActive()) { Environment environment = sessionFactory.getConfiguration().getEnvironment(); if (environment.getTransactionFactory() instanceof SpringManagedTransactionFactory) { holder = new SqlSessionHolder(session, executorType, exceptionTranslator); TransactionSynchronizationManager.bindResource(sessionFactory, holder); // 重点：注册会话管理的回调钩子，真正的关闭动作是在回调里完成的。 TransactionSynchronizationManager.registerSynchronization(new SqlSessionSynchronization(holder, sessionFactory)); holder.setSynchronizedWithTransaction(true); // 维护会话的引用计数 holder.requested(); } else { if (TransactionSynchronizationManager.getResource(environment.getDataSource()) == null) { } else { throw new TransientDataAccessResourceException( \"SqlSessionFactory must be using a SpringManagedTransactionFactory in order to use Spring transaction synchronization\"); } } } else { } } public static void closeSqlSession(SqlSession session, SqlSessionFactory sessionFactory) { // 从线程本地变量里获取 Spring 管理的会话 SqlSessionHolder holder = (SqlSessionHolder) TransactionSynchronizationManager.getResource(sessionFactory); if ((holder != null) && (holder.getSqlSession() == session)) { // Spring 管理的不直接关闭，由回调钩子来关闭 holder.released(); } else { // 非 Spring 管理的直接关闭 session.close(); } } SqlSessionSynchronization 是 SqlSessionUtils 的内部私有类，用于作为回调钩子与 Spring 的事务管理机制协调工作，TransactionSynchronizationManager 在适当的时候回调其方法。 private static final class SqlSessionSynchronization extends TransactionSynchronizationAdapter { private final SqlSessionHolder holder; private final SqlSessionFactory sessionFactory; private boolean holderActive = true; public SqlSessionSynchronization(SqlSessionHolder holder, SqlSessionFactory sessionFactory) { this.holder = holder; this.sessionFactory = sessionFactory; } public int getOrder() { return DataSourceUtils.CONNECTION_SYNCHRONIZATION_ORDER - 1; } public void suspend() { if (this.holderActive) { TransactionSynchronizationManager.unbindResource(this.sessionFactory); } } public void resume() { if (this.holderActive) { TransactionSynchronizationManager.bindResource(this.sessionFactory, this.holder); } } public void beforeCommit(boolean readOnly) { if (TransactionSynchronizationManager.isActualTransactionActive()) { try { this.holder.getSqlSession().commit(); } catch (PersistenceException p) { if (this.holder.getPersistenceExceptionTranslator() != null) { DataAccessException translated = this.holder .getPersistenceExceptionTranslator() .translateExceptionIfPossible(p); if (translated != null) { throw translated; } } throw p; } } } public void beforeCompletion() { if (!this.holder.isOpen()) { TransactionSynchronizationManager.unbindResource(sessionFactory); this.holderActive = false; // 真正关闭数据库会话 this.holder.getSqlSession().close(); } } public void afterCompletion(int status) { if (this.holderActive) { TransactionSynchronizationManager.unbindResourceIfPossible(sessionFactory); this.holderActive = false; // 真正关闭数据库会话 this.holder.getSqlSession().close(); } this.holder.reset(); } } 3.3 创建新会话 // DefaultSqlSessionFactory private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) { Transaction tx = null; try { final Environment environment = configuration.getEnvironment(); // 获取事务工厂实现 final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); } catch (Exception e) { closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(\"Error opening session. Cause: \" + e, e); } finally { ErrorContext.instance().reset(); } } private TransactionFactory getTransactionFactoryFromEnvironment(Environment environment) { if (environment == null || environment.getTransactionFactory() == null) { return new ManagedTransactionFactory(); } return environment.getTransactionFactory(); } 4. 小结 MyBatis 的核心组件 Executor 通过 Transaction 接口来进行事务控制。 与 Spring 集成时，初始化 Configuration 时会把 transactionFactory 设置为 SpringManagedTransactionFactory 的实例。 每个 Mapper 代理里注入的 SqlSession 是 SqlSessionTemplate 的实例，其实现了 SqlSession 接口； SqlSessionTemplate 把对 SqlSession 接口里声明的方法调用委托给内部的一个动态代理，该代理的方法处理器为内部类 SqlSessionInterceptor 。 SqlSessionInterceptor 接收到方法调用时，通过 SqlSessionUtil 访问 Spring 的事务设施，如果有与 Spring 当前事务关联的 SqlSession 则复用；没有则创建一个。 SqlSessionInterceptor 根据 Spring 当前事务的状态来决定是否提交或回滚事务。会话的真正关闭是通过注册在 TransactionSynchronizationManager 上的回调钩子实现的。 如上图： 步骤 1 是 Spring AOP 添加的切面的执行，事务是其中一个切面。 步骤 2 是 Spring 事务管理机制的部分。 步骤 3 是业务代码，调用 Mapper 代理上的方法。 步骤 4 是代理上的方法调用被 MapperProxy.invoke 拦截。 步骤 5、6 是因为 MapperProxy 持有的 sqlSession 是 SqlSessionTemplate，调用到 template 上的方法，又被转发给内部类 SqlSessionInterceptor ，该类获得 Spring 事务管理持有的数据库连接，用以创建 Executor h和 DefaultSqlSession。 步骤 7 用 DefaultSqlSession 发起数据库调用。 Spring 与 MyBatis 简要概述 在配置文件中对service层的AOP实现，使得SpringMVC等servlet调用容器中的Service对象时，会用TransactionInterceptor对其进行动态处理，为其调用DataSourceTransactionManager为其创建transaction并将connection绑定到ThreadLocal（关键字为datasource，之后用到该datasource的就用该connectionholder，就可以用来判断是否存在事务Spring的事务控制）中 （service对象一经从容器中获取就执行，使得commit等交易transaction管理，sqlsession保存在线程中，可重复使用，否则sqlsession是由每个单独的MapperFactoryBean调用的，每个执行完便提交，再次调用会获取不同的connection，一级缓存失效；就算你不自动提交（没开启事务一般是自动commit），不开启事务也不能保证同一个sqlsession调用的是同一个connection，可能出现更大的问题如脏读等，若是绑定同一个则又是事务的原理了） 对service中的方法进行调用时，就需要调用到dao层的方法与mysql进行交互，而dao层的经过MapperScannerConfigurer的自动扫描以及将方法封装成各个MapperFactoryBean MapperFactoryBean创建时若没有指定特定的注入对象，在初始化时Spring会自动注入相应的sqlsessionTemplate等sqlsession对象（实现了sqlsession接口，但都不是实际负责sql语句执行的） 若是实现了事务管理，则会在TransactionSynchronizationManager寻找真正执行sql语句的且在同一事务中的DefaultSqlsession这类sqlsession真正的实现类，通过SpringManageTransaction调用datasourceUtils，从TransactionSynchronizationManager获取唯一的connection去执行相应的sql语句。 若是声明式事务，则在service方法执行完后自动commit；若编程式则从status获取transaction进行commit //扫描的包 //数据源 //创建特定的SqlSessionFactory // 查 找 类 路 径 下 的 映 射 器 并自 动 将 它 们 创 建 成 MapperFactoryBean //事务管理 MapperScannerConfigurer与MapperFactoryBean MapperScannerConfigurer查 找 类 路 径 下 的 映 射 器 并自 动 将 它 们 创 建 成 MapperFactoryBean basePackage 属性是让你为映射器接口文件设置基本的包路径。 你可以使用分号或逗号 作为分隔符设置多于一个的包路径。每个映射器将会在指定的包路径中递归地被搜索到。 如果不是有特殊需求，不需要 去 指 定 SqlSessionFactory 或 SqlSessionTemplate , 因 为 MapperScannerConfigurer 将会创建 MapperFactoryBean,之后自动装配。但是,如果你使 用了一个 以上的 DataSource ,那 么自动 装配可 能会失效 。这种 情况下 ,你可 以使用 sqlSessionFactoryBeanName 或 sqlSessionTemplateBeanName 属性来设置正确的 bean 名 称来使用。 public class MapperScannerConfigurer implements BeanDefinitionRegistryPostProcessor, InitializingBean, ApplicationContextAware, BeanNameAware { //接口包 private String basePackage; private boolean addToConfig = true; private SqlSessionFactory sqlSessionFactory; private SqlSessionTemplate sqlSessionTemplate; private String sqlSessionFactoryBeanName; private String sqlSessionTemplateBeanName; private Class annotationClass; private Class markerInterface; private ApplicationContext applicationContext; private String beanName; private boolean processPropertyPlaceHolders; private BeanNameGenerator nameGenerator; ··· get/set方法用以注入 ··· } private void processPropertyPlaceHolders() { Map prcs = this.applicationContext.getBeansOfType(PropertyResourceConfigurer.class); if (!prcs.isEmpty() && this.applicationContext instanceof GenericApplicationContext) { BeanDefinition mapperScannerBean = ((GenericApplicationContext)this.applicationContext).getBeanFactory().getBeanDefinition(this.beanName); DefaultListableBeanFactory factory = new DefaultListableBeanFactory(); factory.registerBeanDefinition(this.beanName, mapperScannerBean); Iterator var4 = prcs.values().iterator(); while(var4.hasNext()) { PropertyResourceConfigurer prc = (PropertyResourceConfigurer)var4.next(); prc.postProcessBeanFactory(factory); } PropertyValues values = mapperScannerBean.getPropertyValues(); this.basePackage = this.updatePropertyValue(\"basePackage\", values); this.sqlSessionFactoryBeanName = this.updatePropertyValue(\"sqlSessionFactoryBeanName\", values); this.sqlSessionTemplateBeanName = this.updatePropertyValue(\"sqlSessionTemplateBeanName\", values); } } MapperFactoryBean 数据映射器接口可以按照如下做法加入到 Spring 中: MapperFactoryBean 创建的代理类实现了 UserMapper 接口,并且注入到应用程序中。 因为代理创建在运行时环境中 ,那么指定的映射器必须是一个接口,而 不是一个具体的实现类。 如果 UserMapper 有一个对应的 MyBatis 的 XML 映射器文件, 如果 XML 文件在类路径的 位置和映射器类相同时, 它会被 MapperFactoryBean 自动解析。 没有必要在 MyBatis 配置文 件 中 去 指 定 映 射 器 , 除 非 映 射 器 的 XML 文 件 在 不 同 的 类 路 径 下 。 当 MapperFactoryBean 需要 SqlSessionFactory 或 SqlSessionTemplate 时。 这些可以通过各自的 SqlSessionFactory 或 SqlSessionTemplate 属性来设置, 或者可以由 Spring 来自动装配。如果两个属性都设置了,那么 SqlSessionFactory 就会被忽略,因为 SqlSessionTemplate 是需要有一个 session 工厂的设置; 那个工厂会由 MapperFactoryBean. 来使用。 由于使用spring管理bean，当我们在代码中需要使用这个bean的时候，会首先去容器中找，第一次需要调用MapperFactoryBean的getObject方法获取一个bean，并保存到容器中。（这里生成的对象最终也是用sqlssion的getMapper生成的，与mybatis一致） MapperFactoryBean的getObject方法如下： public class MapperFactoryBean extends SqlSessionDaoSupport implements FactoryBean { public T getObject() throws Exception { return this.getSqlSession().getMapper(this.mapperInterface); } } 又到SqlSessionDaoSupport中去获取 public abstract class SqlSessionDaoSupport extends DaoSupport { private SqlSession sqlSession; private boolean externalSqlSession; public SqlSession getSqlSession() { return this.sqlSession; } // 在MapperScannerConfigurer自动扫描生成/自己创建MapperFactoryBean时，由传入的工厂类型/Spring自动配置的工厂类型 时，就创建了SqlSessionTemplate（实现了sqlsession接口），因此调用方法时会通过SqlSessionTemplate的proxysqlsession（动态代理生成的代理类）去TransactionSynchronizationManager寻找真正的sqlsession或创建 public void setSqlSessionFactory(SqlSessionFactory sqlSessionFactory) { if (!this.externalSqlSession) { this.sqlSession = new SqlSessionTemplate(sqlSessionFactory); } } } //spring整合mybatis后，非事务环境下，每个执行完便提交，每次操作数据库都使用新的sqlSession对象，获取不同的connection。因此mybatis的一级缓存无法使用，若有则取TransactionSynchronizationManager中的sqlsession则一级缓存生效（一级缓存针对同一个sqlsession有效） TransactionDefinition与TransactionAttributes 获取事务属性对象(TransactionAttributes若无设置attributes则默认为DefaultTransactionDefinition)，放置在容器中，transactionManager调用getTransaction时作为参数传入 事务属性对象持有事务的相关配置，比如事务的隔离级别，传播行为，是否只读等。我们开启spring事务管理时，通常都会在配置文件里加入这样一段配置。 TransactionDefinition主要定义了有哪些事务属性可以指定： 事务的隔离级别（Isolation） 事务的传播行为（Propagation Behavior） 默认为PROPAGATION_REQUIRED ,若存在事务则加入当前事务，若没有则自己新建一个事务。 Propagation Behavior注意PROPAGATION_REQUIRES_NEW和PROPAGATION_NESTED的区别，前者将当前事务挂起，创建新的事务执行；后者，则是在当前事务种的一个嵌套事务中执行 事务的超时时间（Timeout） 是否为只读事务(ReadOnly) SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。 ISOLATION_DEFAULT 数据库的默认级别：mysql为Repeatable Read（可重读），其他的一般为Read Committed（读取提交内容） Read Uncommitted（读取未提交内容） ISOLATION_READ_UNCOMITTED 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。**读取未提交的数据，也被称之为脏读（Dirty Read）。** Read Committed（读取提交内容）ISOLATION_READ_COMITTED ​ 这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），如果一个用户在一个事务中多次读取一条数据，而另外一个用户则同时更新啦这条数据，造成第一个用户多次读取数据不一致。** Repeatable Read（可重读）ISOLATION_REPEATABLE_READ 这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指第一个事务读取一个结果集后，第二个事务，对这个结果集经行增删操作（第一个事务暂时没用到，没添加行锁），然而第一个事务中再次对这个结果集进行查询时，数据发现丢失或新增，出现了“幻影”。通过加表级锁解决，如间隙锁InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。 Serializable（可串行化）ISOLATION_SERIALIZABLE 这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 TransactionAttribute在TransactionDefinition的基础上添加了rollbackon方法，可以通过声明的方式指定业务方法在抛出的哪些的异常的情况下可以回滚事务。（该接口主要面向使用Spring AOP进行声明式事务管理的场合） 编程式的事务管理与声明式事务处理 spring提供编程式的事务管理（自己创建事务管理器，由自己调用管理器的方法创建事务或处理事务，可以使用动态代理减少代理类的编写，但是还是要编写很多事务的代码在业务代码的前后，不是很方便管理，对于事务管理这种特性基本一致的操作用声明反而更利于维护）和声明式事务处理（声明后由容器创建，由声明来调用相应的方法，声明式事务管理用AOP避免了我们为每一个需要事务管理的类创建一个代理类）。 若是声明式事务管理，我们会发现，我们没有自己操作commit的空间，他会在你事务声明的方法结束时就commit，你可以把很多业务代码放在一个事务service方法中实现同一事务，但若是不想则可以编码式事务控制。 声明式事务管理中我们用到org.springframework.transaction.interceptor.TransactionInterceptor帮我们拦截业务方法，使得我们在springMVc中调用serviece中的方法时（我们从容器拿到的Service是经过AOP（也就是动态代理）处理的了），需要先经过它。 需要一个容器去放置用不同ORM框架时，在TransactionInterceptor帮我们拦截后，将service增强成我们想要的模板，加入到像JdbcTempalte/SqlsessionTemplate这样不同的模板中，以便在调用service的方法时，自动去调用相应的数据库框架执行sql语句时需要的流程，如SqlsessionTemplate（它的原理跟sqlsession.getMapper相似）会自动对sqlsession进行创建，再进行该框架对sql语句执行流程，这个容器可以是ProxyFactory（ProxyFactoryBean）或者其他的interceptor的实现类 而sqlsessionTemplate所需要的springMapConfig已经用SqlSessionFactoryBean创建的SqlSessionFactory中了，sqlsessionTemplate会将SqlSessionFactory注入其中，便可以拥有需要的Dao层信息和需要的sql语句等了 在spring 1.x到2.x中我们可以使用4种配置的方式在IoC容器的配置文件种指定事务需要的元数据 使用ProxyFactory（ProxyFactoryBean）+TransactionIntercepter //拦截我们需要的service方法 org.springframework.prospring.ticket.service.PaymentService.transfer=PROPAGATION_REQUIRED,ISOLATION_SERIALIZABLE,timeout_30,-Exception // 将需要的框架Template/相应的dao实现，与Service层接口以及transactionInterceptor集成一个Bean方便Client调用 org.springframework.prospring.ticket.service.PaymentService transactionInterceptor 使用“一站式”的TransactionProxyFactoryBean 使用BeanNameAutoProxyCreater 使用Spring2.x的声明事务配置方式（我们使用的） // 专门为声明事务Advice而设置的配置元素，底层还是TransactionInterceptor 若不指定，则采用DefaultTransactionDefinition // 作用如同ProxyFactoryBean一直，不过使用了aop实现，只需指定ServiceTarget的class，之后的便依靠动态代理实现 //这个实现使得我们在MVC层调用的service对象已经是经过代理实现了的，我们可以直接用其中的方法，底层都会依据流程自己完成，我们会发现，我们没有自己操作commit的空间，他会在你事务声明的方法结束时就commit，你可以把很多业务代码放在一个事务service方法中实现同一事务，但若是不想则可以编码式事务控制。 Spring的事务处理 我们一般通过TransactionTemplate来对PlatformTransactionManager的事务进行封装，使得出现异常需要callback时，将其依据callback接口的实现可以在TransactionTemplate类的excute方法中便处理成功，我们就只需要注意call back接口的设计与实现，就能将PlatformTransactionManager的callback都由该模板实现，减少重复代码的编写。当然小型测试直接用也可以。 Spring的事务处理中，通用的事务处理流程是由抽象事务管理器AbstractPlatformTransactionManager来提供的，而具体的底层事务处理实现，由PlatformTransactionManager的具体实现类来实现，如 DataSourceTransactionManager 、JtaTransactionManager和 HibernateTransactionManager等。（我们通常使用的是DataSourceTransactionManager来与mybatis集成） spring事务处理的一个关键是保证在整个事务的生命周期里所有执行sql的jdbc connection和处理事务的jdbc connection始终是同一个（不然事务通过不同connection commit的时候可能会相互覆盖，顺序也难以确定）。然后执行sql的业务代码一般都分散在程序的不同地方，如何让它们共享一个jdbc connection呢？ 这里spring做了一个前提假设：即一个事务的操作一定是在一个thread中执行，在事务未结束前该事务一直存在于该thread中，且一个thread中如果有多个事务在不同的jdbc connection的话，他们必须顺序执行（在上一个结束的情况下），不能同时存在，此时若是将connection也绑定到线程中，那（这个假设在绝大多数情况下都是成立的，mybatis自身的事务处理中，sqlsession也可以多次执行commit，connection的获取是在sqlsession执行sql时进行的，因此sqlsession也可有多个不同jdbc connection生成的事务，必须顺序执行）。 基于这个假设，spring在transaction创建时，会用ThreadLocal把创建这个事务的jdbc connection绑定到当前thread，接下来在事务的整个生命周期中都会从ThreadLocal中获取同一个jdbc connection（只要没有主动断开connection）。 Spring本身的事务流程（配合JdbcTemplate） 从上面可以看出，Spring事务管理一共可分为三个步骤，分别是初始化事务、提交事务、回滚事务，spring事务工作流相当于为用户屏蔽了具体orm框架的底层处理逻辑，基于spring开发的程序，即便更换了orm框架也是跟换了调用的sqlTemplate，我们的事务管理器更换成适合于他的便可，基本不用改变其他的实现。这是Spring的优点 Spring控制datasourcetransaction ，mybaitis用springManagedTransaction对数据库进行sql操作，但commit等都是最后由datasourceTransaction 进行. SqlSessionFactoryBean 再看这个容器中的参数，无论如何我们都需要SqlSessionFactoryBean（无论是否有用Spring的事务管理都需要）和DataSourceTransactionManager 由SqlSessionFactoryBean创建工厂，此工厂将生成SpringManagedTransactionFactory（可能有人会好奇为什么要大费周章重新实现一个TransactionFactory，到下面进入sqlsession的部分就可以知道答案了），再封装成 Environment 对象。最后封装成configuration对象来调用sqlSessionFactoryBuilder.build(configuration); 需要注意的是 SqlSessionFactoryBean 实现了 Spring 的 FactoryBean 接口。这意味着由 Spring 最终创建的 bean 并不是 SqlSessionFactoryBean 本身，而是工厂类（SqlSessionFactoryBean）的 getObject() 方法的返回结果。这种情况下，Spring 将会在应用启动时为你创建 SqlSessionFactory，并使用 sqlSessionFactory 这个名字存储起来。因为是spring容器生成的，所以xml解析时对实现这个接口的对象会自动调用getObject() 方法 等效的 Java 代码如下： @Bean public SqlSessionFactory sqlSessionFactory() { SqlSessionFactoryBean factoryBean = new SqlSessionFactoryBean(); factoryBean.setDataSource(dataSource()); return factoryBean.getObject(); } 生成sqlsqlSessionFactory，由XMLConfigBuilder读取parse成一个sqlsqlSessionFactory，交由SqlsessionTemplate使用（其实现了 SqlSession 接口，并不直接调用具体的 SqlSession 的方法，而是委托给一个动态代理，通过代理 SqlSessionInterceptor 对方法调用进行拦截，用调用接口的方法时，会去寻找是否有了停留在resources中的未关闭的sqlsession） 若是编程式事务控制的话我们应该先创建SqlSessionFactoryBean创建sqlsqlSessionFactory，再将其注入自己创建的SqlsessionTemplate对象中，再利用SqlsessionTemplate进行增删查改 public class SqlSessionFactoryBean implements FactoryBean, InitializingBean, ApplicationListener { protected SqlSessionFactory buildSqlSessionFactory() throws IOException { XMLConfigBuilder xmlConfigBuilder = null; Configuration configuration; ··· 各种configuration的参数判断 ···· //生成SpringManagedTransactionFactory if (this.transactionFactory == null) { this.transactionFactory = new SpringManagedTransactionFactory(); } //封装成 Environment 对象 //封装成configuration对象 configuration.setEnvironment(new Environment(this.environment, this.transactionFactory, this.dataSource)); ··· 参数检查 ··· //调用sqlSessionFactoryBuilder.build(configuration);生成sqlsqlSessionFactory， return this.sqlSessionFactoryBuilder.build(configuration); } } DataSourceTransactionManager 若要使用spring的事务，使用sqlsessionTemplate前，我们要分析DataSourceTransactionManager整个流程，我们需要一步一步来，从最顶层开始 Spring的PlatformTransactionManager是事务管理的顶层接口，其中定义的三个方法对应的就是初始化事务、提交事务、回滚事务，，然后AbstractPlatformTransactionManager抽象类（作为模板）给出了三个步骤的具体实现。 但对诸如doGetTransaction()之类的和doBegin等还是弄成了抽象方法由DataSourceTransactionManager等实现，以便自己决定对 TransactionSynchronization接口的实现类进行调用，自己实现线程安全、获得ConnectionHolder和创建新事务时按何种方式dobgin，连接方式和注入各种参数。 public abstract class AbstractPlatformTransactionManager implements PlatformTransactionManager, Serializable { public final TransactionzhuangtStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException { //在DataSourceTransactionManager中重写了的类，查询当前线程中是否有传入的datasource对应的ConnectionHolder，有则依据他创建transaction Object transaction = this.doGetTransaction(); boolean debugEnabled = this.logger.isDebugEnabled(); if (definition == null) { definition = new DefaultTransactionDefinition(); } if (this.isExistingTransaction(transaction)) { return this.handleExistingTransaction((TransactionDefinition)definition, transaction, debugEnabled); } ··· 各种条件判断 ··· try { boolean newSynchronization = this.getTransactionSynchronization() != 2; //被创建的事务状态对象类型是DefaultTransactionStatus，它持有上述创建的事务对象。事务状态对象主要用于获取当前事务对象的状态，比如事务是否被标记了回滚，是否是一个新事务等等。 DefaultTransactionStatus status = this.newTransactionStatus((TransactionDefinition)definition, transaction, true, newSynchronization, debugEnabled, suspendedResources); //事务开始处理，进行促使化，连接获取 this.doBegin(transaction, (TransactionDefinition)definition); //对象都交由TransactionSynchronizationManager管理，TransactionSynchronizationManager把这些对象都保存在ThreadLocal中。在该transaction未commit/关闭前，该线程就会与该connection一直绑定在一起，通过只能同时绑定一个connection的原理，实现事务管理。 this.prepareSynchronization(status, (TransactionDefinition)definition); 返回事务状态对象：（主要进行：查询事务状态、通过setRollbackOnly（）方法标记当前事务使其回滚，根据事务参数创建内嵌事务），statu的意思是方便先对事务进行判断再获取transaction进行操作。 return status; } catch (Error | RuntimeException var7) { this.resume((Object)null, suspendedResources); throw var7; } } } } DataSourceTransactionManager（继承了AbstractPlatformTransactionManager）的主要作用是创建transaction和对transaction的初始化 public class DataSourceTransactionManager extends AbstractPlatformTransactionManager implements ResourceTransactionManager, InitializingBean { @Override//主要增加了对TransactionSynchronizationManager中当前线程是否有connectionHolder protected Object doGetTransaction() { // 创建事务对象 DataSourceTransactionObject txObject = new DataSourceTransactionObject(); txObject.setSavepointAllowed(isNestedTransactionAllowed()); ConnectionHolder conHolder = (ConnectionHolder) TransactionSynchronizationManager.getResource(obtainDataSource()); //ConnectionHolder赋值给事务对象txObject txObject.setConnectionHolder(conHolder, false); return txObject; } } //处理事务开始的方法 protected void doBegin(Object transaction, TransactionDefinition definition) { DataSourceTransactionObject txObject = (DataSourceTransactionObject) transaction; Connection con = null; try { //如果数据源事务对象的ConnectionHolder为null或者是事务同步的 if (txObject.getConnectionHolder() == null || txObject.getConnectionHolder().isSynchronizedWithTransaction()) { //获取当前数据源的数据库连接 Connection newCon = this.dataSource.getConnection(); if (logger.isDebugEnabled()) { logger.debug(\"Acquired Connection [\" + newCon + \"] for JDBC transaction\"); } //为数据源事务对象设置ConnectionHolder txObject.setConnectionHolder(new ConnectionHolder(newCon), true); } //设置数据源事务对象的事务同步 txObject.getConnectionHolder().setSynchronizedWithTransaction(true); //获取数据源事务对象的数据库连接 con = txObject.getConnectionHolder().getConnection(); //根据数据连接和事务属性，获取数据库连接的事务隔离级别 Integer previousIsolationLevel = DataSourceUtils.prepareConnectionForTransaction(con, definition); //为数据源事务对象设置事务隔离级别 txObject.setPreviousIsolationLevel(previousIsolationLevel); //如果数据库连接设置了自动事务提交属性，则关闭自动提交 if (con.getAutoCommit()) { //保存数据库连接设置的自动连接到数据源事务对象中 txObject.setMustRestoreAutoCommit(true); if (logger.isDebugEnabled()) { logger.debug(\"Switching JDBC Connection [\" + con + \"] to manual commit\"); } //设置数据库连接自动事务提交属性为false，即禁止自动事务提交 con.setAutoCommit(false); } //激活当前数据源事务对象的事务配置 txObject.getConnectionHolder().setTransactionActive(true); //获取事务配置的超时时长 int timeout = determineTimeout(definition); //如果事务配置的超时时长不等于事务的默认超时时长 if (timeout != TransactionDefinition.TIMEOUT_DEFAULT) { //数据源事务对象设置超时时长 txObject.getConnectionHolder().setTimeoutInSeconds(timeout); } //把当前数据库Connection和线程ThreadLocal绑定（key为DataSource，value为getConnectionHolder）spring事务管理的关键一步 if (txObject.isNewConnectionHolder()) { TransactionSynchronizationManager.bindResource(getDataSource(), txObject.getConnectionHolder()); } } catch (Exception ex) { DataSourceUtils.releaseConnection(con, this.dataSource); throw new CannotCreateTransactionException(\"Could not open JDBC Connection for transaction\", ex); } } TransactionSynchronizationManager 主要从线程中根据键值获取资源 public abstract class TransactionSynchronizationManager { private static final Log logger = LogFactory.getLog(TransactionSynchronizationManager.class); private static final ThreadLocal> resources = new NamedThreadLocal(\"Transactional resources\"); private static final ThreadLocal> synchronizations = new NamedThreadLocal(\"Transaction synchronizations\"); private static final ThreadLocal currentTransactionName = new NamedThreadLocal(\"Current transaction name\"); private static final ThreadLocal currentTransactionReadOnly = new NamedThreadLocal(\"Current transaction read-only status\"); private static final ThreadLocal currentTransactionIsolationLevel = new NamedThreadLocal(\"Current transaction isolation level\"); private static final ThreadLocal actualTransactionActive = new NamedThreadLocal(\"Actual transaction active\"); //获取相应的资源，键值先unwrapResourceIfNecessary处理，交由doget获取 public static Object getResource(Object key) { Object actualKey = TransactionSynchronizationUtils.unwrapResourceIfNecessary(key); Object value = doGetResource(actualKey); if (value != null && logger.isTraceEnabled()) { logger.trace(\"Retrieved value [\" + value + \"] for key [\" + actualKey + \"] bound to thread [\" + Thread.currentThread().getName() + \"]\"); } return value; } //从保存在线程的resource获取 @Nullable private static Object doGetResource(Object actualKey) { Map map = (Map)resources.get(); if (map == null) { return null; } else { Object value = map.get(actualKey); if (value instanceof ResourceHolder && ((ResourceHolder)value).isVoid()) { map.remove(actualKey); if (map.isEmpty()) { resources.remove(); } value = null; } return value; } } public static void bindResource(Object key, Object value) throws IllegalStateException { Object actualKey = TransactionSynchronizationUtils.unwrapResourceIfNecessary(key); Assert.notNull(value, \"Value must not be null\"); Map map = (Map)resources.get(); if (map == null) { map = new HashMap(); resources.set(map); } Object oldValue = ((Map)map).put(actualKey, value); if (oldValue instanceof ResourceHolder && ((ResourceHolder)oldValue).isVoid()) { oldValue = null; } if (oldValue != null) { throw new IllegalStateException(\"Already value [\" + oldValue + \"] for key [\" + actualKey + \"] bound to thread [\" + Thread.currentThread().getName() + \"]\"); } else { if (logger.isTraceEnabled()) { logger.trace(\"Bound value [\" + value + \"] for key [\" + actualKey + \"] to thread [\" + Thread.currentThread().getName() + \"]\"); } } } 此时若是注释运行的便需要把PlatformTransactionManager，他生成的TransactionStatus、TransactionAttribute等封装到TransactionInfo中以便注解能自己调用 Spring+mybatis 完成了对事务的创建，并将其用TransactionSynchronizationManager绑定到了thread local上，接着便是对sqlsessionTemplate的调用了即对mybatis框架的sqlsession的整合了 Spring+mybatis的事务控制流程 SqlsessionTemplate SqlsessionTemplate的使用并不是直接用sqlsession进行增删查改（实际上是sqlsession的一个代理类，其实现了 SqlSession 接口，并不直接调用具体的 SqlSession 的方法，而是委托给一个动态代理，通过代理 SqlSessionInterceptor 对方法调用进行拦截，用调用接口的方法时，会去寻找是否有了停留在resources中的未关闭的sqlsession） public class SqlSessionTemplate implements SqlSession, DisposableBean { public SqlSessionTemplate(SqlSessionFactory sqlSessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) { Assert.notNull(sqlSessionFactory, \"Property 'sqlSessionFactory' is required\"); Assert.notNull(executorType, \"Property 'executorType' is required\"); this.sqlSessionFactory = sqlSessionFactory; this.executorType = executorType; this.exceptionTranslator = exceptionTranslator; this.sqlSessionProxy = (SqlSession)Proxy.newProxyInstance(SqlSessionFactory.class.getClassLoader(), new Class[]{SqlSession.class}, new SqlSessionTemplate.SqlSessionInterceptor()); } // 动态代理中的 InvocationHandler类，真正对方法的前后进行通知的类，代理类对象只是提供一个目标对象的封装以调用而已 private class SqlSessionInterceptor implements InvocationHandler { private SqlSessionInterceptor() { } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { //查看是否有为关闭的闲置的同个配置的sqlsession，有则调用，这也是你用sqlsessionTemplate代理执行增删查改一直是同一个sqlsession完成事务管理的原因，没有则生成。我们可以看到是由SqlSessionUtils实现的 SqlSession sqlSession = SqlSessionUtils.getSqlSession(SqlSessionTemplate.this.sqlSessionFactory, SqlSessionTemplate.this.executorType, SqlSessionTemplate.this.exceptionTranslator); Object unwrapped; try { Object result = method.invoke(sqlSession, args); if (!SqlSessionUtils.isSqlSessionTransactional(sqlSession, SqlSessionTemplate.this.sqlSessionFactory)) { //查询该sqlsession是否在Spring事务中，若是则步不自动提交 //若是自动则为一条sql一次commit，是为新手设置的，一般手动提交。 sqlSession.commit(true); } unwrapped = result; } catch (Throwable var11) { unwrapped = ExceptionUtil.unwrapThrowable(var11); if (SqlSessionTemplate.this.exceptionTranslator != null && unwrapped instanceof PersistenceException) { SqlSessionUtils.closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); sqlSession = null; Throwable translated = SqlSessionTemplate.this.exceptionTranslator.translateExceptionIfPossible((PersistenceException)unwrapped); if (translated != null) { unwrapped = translated; } } throw (Throwable)unwrapped; } finally { //关闭sqlsession，同一个事务可以有多个sqlsession if (sqlSession != null) { SqlSessionUtils.closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); } } return unwrapped; } } } SqlSessionUtils 我们来看下SqlSessionUtils的实现，这里的sqlsession的创建需要注意transactionFactory是SpringManageTransactionFactory。 public final class SqlSessionUtils { public static SqlSession getSqlSession(SqlSessionFactory sessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) { Assert.notNull(sessionFactory, \"No SqlSessionFactory specified\"); Assert.notNull(executorType, \"No ExecutorType specified\"); //有相应的sqlsessionHolder则取出里面的SqlSession SqlSessionHolder holder = (SqlSessionHolder)TransactionSynchronizationManager.getResource(sessionFactory); SqlSession session = sessionHolder(executorType, holder); if (session != null) { return session; } else { if (LOGGER.isDebugEnabled()) { LOGGER.debug(\"Creating a new SqlSession\"); } //没有则用工厂创建sqlsession，注意：此时的sqlsessionfactory是sqlsessionfactoryBean创建的，即他的configuration中的Enviroment中的transactionFactory是SpringManageTransactionFactory。 session = sessionFactory.openSession(executorType); // 注册sqlsession到TransactionSyncronizedMager， registerSessionHolder(sessionFactory, executorType, exceptionTranslator, session); return session; } } } 关于SqlSessionUtils中的registerSessionHolder方法，注册sqlsession到TransactionSyncronizedMager中， 包含两个方法 bindResource(sessionFactory, holder); registerSynchronization(new SqlSessionUtils.SqlSessionSynchronization(holder, sessionFactory))； private static void registerSessionHolder(SqlSessionFactory sessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator, SqlSession session) { if (TransactionSynchronizationManager.isSynchronizationActive()) { //从sessionFactory中获取Environment Environment environment = sessionFactory.getConfiguration().getEnvironment(); //判断environment中封装的TransactionFactorySpringManagedTransactionFactory，是否用到了Spring的事务管理服务，sqlsession中对TransactionSyncronizedMager的应用只有与Spring集成时才用到，mybatis自身的事务管理并不会用到。 if (environment.getTransactionFactory() instanceof SpringManagedTransactionFactory) { if (LOGGER.isDebugEnabled()) { LOGGER.debug(\"Registering transaction synchronization for SqlSession [\" + session + \"]\"); } SqlSessionHolder holder = new SqlSessionHolder(session, executorType, exceptionTranslator); //绑定、注册到TransactionSynchronizationManager TransactionSynchronizationManager.bindResource(sessionFactory, holder); TransactionSynchronizationManager.registerSynchronization(new SqlSessionUtils.SqlSessionSynchronization(holder, sessionFactory)); holder.setSynchronizedWithTransaction(true); holder.requested(); } else { if (TransactionSynchronizationManager.getResource(environment.getDataSource()) != null) { throw new TransientDataAccessResourceException(\"SqlSessionFactory must be using a SpringManagedTransactionFactory in order to use Spring transaction synchronization\"); } if (LOGGER.isDebugEnabled()) { LOGGER.debug(\"SqlSession [\" + session + \"] was not registered for synchronization because DataSource is not transactional\"); } } } else if (LOGGER.isDebugEnabled()) { LOGGER.debug(\"SqlSession [\" + session + \"] was not registered for synchronization because synchronization is not active\"); } } SqlSessionSynchronization 其中SqlSessionSynchronization是一个事务生命周期的callback接口，mybatis-spring通过SqlSessionSynchronization在事务提交和回滚前分别调用DefaultSqlSession.commit()和DefaultSqlSession.rollback() private static final class SqlSessionSynchronization extends TransactionSynchronizationAdapter { public void beforeCommit(boolean readOnly) { if (TransactionSynchronizationManager.isActualTransactionActive()) { try { if (SqlSessionUtils.LOGGER.isDebugEnabled()) { SqlSessionUtils.LOGGER.debug(\"Transaction synchronization committing SqlSession [\" + this.holder.getSqlSession() + \"]\"); } this.holder.getSqlSession().commit(); } catch (PersistenceException var4) { if (this.holder.getPersistenceExceptionTranslator() != null) { DataAccessException translated = this.holder.getPersistenceExceptionTranslator().translateExceptionIfPossible(var4); if (translated != null) { throw translated; } } throw var4; } } } public void beforeCompletion() { if (!this.holder.isOpen()) { if (SqlSessionUtils.LOGGER.isDebugEnabled()) { SqlSessionUtils.LOGGER.debug(\"Transaction synchronization deregistering SqlSession [\" + this.holder.getSqlSession() + \"]\"); } TransactionSynchronizationManager.unbindResource(this.sessionFactory); this.holderActive = false; if (SqlSessionUtils.LOGGER.isDebugEnabled()) { SqlSessionUtils.LOGGER.debug(\"Transaction synchronization closing SqlSession [\" + this.holder.getSqlSession() + \"]\"); } this.holder.getSqlSession().close(); } } public void afterCompletion(int status) { if (this.holderActive) { if (SqlSessionUtils.LOGGER.isDebugEnabled()) { SqlSessionUtils.LOGGER.debug(\"Transaction synchronization deregistering SqlSession [\" + this.holder.getSqlSession() + \"]\"); } TransactionSynchronizationManager.unbindResourceIfPossible(this.sessionFactory); this.holderActive = false; if (SqlSessionUtils.LOGGER.isDebugEnabled()) { SqlSessionUtils.LOGGER.debug(\"Transaction synchronization closing SqlSession [\" + this.holder.getSqlSession() + \"]\"); } this.holder.getSqlSession().close(); } this.holder.reset(); } datasourceUtils和SpringManagedTransaction 一番周折终于拿到了sqlsession到了代理类SqlsessionTemplate中，接着便是目标代码本身(sql语句)的执行了 在此之前我们谈及为何要专门用SpringManagedTransaction，现在就要揭晓了，之前我们创建了sqlsession到了SpringTemplate，但是还未获取connection，所以我们执行语句前就要获取connection，我们用mybatis的Executor执行语句，会在prepareStatement中获得connection protected Connection getConnection(Log statementLog) throws SQLException { Connection connection = this.transaction.getConnection(); return statementLog.isDebugEnabled() ? ConnectionLogger.newInstance(connection, statementLog, this.queryStack) : connection; } 看到上面的transsaction便可以知道我们需要一个transaction来实现连接的获取，但是我们的连接是之前绑定到了ThreadLocal中了，我们Spring事务管理的关键要让此时Thread中的connection用上，这时候我们就需要用到DataSourceUtils来获取了 datasourceUtils:也是从TransactionSynchronizationManager获取connection SqlSessionUtils从TransactionSynchronizationManager获取ConnectionHolder交给SpringManagedTransaction，并作为参数封装进Executor，最后封装进DefaultSqlSession，将Spring的事务管理与它的数据访问框架是紧密结合的 SpringManagedTransaction中保留由datasource等信息，而且它特别实现了对SqlSessionUtils的调用（其他的jdbcTransaction和managedTransaction都没有对SqlSessionUtils调用的方法，这两者只适用于mybaitis自身事务情况，不适用于集成） 并且可以用来给datasourceUtils提供参数，让其可以直接从TransactionSynchronizationManager中调用相应的connection，因为是同一线程且事务没结束所以能保证是同一个了 SpringManagedTransaction的意义便在此，我们的问题也就解决了，最后便是jdbc.connection的excute操作了 public abstract class DataSourceUtils { public static Connection doGetConnection(DataSource dataSource) throws SQLException { Assert.notNull(dataSource, \"No DataSource specified\"); ConnectionHolder conHolder = (ConnectionHolder) TransactionSynchronizationManager.getResource(dataSource); if (conHolder != null && (conHolder.hasConnection() || conHolder.isSynchronizedWithTransaction())) { conHolder.requested(); if (!conHolder.hasConnection()) { logger.debug(\"Fetching resumed JDBC Connection from DataSource\"); conHolder.setConnection(fetchConnection(dataSource)); } return conHolder.getConnection(); } // Else we either got no holder or an empty thread-bound holder here. logger.debug(\"Fetching JDBC Connection from DataSource\"); Connection con = fetchConnection(dataSource); if (TransactionSynchronizationManager.isSynchronizationActive()) { logger.debug(\"Registering transaction synchronization for JDBC Connection\"); // Use same Connection for further JDBC actions within the transaction. // Thread-bound object will get removed by synchronization at transaction completion. ConnectionHolder holderToUse = conHolder; if (holderToUse == null) { holderToUse = new ConnectionHolder(con); } else { holderToUse.setConnection(con); } holderToUse.requested(); TransactionSynchronizationManager.registerSynchronization( new ConnectionSynchronization(holderToUse, dataSource)); holderToUse.setSynchronizedWithTransaction(true); if (holderToUse != conHolder) { TransactionSynchronizationManager.bindResource(dataSource, holderToUse); } } return con; } 可以看到mybatis-spring处理事务的主要流程和spring jdbc处理事务并没有什么区别，都是通过DataSourceTransactionManager的getTransaction(), rollback(), commit()完成事务的生命周期管理，而且jdbc connection的创建也是通过DataSourceTransactionManager.getTransaction()完成，mybatis并没有参与其中，mybatis只是在执行sql时通过DataSourceUtils.getConnection()获得当前thread的jdbc connection，然后在其上执行sql。 commit 最后写一下commit的流程（rollback实现差不多，回到保存的回调点而已） 调用DataSourceTransactionManager的父类AbstractPlatformTransactionManager的commit public abstract class AbstractPlatformTransactionManager implements PlatformTransactionManager, Serializable { public final void commit(TransactionStatus status) throws TransactionException { ··· status 是否已完成或是否需要rollback ··· //正常流程会调用processCommit this.processCommit(defStatus); } } } } ​ private void processCommit(DefaultTransactionStatus status) throws TransactionException { try { boolean beforeCompletionInvoked = false; try { boolean unexpectedRollback = false; //commit准备 this.prepareForCommit(status); //BeforeCommit：commit前对springTransaciton、sqlsession的commit，大部分未实现这个的功能 this.triggerBeforeCommit(status); //BeforeCompletion：Complete前对springTransaciton、sqlsession的commit this.triggerBeforeCompletion(status); beforeCompletionInvoked = true; ··· 是否有记录回调点/是否需要回调 ··· // 在DataSourceTransactionManager中实现对connection的commit this.doCommit(status); ··· 异常处理 ··· try { //对TransactionSynchronizationManager中的synchronizations用Iterator计数器遍历删除 this.triggerAfterCommit(status); } finally { // 若上一步没清理完成，则再次清理一次，用synchronizations用Iterator计数器遍历删除 this.triggerAfterCompletion(status, 0); } } finally { // 清空TransactionSynchronizationManager，若有挂起的事务，则恢复执行 this.cleanupAfterCompletion(status); } } // 清空TransactionSynchronizationManager，若有挂起的事务，则恢复执行，这和definiton设置的事务级别有关 private void cleanupAfterCompletion(DefaultTransactionStatus status) { status.setCompleted(); if (status.isNewSynchronization()) { TransactionSynchronizationManager.clear(); } if (status.isNewTransaction()) { this.doCleanupAfterCompletion(status.getTransaction()); } if (status.getSuspendedResources() != null) { if (status.isDebug()) { this.logger.debug(\"Resuming suspended transaction after completion of inner transaction\"); } Object transaction = status.hasTransaction() ? status.getTransaction() : null; //SuspendedResourcesHolder用静态方法放置在JVM的方法区，可以直接调用 this.resume(transaction, (AbstractPlatformTransactionManager.SuspendedResourcesHolder)status.getSuspendedResources()); } ``` 这便是整个spring与mybatis集成的事务控制了。 powered by Gitbook该文件最后修改时间： 2021-04-21 00:29:42 "},"数据库/mybatisPlus.html":{"url":"数据库/mybatisPlus.html","title":"MybatisPlus","keywords":"","body":"特性 无侵入：只做增强不做改变，引入它不会对现有工程产生影响，如丝般顺滑 损耗小：启动即会自动注入基本 CURD，性能基本无损耗，直接面向对象操作， BaseMapper 强大的 CRUD 操作：内置通用 Mapper、通用 Service，仅仅通过少量配置即可实现单表大部分 CRUD 操作，更有强大的条件构造器，满足各类使用需求, 以后简单的CRUD操作，它不用自己编写 了！ 支持 Lambda 形式调用：通过 Lambda 表达式，方便的编写各类查询条件，无需再担心字段写错 支持主键自动生成：支持多达 4 种主键策略（内含分布式唯一 ID 生成器 - Sequence），可自由配 置，完美解决主键问题 支持 ActiveRecord 模式：支持 ActiveRecord 形式调用，实体类只需继承 Model 类即可进行强大 的 CRUD 操作 支持自定义全局通用操作：支持全局通用方法注入（ Write once, use anywhere ） 内置代码生成器：采用代码或者 Maven 插件可快速生成 Mapper 、 Model 、 Service 、 Controller 层代码，支持模板引擎，更有超多自定义配置等您来使用（自动帮你生成代码） 内置分页插件：基于 MyBatis 物理分页，开发者无需关心具体操作，配置好插件之后，写分页等同 于普通 List 查询 分页插件支持多种数据库： 支持 MySQL、MariaDB、Oracle、DB2、H2、HSQL、SQLite、 Postgre、SQLServer 等多种数据库 内置性能分析插件：可输出 Sql 语句以及其执行时间，建议开发测试时启用该功能，能快速揪出慢 查询 内置全局拦截插件：提供全表 delete 、 update 操作智能分析阻断，也可自定义拦截规则，预防误 id, gmt_create, gmt_modified。 说明：其中 id 必为主键，类型为 unsigned bigint、单表时自增、步长为 1。 gmt_create,gmt_modified 的类型均为 date_time 类型，前者现在时表示主动创建，后者过去分词表示被 动更新。 eg： create table `test`( `id` bigint unsigned not null auto_increment, `gmt_create` datetime null default current_timestamp, `gmt_modified` datetime null default current_timestamp on update current_timestamp, primary key(`id`) ); on update current_timestamp 有数据更改时自动记录修改时间，记录的是修改的时间，不是提交的时间，不能用来做同步 项目构建 初始化项目 创建一个空的 Spring Boot 工程（工程将以 H2 作为默认数据库进行演示） 创建数据库 mybatis_plus 创建user表 可以使用 Spring Initializer 快速初始化一个 Spring Boot 工程 4、导入依赖 mysql mysql-connector-java org.projectlombok lombok com.baomidou mybatis-plus-boot-starter 3.0.5 说明：我们使用 mybatis-plus 可以节省我们大量的代码，尽量不要同时导入 mybatis 和 mybatisplus！版本的差异！ 5、连接数据库！这一步和 mybatis 相同！ # mysql 5 驱动不同 com.mysql.jdbc.Driver # mysql 8 驱动不同com.mysql.cj.jdbc.Driver、需要增加时区的配置 serverTimezone=GMT%2B8 spring.datasource.username=root spring.datasource.password=123456 spring.datasource.url=jdbc:mysql://localhost:3306/mybatis_plus? useSSL=false&useUnicode=true&characterEncoding=utf-8&serverTimezone=GMT%2B8 spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver 6、传统方式pojo-dao（连接mybatis，配置mapper.xml文件）-service-controller ​ 用了mybatis-plus 之后 pojo @Data @AllArgsConstructor @NoArgsConstructor public class User { private Long id; private String name; private Integer age; private String email; } mapper接口 package com.kuang.mapper; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import com.kuang.pojo.User; import org.springframework.stereotype.Repository; // 在对应的Mapper上面继承基本的类 BaseMapper @Repository // 代表持久层 public interface UserMapper extends BaseMapper { // 所有的CRUD操作都已经编写完成了 // 你不需要像以前的配置一大堆文件了！ } 注意点，我们需要在主启动类上去扫描我们的mapper包下的所有接口 @MapperScan(\"com.kuang.mapper\") 测试类中测试 @SpringBootTest class MybatisPlusApplicationTests { // 继承了BaseMapper，所有的方法都来自己父类 // 我们也可以编写自己的扩展方法！ @Autowired private UserMapper userMapper; @Test void contextLoads() { // 参数是一个 Wrapper ，条件构造器，这里我们先不用 null // 查询全部用户 List users = userMapper.selectList(null); users.forEach(System.out::println); } } 配置日志 mybatis-plus.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl //控制台基本的日志 CRUD service CRUD接口和Mapper CRUD 接口 https://mp.baomidou.com/guide/crud-interface.html#mapper-%E5%B1%82-%E9%80%89%E8%A3%85%E4%BB%B6 Service CRUD 接口 通用 Service CRUD 封装IService接口，进一步封装 CRUD 采用 get 查询单行 remove 删除 list 查询集合 page 分页 前缀命名方式区分 Mapper 层避免混淆， 泛型 T 为任意实体对象 建议如果存在自定义通用 Service 方法的可能，请创建自己的 IBaseService 继承 Mybatis-Plus 提供的基类 对象 Wrapper 为 条件构造器 Save、Update、Get、List、Page、Count、Chain Mapper CRUD 接口 通用 CRUD 封装BaseMapper接口，为 Mybatis-Plus 启动时自动解析实体表关系映射转换为 Mybatis 内部对象注入容器 泛型 T 为任意实体对象 参数 Serializable 为任意类型主键 Mybatis-Plus 不推荐使用复合主键约定每一张表都有自己的唯一 id 主键 对象 Wrapper 为 条件构造器 Insert、Delete、Update、Select Insert @Test public void testInsert(){ User user = new User(); user.setName(\"eternal\"); user.setAge(21); user.setEmail(\"254908537@qq.com\"); int result = userMapper.insert(user); System.out.println(result); System.out.println(user); } 主键生成策略 1. 数据库自增长序列或字段 最常见的方式。利用数据库，全数据库唯一。 优点： 1）简单，代码方便，性能可以接受。 2）数字ID天然排序，对分页或者需要排序的结果很有帮助。 缺点： 1）不同数据库语法和实现不同，数据库迁移的时候或多数据库版本支持的时候需要处理。 2）在单个数据库或读写分离或一主多从的情况下，只有一个主库可以生成。有单点故障的风险。 3）在性能达不到要求的情况下，比较难于扩展。 4）如果遇见多个系统需要合并或者涉及到数据迁移会相当痛苦。 5）分表分库的时候会有麻烦。 优化方案： 1）针对主库单点，如果有多个Master库，则每个Master库设置的起始数字不一样，步长一样，可以是Master的个数。比如：Master1 生成的是 1，4，7，10，Master2生成的是2,5,8,11 Master3生成的是 3,6,9,12。这样就可以有效生成集群中的唯一ID，也可以大大降低ID生成数据库操作的负载。 2. UUID 常见的方式。可以利用数据库也可以利用程序生成，一般来说全球唯一。 优点： 1）简单，代码方便。 2）生成ID性能非常好，基本不会有性能问题。 3）全球唯一，在遇见数据迁移，系统数据合并，或者数据库变更等情况下，可以从容应对。 缺点： 1）没有排序，无法保证趋势递增。 2）UUID往往是使用字符串存储，查询的效率比较低。 3）存储空间比较大，如果是海量数据库，就需要考虑存储量的问题。 4）传输数据量大 5）不可读。 UUID UUID(Universally Unique IDentifier)是一个128位数字的唯一标识。RFC 4122描述了具体的规范实现。本文尝试从它的结构一步步分析为什么它能做到唯一性？及各个版本的使用场景。 Format UUID使用16进制表示，共有36个字符(32个字母数字+4个连接符”-“)，格式为8-4-4-4-12，如： 6d25a684-9558-11e9-aa94-efccd7a0659b xxxxxxxx-xxxx-Mxxx-Nxxx-xxxxxxxxxxxx M中使用4位来表示UUID的版本，N中使用1-3位表示不同的variant。如上面所示：M =1, N = a表示此UUID为version-1，variant-1的UUID(Time-based ECE/RFC 4122 UUID)。 但是为什么最开始说它是一个128位的唯一标识呢？这里明明字母位数是(8+4+4+4+12)*8=256位。 因为上面的字母是用的16进制，一个16进制只代表4个bit，所以应该是(8+4+4+4+12)*4=128位。 UUID使用的是大数位format(big-endian)，比如： 00112233-4455-6677-8899-aabbccddeeff 编码就是 00 11 22 33 44 55 66 77 88 99 aa bb cc dd ee ff. UUID现有5种版本，是根据不同的使用场景划分的，而不是根据精度，所以Version5并不会比Version1精度高，在精度上，大家都能保证唯一性，重复的概率近乎于0。 V1(date-time MAC address) 基于时间戳及MAC地址的UUID实现。它包括了48位的MAC地址和60位的时间戳， 接下来使用ossp-uuid命令行创建5个UUID v1。(在Mac安装brew install ossp-uuid) uuid -n 5 -v1 5b01c826-9561-11e9-9659-cb41250df352 5b01cc7c-9561-11e9-965a-57ad522dee7f 5b01cea2-9561-11e9-965b-a3d050dd0f99 5b01cf60-9561-11e9-965c-1b66505f58da 5b01d118-9561-11e9-965d-97354eb9e996 肉眼一看，有一种所有的UUID都很相似的感觉，几乎就要重复了！怎么回事？ 其实v1为了保证唯一性，当时间精度不够时，会使用13~14位的clock sequence来扩展时间戳，比如： 当UUID的生产成速率太快，超过了系统时间的精度。时间戳的低位部分会每增加一个UUID就+1的操作来模拟更高精度的时间戳，换句话说，就是当系统时间精度无法区分2个UUID的时间先后顺序时，为了保证唯一性，会在其中一个UUID上+1。所以UUID重复的概率几乎为0，时间戳加扩展的clock sequence一共有74bits,(2的74次方，约为1.8后面加22个零),即在每个节点下，每秒可产生1630亿不重复的UUID(因为只精确到了秒,不再是74位，所以换算了一下)。 相对于其它版本，v1还加入48位的MAC地址，这依赖于网卡供应商能提供唯一的MAC地址，同时也可能通过它反查到对应的MAC地址。Melissa)病毒就是这样做到的。 V2(date-time Mac address) 这是最神秘的版本，RFC没有提供具体的实现细节，以至于大部分的UUID库都没有实现它，只在特定的场景(DCE security)才会用到。所以绝大数情况，我们也不会碰到它。 V3,5(namespace name-based) V3和V5都是通过hash namespace的标识符和名称生成的。V3使用MD5作为hash函数，V5则使用SHA-1。 因为里面没有不确定的部分，所以当namespace与输入参数确定时，得到的UUID都是确定唯一的。比如： uuid -n 3 -v3 ns:URL http://www.baidu.com 2f67490d-55a4-395e-b540-457195f7aa95 2f67490d-55a4-395e-b540-457195f7aa95 2f67490d-55a4-395e-b540-457195f7aa95 可以看到这3个UUID都是一样的。 具体的流程就是 把namespace和输入参数拼接在一起，如”http/wwwbaidu.com” ++ “/query=uuid”； 使用MD5算法对拼接后的字串进行hash，截断为128位； 把UUID的Version和variant字段都替换成固定的; 如果需要to_string，需要转为16进制和加上连接符”-“。 把V3的hash算法由MD5换成SHA-1就成了V5。 V4(random) 这个版本使用最为广泛: uuid -n 5 -v4 a3535b78-69dd-4a9e-9a79-57e2ea28981b a9ba3122-d89b-4855-85f1-92a018e5c457 7c59d031-a143-4676-a8ce-1b24200ab463 533831da-eab4-4c7d-a3f6-1e2da5a4c9a0 def539e8-d298-4575-b769-b55d7637b51e 其中4位代表版本，2-3位代表variant。余下的122-121位都是全部随机的。即有2的122次方(5.3后面36个0)个UUID。一个标准实现的UUID库在生成了2.71万亿个UUID会产生重复UUID的可能性也只有50%的概率 这相当于每秒产生10亿的UUID，持续85年，而把这些UUID都存入文件，每个UUID占16bytes,总需要45 EB(exabytes)，比目前最大的数据库(PB)还要大很多倍。 Summary 如果只是需要生成一个唯一ID,你可以使用V1或V4。 V1基于时间戳和Mac地址,这些ID有一定的规律(你给出一个，是有可能被猜出来下一个是多少的)，而且会暴露你的Mac地址。 V4是完全随机(伪)的。 如果对于相同的参数需要输出相同的UUID,你可以使用V3或V5。 V3基于MD5 hash算法，如果需要考虑与其它系统的兼容性的话，就用它,因为它出来得早，大概率大家都是用它的。 V5基于SHA-1 hash算法，这个是首选。 3. UUID的变种 1）为了解决UUID不可读，可以使用UUID to Int64的方法。及 /// /// 根据GUID获取唯一数字序列 /// public static long GuidToInt64() { byte[] bytes = Guid.NewGuid().ToByteArray(); return BitConverter.ToInt64(bytes, 0); } 2）为了解决UUID无序的问题，NHibernate在其主键生成方式中提供了Comb算法（combined guid/timestamp）。保留GUID的10个字节，用另6个字节表示GUID生成的时间（DateTime）。 /// /// Generate a new using the comb algorithm. /// private Guid GenerateComb() { byte[] guidArray = Guid.NewGuid().ToByteArray(); DateTime baseDate = new DateTime(1900, 1, 1); DateTime now = DateTime.Now; // Get the days and milliseconds which will be used to build //the byte string TimeSpan days = new TimeSpan(now.Ticks - baseDate.Ticks); TimeSpan msecs = now.TimeOfDay; // Convert to a byte array // Note that SQL Server is accurate to 1/300th of a // millisecond so we divide by 3.333333 byte[] daysArray = BitConverter.GetBytes(days.Days); byte[] msecsArray = BitConverter.GetBytes((long) (msecs.TotalMilliseconds / 3.333333)); // Reverse the bytes to match SQL Servers ordering Array.Reverse(daysArray); Array.Reverse(msecsArray); // Copy the bytes into the guid Array.Copy(daysArray, daysArray.Length - 2, guidArray, guidArray.Length - 6, 2); Array.Copy(msecsArray, msecsArray.Length - 4, guidArray, guidArray.Length - 4, 4); return new Guid(guidArray); } 用上面的算法测试一下，得到如下的结果：作为比较，前面3个是使用COMB算法得出的结果，最后12个字符串是时间序（统一毫秒生成的3个UUID），过段时间如果再次生成，则12个字符串会比图示的要大。后面3个是直接生成的GUID。 如果想把时间序放在前面，可以生成后改变12个字符串的位置，也可以修改算法类的最后两个Array.Copy。 4. Redis生成ID 当使用数据库来生成ID性能不够要求的时候，我们可以尝试使用Redis来生成ID。这主要依赖于Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作 INCR和INCRBY来实现。 可以使用Redis集群来获取更高的吞吐量。假如一个集群中有5台Redis。可以初始化每台Redis的值分别是1,2,3,4,5，然后步长都是5。各个Redis生成的ID为： A：1,6,11,16,21 B：2,7,12,17,22 C：3,8,13,18,23 D：4,9,14,19,24 E：5,10,15,20,25 这个，随便负载到哪个机确定好，未来很难做修改。但是3-5台服务器基本能够满足器上，都可以获得不同的ID。但是步长和初始值一定需要事先需要了。使用Redis集群也可以方式单点故障的问题。 另外，比较适合使用Redis来生成每天从0开始的流水号。比如订单号=日期+当日自增长号。可以每天在Redis中生成一个Key，使用INCR进行累加。 优点： 1）不依赖于数据库，灵活方便，且性能优于数据库。 2）数字ID天然排序，对分页或者需要排序的结果很有帮助。 缺点： 1）如果系统中没有Redis，还需要引入新的组件，增加系统复杂度。 2）需要编码和配置的工作量比较大。 5. Twitter的snowflake算法 snowflake是Twitter开源的分布式ID生成算法，结果是一个long型的ID。其核心思想是：使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0。具体实现的代码可以参看https://github.com/twitter/snowflake。 C#代码如下： /// /// From: https://github.com/twitter/snowflake /// An object that generates IDs. /// This is broken into a separate class in case /// we ever want to support multiple worker threads /// per process /// public class IdWorker { private long workerId; private long datacenterId; private long sequence = 0L; private static long twepoch = 1288834974657L; private static long workerIdBits = 5L; private static long datacenterIdBits = 5L; private static long maxWorkerId = -1L ^ (-1L maxWorkerId || workerId maxDatacenterId || datacenterId 测试代码如下： private static void TestIdWorker() { HashSet set = new HashSet(); IdWorker idWorker1 = new IdWorker(0, 0); IdWorker idWorker2 = new IdWorker(1, 0); Thread t1 = new Thread(() => DoTestIdWoker(idWorker1, set)); Thread t2 = new Thread(() => DoTestIdWoker(idWorker2, set)); t1.IsBackground = true; t2.IsBackground = true; t1.Start(); t2.Start(); try { Thread.Sleep(30000); t1.Abort(); t2.Abort(); } catch (Exception e) { } Console.WriteLine(\"done\"); } private static void DoTestIdWoker(IdWorker idWorker, HashSet set) { while (true) { long id = idWorker.nextId(); if (!set.Add(id)) { Console.WriteLine(\"duplicate:\" + id); } Thread.Sleep(1); } } snowflake算法可以根据自身项目的需要进行一定的修改。比如估算未来的数据中心个数，每个数据中心的机器数以及统一毫秒可以能的并发数来调整在算法中所需要的bit数。 优点： 1）不依赖于数据库，灵活方便，且性能优于数据库。 2）ID按照时间在单机上是递增的。 缺点： 1）在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，也许有时候也会出现不是全局递增的情况。 6. 利用zookeeper生成唯一ID zookeeper主要通过其znode数据版本来生成序列号，可以生成32位和64位的数据版本号，客户端可以使用这个版本号来作为唯一的序列号。 很少会使用zookeeper来生成唯一ID。主要是由于需要依赖zookeeper，并且是多步调用API，如果在竞争较大的情况下，需要考虑使用分布式锁。因此，性能在高并发的分布式环境下，也不甚理想。 7. MongoDB的ObjectId MongoDB的ObjectId和snowflake算法类似。它设计成轻量型的，不同的机器都能用全局唯一的同种方法方便地生成它。MongoDB 从一开始就设计用来作为分布式数据库，处理多个节点是一个核心要求。使其在分片环境中要容易生成得多。 其格式如下： 前4 个字节是从标准纪元开始的时间戳，单位为秒。时间戳，与随后的5 个字节组合起来，提供了秒级别的唯一性。由于时间戳在前，这意味着ObjectId 大致会按照插入的顺序排列。这对于某些方面很有用，如将其作为索引提高效率。这4 个字节也隐含了文档创建的时间。绝大多数客户端类库都会公开一个方法从ObjectId 获取这个信息。 接下来的3 字节是所在主机的唯一标识符。通常是机器主机名的散列值。这样就可以确保不同主机生成不同的ObjectId，不产生冲突。 为了确保在同一台机器上并发的多个进程产生的ObjectId 是唯一的，接下来的两字节来自产生ObjectId 的进程标识符（PID）。 前9 字节保证了同一秒钟不同机器不同进程产生的ObjectId 是唯一的。后3 字节就是一个自动增加的计数器，确保相同进程同一秒产生的ObjectId 也是不一样的。同一秒钟最多允许每个进程拥有2563（16 777 216）个不同的ObjectId。 实现的源码可以到MongoDB官方网站下载。 sequence oracle等一些数据库没有如mysql像ID这种的可以设置自增的字段，所有需要通过专门设置一个值来自增，一般有两个用途： 一：作为代理主键，唯一识别；ID = sequence_name.nextval 这种操作 二：用于记录数据库中最新动作的语句，只要语句有动作(I/U/D等)，sequence号都会随着更新，每有IUD就sequence_name+1，配合日志，我们就可以根据sequence号来select出更新的语句。 Mybatis-Plus 已经定义好了常见的数据库主键序列，我们首先只需要在 @Configuration 类中定义好 @Bean： Mybatis-Plus 内置了如下数据库主键序列（如果内置支持不满足你的需求，可实现 接口来进行扩展）： DB2KeyGenerator H2KeyGenerator KingbaseKeyGenerator OracleKeyGenerator PostgreKeyGenerator @Bean public OracleKeyGenerator oracleKeyGenerator(){ return new OracleKeyGenerator(); } 1234 然后实体类配置主键 Sequence，指定主键策略为 IdType.INPUT 即可： 提示：支持父类定义 @KeySequence 子类使用，这样就可以几个表共用一个 Sequence @TableName(\"TEST_SEQUSER\") @KeySequence(\"SEQ_TEST\")//类注解 public class TestSequser{ @TableId(value = \"ID\", type = IdType.INPUT) private Long id; } 123456 如果主键是 String 类型的，也可以使用： 如何使用 Sequence String varchar2 sequence 作为主键，但是实体主键类型是 也就是说，表的主键是 ，但是需要从中取值 实体定义 @KeySequence 注解 clazz 指定类型 String.class 实体定义主键的类型 String 注意：oracle 的 sequence 返回的是 Long 类型，如果主键类型是 Integer，可能会引起 ClassCastException @KeySequence(value = \"SEQ_ORACLE_STRING_KEY\", clazz = String.class) public class YourEntity{ @TableId(value = \"ID_STR\", type = IdType.INPUT) private String idStr; } IdType mybatis-PLUS默认实现的是雪花算法，若需要换成自增 实体类字段上 @TableId(type = IdType.AUTO) 数据库字段一定要是自增！ @TableId关键字说明 public enum IdType { AUTO(0),//自增 NONE(1),//不做操作 INPUT(2),//手动输入 ASSIGN_ID(3),//默认的全局唯一ID：雪花算法 ASSIGN_UUID(4),//全局唯一ID /** @deprecated */ @Deprecated ID_WORKER(3), /** @deprecated */ @Deprecated ID_WORKER_STR(3), /** @deprecated */ @Deprecated UUID(4); private final int key; private IdType(int key) { this.key = key; } public int getKey() { return this.key; } } 更新 @Test public void testUpdate(){ User user = new User(); user.setId(5l); user.setName(\"eternal\"); user.setAge(221); user.setEmail(\"254908537@qq.com\"); int result = userMapper.updateById(user); System.out.println(result); System.out.println(user); } 自动填充 方式一：数据库级别 1、在表中新增字段 create_time, update_time 阿里开发手册数据库必备： create table `test`( `id` bigint unsigned not null auto_increment, `gmt_create` datetime null default current_timestamp, `gmt_modified` datetime null default current_timestamp on update current_timestamp, primary key(`id`) ); 或者可视化操作 2、再次测试插入方法，我们需要先把实体类同步！ private Date createTime; private Date updateTime; 方式二：代码级别 （工作时突然要对某个属性进行填充，又不能更改数据库，想要自动填充就得自己编写一套工具会自动将需要填充的sql语句拼接上去，下面为该mybatisplus编写好的工具） 1、删除数据库的默认值、更新操作！ 2、实体类字段属性上需要增加注解 //对该类的每次的插入操作，该属性都会触发一个自动填充handler处理该属性的值，此处为填充下面的Date类型 @TableField(fill = FieldFill.INSERT) private Date gmt_create; @TableField(fill = FieldFill.INSERT_UPDATE) private Date gmt_modified; 3、编写处理器来处理这个注解即可！ @Slf4j//日志 @Component// IOC加入 public class MyMetaObjectHandler implements MetaObjectHandler { @Override public void insertFill(MetaObject metaObject) { log.info(\"start insert fill..\"); this.setFieldValByName(\"gmt_create\",new Date(),metaObject); this.setFieldValByName(\"gmt_modified\",new Date(),metaObject); } @Override public void updateFill(MetaObject metaObject) { log.info(\"start update fill..\"); this.setFieldValByName(\"gmt_modified\",new Date(),metaObject); } } 乐观锁插件 主要适用场景 意图： 当要更新一条记录的时候，希望这条记录没有被别人更新 乐观锁实现方式： 取出记录时，获取当前version 更新时，带上这个version 执行更新时， set version = newVersion where version = oldVersion 如果version不对，就更新失败 乐观锁配置需要2步 记得两步 插件配置1.插件配置 spring xml: spring boot: @Bean public OptimisticLockerInterceptor optimisticLockerInterceptor() { return new OptimisticLockerInterceptor(); } 2.注解实体字段 @Version 必须要! @Version private Integer version; 特别说明: 支持的数据类型只有:int,Integer,long,Long,Date,Timestamp,LocalDateTime 整数类型下 newVersion = oldVersion + 1 newVersion 会回写到 entity 中 仅支持 updateById(id) 与 update(entity, wrapper) 方法 在 update(entity, wrapper) 方法下, wrapper 不能复用!!! Select // 测试查询 @Test public void testSelectById(){ User user = userMapper.selectById(1L); System.out.println(user); } // 测试批量查询！ @Test public void testSelectByBatchId(){ List users = userMapper.selectBatchIds(Arrays.asList(1, 2, 3)); users.forEach(System.out::println); } // 按条件查询之一使用map操作，map中的参数在sql自动转换成where条件，上面的都需要ID @Test public void testSelectByBatchIds(){ HashMap map = new HashMap<>(); // 自定义要查询 map.put(\"name\",\"狂神说Java\"); map.put(\"age\",3); List users = userMapper.selectByMap(map); users.forEach(System.out::println); } 分页查询 1、原始的 limit 进行分页 2、pageHelper 第三方插件 3、MP 其实也内置了分页插件！ 配置： //Spring boot方式 @EnableTransactionManagement @Configuration @MapperScan(\"com.baomidou.cloud.service.*.mapper*\") public class MybatisPlusConfig { @Bean public PaginationInterceptor paginationInterceptor() { PaginationInterceptor paginationInterceptor = new PaginationInterceptor(); // 设置请求的页面大于最大页后操作， true调回到首页，false 继续请求 默认false // paginationInterceptor.setOverflow(false); // 设置最大单页限制数量，默认 500 条，-1 不受限制 // paginationInterceptor.setLimit(500); // 开启 count 的 join 优化,只针对部分 left join paginationInterceptor.setCountSqlParser(new JsqlParserCountOptimize(true)); return paginationInterceptor; } } 示例： // 测试分页查询 @Test public void testPage(){ // 参数一：当前页 // 参数二：页面大小 // 使用了分页插件之后，所有的分页操作也变得简单的！ Page page = new Page<>(2,5); userMapper.selectPage(page,null); page.getRecords().forEach(System.out::println); System.out.println(page.getTotal()); } 删除操作 1、根据 id 删除记录 // 测试删除 @Test public void testDeleteById(){ userMapper.deleteById(1240620674645544965L); } // 通过id批量删除 @Test public void testDeleteBatchId(){ userMapper.deleteBatchIds(Arrays.asList(1240620674645544961L,124062067464554496 2L)); } // 通过map删除 @Test public void testDeleteMap(){ HashMap map = new HashMap<>(); map.put(\"name\",\"狂神说Java\"); userMapper.deleteByMap(map); } 逻辑删除 物理删除 ：从数据库中直接移除 逻辑删除 ：再数据库中没有被移除，而是通过一个变量来让他失效！ deleted = 0 => deleted = 1 mybatis-plus 在将属性设定为逻辑删除属性后，删除和select语句会自动注入deleted = 0在sql语句后面 说明: 只对自动注入的sql起效: 插入: 不作限制 查找: 追加where条件过滤掉已删除数据,且使用 wrapper.entity 生成的where条件会忽略该字段 更新: 追加where条件防止更新到已删除数据,且使用 wrapper.entity 生成的where条件会忽略该字段 删除: 转变为 更新 例如: 删除: update user set deleted=1 where id = 1 and deleted=0 查找: select id,name,deleted from user where deleted=0 字段类型支持说明: 支持所有数据类型(推荐使用 Integer,Boolean,LocalDateTime) 如果数据库字段使用datetime,逻辑未删除值和已删除值支持配置为字符串null,另一个值支持配置为函数来获取值如now() 附录: 逻辑删除是为了方便数据恢复和保护数据本身价值等等的一种方案，但实际就是删除。 如果你需要频繁查出来看就不应使用逻辑删除，而是以一个状态去表示。 使用方法 步骤1: 配置com.baomidou.mybatisplus.core.config.GlobalConfig$DbConfig yml: mybatis-plus: global-config: db-config: logic-delete-field: flag # 全局逻辑删除的实体字段名(since 3.3.0,配置后可以忽略不配置步骤2) logic-delete-value: 1 # 逻辑已删除值(默认为 1) logic-not-delete-value: 0 # 逻辑未删除值(默认为 0) properties: mybatis-plus.global-config.db-config.logic-delete-field=flag# 全局逻辑删除的实体字段名(since 3.3.0,配置后可以忽略不配置步骤2) mybatis-plus.global-config.db-config.logic-delete-value=1 mybatis-plus.global-config.db-config.logic-not-delete-value=0 步骤2: 实体类字段上加上@TableLogic注解 @TableLogic private Integer deleted; 性能分享插件 我们在平时的开发中，会遇到一些慢sql。测试！ druid,,,,, 作用：性能分析拦截器，用于输出每条 SQL 语句及其执行时间 MP也提供性能分析插件，如果超过这个时间就停止运行！ 1、导入插件(记住，要在SpringBoot中配置环境为dev或者 test 环境！) /** * SQL执行效率插件 */ @Bean @Profile({\"dev\",\"test\"})// 设置 dev test 环境开启，保证我们的效率 public PerformanceInterceptor performanceInterceptor() { PerformanceInterceptor performanceInterceptor = new PerformanceInterceptor(); performanceInterceptor.setMaxTime(100); // ms设置sql执行的最大时间，如果超过了则不 执行 performanceInterceptor.setFormat(true); // 是否格式化代码 return performanceInterceptor; } 条件构造器 QueryWrapper(LambdaQueryWrapper) 和 UpdateWrapper(LambdaUpdateWrapper) 的父类 用于生成 sql 的 where 条件, entity 属性也用于生成 sql 的 where 条件 注意: entity 生成的 where 条件与 使用各个 api 生成的 where 条件没有任何关联行为 代码生成器 dao、pojo、service、controller都给我自己去编写完成！ AutoGenerator 是 MyBatis-Plus 的代码生成器，通过 AutoGenerator 可以快速生成 Entity、 Mapper、Mapper XML、Service、Controller 等各个模块的代码，极大的提升了开发效率。 import com.baomidou.mybatisplus.annotation.DbType; import com.baomidou.mybatisplus.annotation.FieldFill; import com.baomidou.mybatisplus.annotation.IdType; import com.baomidou.mybatisplus.annotation.TableField; import com.baomidou.mybatisplus.generator.AutoGenerator; import com.baomidou.mybatisplus.generator.config.DataSourceConfig; import com.baomidou.mybatisplus.generator.config.GlobalConfig; import com.baomidou.mybatisplus.generator.config.PackageConfig; import com.baomidou.mybatisplus.generator.config.StrategyConfig; import com.baomidou.mybatisplus.generator.config.po.TableFill; import com.baomidou.mybatisplus.generator.config.rules.DateType; import com.baomidou.mybatisplus.generator.config.rules.NamingStrategy; import java.util.ArrayList; // 代码自动生成器 public class KuangCode { public static void main(String[] args) { // 需要构建一个 代码自动生成器 对象 AutoGenerator mpg = new AutoGenerator(); // 配置策略 // 1、全局配置 GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(\"user.dir\"); gc.setOutputDir(projectPath+\"/src/main/java\"); gc.setAuthor(\"狂神说\"); gc.setOpen(false); gc.setFileOverride(false); // 是否覆盖 gc.setServiceName(\"%sService\"); // 去Service的I前缀 gc.setIdType(IdType.ID_WORKER); gc.setDateType(DateType.ONLY_DATE); gc.setSwagger2(true); mpg.setGlobalConfig(gc); //2、设置数据源 DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(\"jdbc:mysql://localhost:3306/kuang_community? useSSL=false&useUnicode=true&characterEncoding=utf-8&serverTimezone=GMT%2B8\"); dsc.setDriverName(\"com.mysql.cj.jdbc.Driver\"); dsc.setUsername(\"root\"); dsc.setPassword(\"123456\"); dsc.setDbType(DbType.MYSQL); mpg.setDataSource(dsc); //3、包的配置 PackageConfig pc = new PackageConfig(); pc.setModuleName(\"blog\"); pc.setParent(\"com.kuang\"); pc.setEntity(\"entity\"); pc.setMapper(\"mapper\"); pc.setService(\"service\"); pc.setController(\"controller\"); mpg.setPackageInfo(pc); //4、策略配置 StrategyConfig strategy = new StrategyConfig(); strategy.setInclude(\"blog_tags\",\"course\",\"links\",\"sys_settings\",\"user_record\",\" user_say\"); // 设置要映射的表名 strategy.setNaming(NamingStrategy.underline_to_camel); strategy.setColumnNaming(NamingStrategy.underline_to_camel); strategy.setEntityLombokModel(true); // 自动lombok； strategy.setLogicDeleteFieldName(\"deleted\"); // 自动填充配置 TableFill gmtCreate = new TableFill(\"gmt_create\", FieldFill.INSERT); TableFill gmtModified = new TableFill(\"gmt_modified\", FieldFill.INSERT_UPDATE); ArrayList tableFills = new ArrayList<>(); tableFills.add(gmtCreate); tableFills.add(gmtModified); strategy.setTableFillList(tableFills); // 乐观锁 strategy.setVersionFieldName(\"version\"); strategy.setRestControllerStyle(true); strategy.setControllerMappingHyphenStyle(true); // localhost:8080/hello_id_2 mpg.setStrategy(strategy); mpg.execute(); //执行 } } powered by Gitbook该文件最后修改时间： 2020-08-01 17:31:32 "},"数据库/mysql.html":{"url":"数据库/mysql.html","title":"Mysql","keywords":"","body":"范式 https://blog.csdn.net/wenco1/article/details/88077279 码 设K为R中的属性或属性组合，若K-(F)>U,则K为R的候选码。 注意U是完全函数依赖与K，而不是部分函数依赖于K。如果U部分函数依赖于K，即K-（P）>U，则K称为超码。候选码是最小的超码，即K的任意一个真子集都不是候选码 若候选码多于一个，则选定其中一个为主码 包含在任意一个候选码中的属性称为主属性；不包含在任何候选码中的属性称为非主属性，最简单的情况，单个属性是码；最极端的情况，整个属性组是码，称为全码。 第一范式 1NF的定义为：符合1NF的关系中的每个属性都不可再分 即每个属性每一行只能有一个值（多种但只有一个） 第二范式 2NF在1NF的基础之上，消除了非主属性对于码的部分函数依赖。 完全函数依赖 在一张表中，若 X → Y，且对于 X 的任何一个真子集（假如属性组 X 包含超过一个属性的话），X ’ → Y 不成立，那么我们称 Y 对于 X 完全函数依赖，记作 X F→ Y。（那个F应该写在箭头的正上方，没办法打出来……，正确的写法如图1） 图1 例如： 学号 F→ 姓名 （学号，课名） F→ 分数 （注：因为同一个的学号对应的分数不确定，同一个课名对应的分数也不确定） 2.1.2 部分函数依赖 假如 Y 函数依赖于 X，但同时 Y 并不完全函数依赖于 X，那么我们就称 Y 部分函数依赖于 X，记作 X P→ Y，如图2。 图2 2.1.2 传递函数依赖 例如：（学号，课名） P→ 姓名 传递函数依赖假如 Z 函数依赖于 Y，且 Y 函数依赖于 X （感谢 @百达 指出的错误，这里改为：『Y 不包含于 X，且 X 不函数依赖于 Y』这个前提），那么我们就称 Z 传递函数依赖于 X ，记作 X T→ Z，如图3。 第三范式 3NF在2NF的基础之上，消除了非主属性对于码的传递函数依赖。 BCNF 在 3NF 的基础上消除主属性对于不包含它的码的部分与传递函数依赖。（一般出现在由于1：1的方式出现的多个候选码的情况） sql函数 mysql_connect() mysql_connect() 函数打开非持久的 MySQL 连接。 mysql_connect(server,user,pwd,newlink,clientflag) mysql_pconnect() mysql_pconnect() 函数打开一个到 MySQL 服务器的持久连接。 mysql_pconnect() 和 mysql_connect() 非常相似，但有两个主要区别： 当连接的时候本函数将先尝试寻找一个在同一个主机上用同样的用户名和密码已经打开的（持久）连接，如果找到，则返回此连接标识而不打开新连接。 其次，当脚本执行完毕后到 SQL 服务器的连接不会被关闭，此连接将保持打开以备以后使用（mysql_close() 不会关闭由 mysql_pconnect() 建立的连接）。 mysql_pconnect(server,user,pwd,clientflag) sql语句 case when end --简单case函数 case sex when '1' then '男' when '2' then '女’ else '其他' end --case搜索函数 case when sex = '1' then '男' when sex = '2' then '女' else '其他' end 别名 执行顺序 select语句完整语法： 1) select 目标表的列名或列表达式序列 2) from 基本表名和（或）视图序列 3) [where 行条件表达式] 4) [group by 列名序列] [having 组条件表达式] 5) [order by 列名[asc | desc]]，则sql语句的执行顺序是： 写法顺序：select--from--where--group by--having--order by 执行顺序：from--where--group by--having--select--order by count 除了count(1)与count(*)，count（column）会忽略值为空的 group by where子句 = 指定行所对应的条件 having子句 = 指定组所对应的条件 D中是Group by才用来分组的，group by的作用是限定分组条件，而having则是对group by中分出来的组进行条件筛选。 所以用having就一定要和group by连用，且是先group by XXX 再having XXX，用group by不一有having（它只是一个筛选条件用的） isnull与ifnull与nullif mysql中： 1.isnull(exper) 判断exper是否为空，是则返回1，否则返回0 2.ifnull(exper1,exper2)判断exper1是否为空，是则用exper2代替 3.nullif(exper1,exper2)如果expr1= expr2 成立，那么返回值为NULL，否则返回值为 expr1。 like 字段 like ‘匹配串1’or 字段 like ‘匹配串2’or ... 有如下简写方式 oracle： select * from tablex where REGEXP_LIKE(字段名, '(匹配串1|匹配串2|...)') ;//全模糊匹配 select * from tablex where REGEXP_LIKE(字段名, '^(匹配串1|匹配串2|...)') \";//右模糊匹配 select * from tablex where REGEXP_LIKE(字段名, '(匹配串1|匹配串2|...)$') \";//左模糊匹配 SqlServer select * from tablex where f1 like '%[匹配串1|匹配串2|匹配串3]%' UNION和UNION ALL UNION 并集，两个表中的所有的数据项，并且去除重复数据，按默认排序方式排序（工作中主要用到的是这个）； UNION ALL，两个表表中的所有都罗列出来； between和时间 写时间条件 ，比如 把2014/3/1 到2014/3/31这个时间段做为条件 的话，很多人都会写成这样 select date from table where date between '2014/3/1' and '2014/3/31' 其实这样查询出来的结果 是从2014/3/1 00:00:00 到 2014/3/31 00:00:00 那么 问题来了，要是其中正好有一条数据的时间在 '2014/3/31 11:12:12'那样的话，会出现什么结果 呢？很明显的，这条数据 就会被忽略掉。所在在查询关于时间的时候 ，千万要记住 时： 分： 秒， 如查询三月份的数据正确的语句应该是 select date from table where date between '2014/3/1' and ‘2014/3/1 23:59:59’ B树与B+树 B树 每个节点都存储key和data，所有节点组成这棵树，并且叶子节点指针为null。 B+树 只有叶子节点存储data，叶子节点包含了这棵树的所有键值，叶子节点不存储指针。 后来，在B+树上增加了顺序访问指针，也就是每个叶子节点增加一个指向相邻叶子节点的指针，这样一棵树成了数据库系统实现索引的首选数据结构。 原因有很多，最主要的是这棵树矮胖，呵呵。一般来说，索引很大，往往以索引文件的形式存储的磁盘上，索引查找时产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的时间复杂度。树高度越小，I/O次数越少。 那为什么是B+树而不是B树呢，因为它内节点不存储data，这样一个节点就可以存储更多的key。 在MySQL中，最常用的两个存储引擎是MyISAM和InnoDB，它们对索引的实现方式是不同的。 MyISAM与InnoDB 目前大部分数据库系统及文件系统都采用B-Tree(B树)或其变种B+Tree(B+树)作为索引结构。B+Tree是数据库系统实现索引的首选数据结构。在MySQL中,索引属于存储引擎级别的概念,不同存储引擎对索引的实现方式是不同的,本文主要讨论MyISAM和InnoDB两个存储引擎的索引实现方式。MyISAM索引实现MyISAM引擎使用B+Tree作为索引结构,叶节点的data域存放的是数据记录的地址。下图是MyISAM索引的原理图:image.png这里设表一共有三列,假设我 ​ 在 MySQL 中,索引属于存储引擎级别的概念,不同存储引擎对索引的实现方式是不同的,本文主要讨论 MyISAM 和 InnoDB 两个存储引擎的索引实现方式。 主键和唯一索引的区别： 主键是一种约束，唯一索引是一种索引，两者在本质上是不同的。 主键创建后一定包含一个唯一性索引，唯一性索引并不一定就是主键。 唯一性索引列允许空值，而主键列不允许为空值。 主键列在创建时，已经默认为非空值 + 唯一索引了。 主键可以被其他表引用为外键，而唯一索引不能。 一个表最多只能创建一个主键，但可以创建多个唯一索引。 主键和唯一索引都可以有多列。 主键更适合那些不容易更改的唯一标识，如自动递增列、身份证号等。 在 RBO 模式下，主键的执行计划优先级要高于唯一索引。 两者可以提高查询的速度。 主键索引、覆盖索引、联合索引 若是我们的表有主键，会自动生成主键索引树，并且主键索引表的叶节点保存有全部数据 若是无法用主键索引，而采取了普通索引，若是我们建立了条件字段的索引树（默认会包括主键，此时叶子节点只包括我们此时建立的索引的字段和主键），那我们查询时就可以通过索引树查询到相应叶子节点，此时若是叶子节点中包含需要查询的字段，则不用回表而是直接在索引树查询，若是没有则需要回表查询 若是查询条件是联合索引，其也会走索引树，联合索引的查询也能调用覆盖索引的功能 MyISAM 索引实现 MyISAM 引擎使用 B+Tree 作为索引结构,叶节点的 data 域存放的是数据记录的地址。下图是 MyISAM 索引的原理图: 这里设表一共有三列,假设我们以 Col1 为主键,则图 8 是一个 MyISAM 表的主索引(Primary key)示意。可以看出 MyISAM 的索引文件仅仅保存数据记录的地址。 辅助索引 在 MyISAM 中,主索引和辅助索引(Secondary key)在结构上没有任何区别,只是主索引要求 key 是唯一的,而辅助索引的 key 可以重复。如果我们在 Col2 上建立一个辅助索引,则此索引的结构如下图所示 同样也是一颗 B+Tree,data 域保存数据记录的地址。因此,MyISAM 中索引检索的算法为首先按照 B+Tree 搜索算法搜索索引,如果指定的 Key 存在,则取出其data 域的值,然后以 data 域的值为地址,读取相应数据记录。 MyISAM 的索引方式也叫做“非聚集索引”,之所以这么称呼是为了与 InnoDB的聚集索引区分。 InnoDB 索引实现 虽然 InnoDB 也使用 B+Tree 作为索引结构,但具体实现方式却与 MyISAM 截然不同。 1.第一个重大区别是 InnoDB 的数据文件本身就是索引文件。从上文知道,MyISAM 索引文件和数据文件是分离的,索引文件仅保存数据记录的地址。 而在InnoDB 中,表数据文件本身就是按 B+Tree 组织的一个索引结构,这棵树的叶点data 域保存了完整的数据记录。这个索引的 key 是数据表的主键,因此 InnoDB 表数据文件本身就是主索引。 上图是 InnoDB 主索引(同时也是数据文件)的示意图,可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为 InnoDB 的数据文件本身要按主键聚集, 1 .InnoDB 要求表必须有主键(MyISAM 可以没有),如果没有显式指定,则 MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键,如果不存在这种列,则MySQL 自动为 InnoDB 表生成一个隐含字段作为主键,类型为长整形。 每个InnoDB的表都拥有一个特殊索引，此索引中存储着行记录（称之为聚簇索引Clustered Index），一般来说，聚簇索引是根据主键生成的。聚簇索引按照如下规则创建： 当定义了主键后，InnoDB会利用主键来生成其聚簇索引； 如果没有主键，InnoDB会选择一个非空的唯一索引来创建聚簇索引； 如果这也没有，InnoDB会隐式的创建一个自增的列(rowid)来作为聚簇索引。 除了主键索引之外的索引，成为二级索引（Secondary Index）。二级索引可以有多个，二级索引建立在经常查询的列上。与聚簇索引的区别在于二级索引的叶子节点中存放的是除了这几个列外用来回表的主键信息（指针）。 同时,请尽量在 InnoDB 上采用自增字段做表的主键。因为 InnoDB 数据文件本身是一棵B+Tree,非单调的主键会造成在插入新记录时数据文件为了维持 B+Tree 的特性而频繁的分裂调整,十分低效,而使用自增字段作为主键则是一个很好的选择。如果表使用自增主键,那么每次插入新的记录,记录就会顺序添加到当前索引节点的后续位置,当一页写满,就会自动开辟一个新的页。如下图所示: 在维护索引上。 2.第二个与 MyISAM 索引的不同是 InnoDB 的辅助索引 data 域存储相应记录主键的值而不是地址。换句话说,InnoDB 的所有辅助索引都引用主键作为 data 域。 例如,图 11 为定义在 Col3 上的一个辅助索引: 聚集索引这种实现方式使得按主键的搜索十分高效,但是辅助索引搜索需要检索两遍索引:首先检索辅助索引获得主键,然后用主键到主索引中检索获得记录。 引申:为什么不建议使用过长的字段作为主键? 因为所有辅助索引都引用主索引,过长的主索引会令辅助索引变得过大。 聚簇索引与非聚簇索引 InnoDB 使用的是聚簇索引, 将主键组织到一棵 B+树中, 而行数据就储存在叶子节点上, 若使用\"where id = 14\"这样的条件查找主键, 则按照 B+树的检索算法即可查找到对应的叶节点, 之后获得行数据。 若对 Name 列进行条件搜索, 则需要两个步骤: 第一步在辅助索引 B+树中检索 Name, 到达其叶子节点获取对应的主键。 第二步使用主键在主索引 B+树种再执行一次 B+树检索操作, 最终到达叶子节点即可获取整行数据。 MyISM 使用的是非聚簇索引, 非聚簇索引的两棵 B+树看上去没什么不同, 节点 的结构完全一致只是存储的内容不同而已, 主键索引 B+树的节点存储了主键, 辅助键索引B+树存储了辅助键。 表数据存储在独立的地方, 这两颗 B+树的叶子节点都使用一个地址指向真正的表数据, 对于表数据来说, 这两个键没有任何差别。 由于索引树是独立的, 通过辅助键检索无需访问主键的索引树。 为了更形象说明这两种索引的区别, 我们假想一个表如下图存储了 4 行数据。 其中Id 作为主索引, Name 作为辅助索引。 图示清晰的显示了聚簇索引和非聚簇索引的差异 联合索引及最左原则 联合索引存储数据结构图： 最左原则： 例如联合索引有三个索引字段（A,B,C） 建立了联合索引（A,B,C）实际上等同于建立了（A）,(A,B),(A,B,C)三个索引 查询条件： （A，，）---会使用索引 （A，B，）---会使用索引 （A，B，C）---会使用索引 （，B，C）---不会使用索引 （，，C）---不会使用索引 （A，，C）---A会使用索引，因为B中断，C全表扫描，不使用索引 最左匹配原则 假定上图联合索引的为（a,b）。联合索引也是一棵B+树，不同的是B+树在对索引a排序的基础上，对索引b排序。所以数据按照（1,1),(1,2)......顺序排放。 对于selete * from table where a=XX and b=XX，显然是可以使用(a,b)联合索引的， 对于selete * from table where a=XX，也是可以使用(a,b)联合索引的。因为在这两种情况下，叶子节点中的数据都是有序的。 但是，对于b列的查询，selete * from table where b=XX。则不可以使用这棵B+树索引。可以发现叶子节点的b值为1,2,1,4,1,2。显然不是有序的，因此不能使用(a,b)联合索引。 By the way:selete * from table where b=XX and a=XX,也是可以使用到联合索引的，你可能会有疑问，这条语句并不符合最左匹配原则。这是由于查询优化器的存在，mysql查询优化器会判断纠正这条sql语句该以什么样的顺序执行效率最高，最后才生成真正的执行计划。所以，当然是我们能尽量的利用到索引时的查询顺序效率最高咯，所以mysql查询优化器会最终以这种顺序进行查询执行。 优化：在联合索引中将选择性最高的列放在索引最前面。 例如：在一个公司里以age 和gender为索引，显然age要放在前面，因为性别就两种选择男或女，选择性不如age。 使用联合索引时，若有>或 如：where a = 3 and b > 4 and c = 5 使用a 和 b索引，c不能在范围之后，c不使用索引 使用like的模糊查询时，把%放前面，并不走索引，把%放关键字后面，还是会走索引的 where a = 3 and b like’kk%’ and c = 4 ，使用a、b、c三个索引 where a = 3 and b like ‘%kk’ and c = 4，使用a索引 where a = 3 and b like ‘%kk%’ and c = 4, 使用a索引 where a = 3 and b like ‘k%kk%’ 使用 a、b、c 三个索引 速率 对于innodb的B+树索引来说，所有的行数据都放在叶子节点，非叶子节点不存储行数据，是为了存储更多的索引建，从而降低B+树的高度，减少IO的次数。所以一般来说高度为3的树就可以存储千万级别的数据。 现在来回答问题： ·假设现在有一个高度为3的B+树，当我们查找数据时，会先发生第一次IO加载高度为1的磁盘块1到内存，然后在内存中二分查找得到下次需要加载的高度为2的磁盘块2的地址，然后再发生第二次IO加载磁盘块2到内存中再比较得到高度为3的磁盘块3的地址，再发生第三次IO加载磁盘3到内存比较就可以得到数据了。 ·mysql加载索引是以磁盘块（页）为单位的，并不是一次性全部加载到内存中，每次只需要加载需要的那一块。然后一般高度为3的树就可以存储千万级别的数据 ，所以一般只需要3次IO就可以得到数据，速度就很快了。 区别： InnoDB支持事务，MyISAM不支持，对于InnoDB每一条SQL语言都默认封装成事务，自动提交，这样会影响速度，所以最好把多条SQL语言放在begin和commit之间，组成一个事务； InnoDB支持外键，而MyISAM不支持。对一个包含外键的InnoDB表转为MYISAM会失败； InnoDB是聚集索引，使用B+Tree作为索引结构，数据文件是和（主键）索引绑在一起的（表数据文件本身就是按B+Tree组织的一个索引结构），必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。 MyISAM是非聚集索引，也是使用B+Tree作为索引结构，索引和数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 也就是说：InnoDB的B+树主键索引的叶子节点就是数据文件，辅助索引的叶子节点是主键的值；而MyISAM的B+树主键索引和辅助索引的叶子节点都是数据文件的地址指针。 InnoDB不保存表的具体行数，执行select count(*) from table时需要全表扫描。而MyISAM用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快（注意不能加有任何WHERE条件）； 那么为什么InnoDB没有了这个变量呢？ 因为InnoDB的事务特性，在同一时刻表中的行数对于不同的事务而言是不一样的，因此count统计会计算对于当前事务而言可以统计到的行数，而不是将总行数储存起来方便快速查询。InnoDB会尝试遍历一个尽可能小的索引除非优化器提示使用别的索引。如果二级索引不存在，InnoDB还会尝试去遍历其他聚簇索引。 如果索引并没有完全处于InnoDB维护的缓冲区（Buffer Pool）中，count操作会比较费时。可以建立一个记录总行数的表并让你的程序在INSERT/DELETE时更新对应的数据。和上面提到的问题一样，如果此时存在多个事务的话这种方案也不太好用。如果得到大致的行数值已经足够满足需求可以尝试SHOW TABLE STATUS Innodb不支持全文索引，而MyISAM支持全文索引，在涉及全文索引领域的查询效率上MyISAM速度更快高；PS：5.7以后的InnoDB支持全文索引了 MyISAM表格可以被压缩后进行查询操作 InnoDB支持表、行(默认)级锁，而MyISAM支持表级锁 InnoDB的行锁是实现在索引上的，而不是锁在物理行记录上。潜台词是，如果访问没有命中索引，也无法使用行锁，将要退化为表锁。 例如： t_user(uid, uname, age, sex) innodb; uid PK 无其他索引 update t_user set age=10 where uid=1; 命中索引，行锁。 update t_user set age=10 where uid != 1; 未命中索引，表锁。 update t_user set age=10 where name='chackca'; 无索引，表锁。 8、InnoDB表必须有主键（用户没有指定的话会自己找或生产一个主键），而Myisam可以没有 9、Innodb存储文件有frm、ibd，而Myisam是frm、MYD、MYI Innodb：frm是表定义文件，ibd是数据文件 Myisam：frm是表定义文件，myd是数据文件，myi是索引文件 如何选择： 1. 是否要支持事务，如果要请选择innodb，如果不需要可以考虑MyISAM； 2. 如果表中绝大多数都只是读查询，可以考虑MyISAM，如果既有读也有写，请使用InnoDB。 3. 系统奔溃后，MyISAM恢复起来更困难，能否接受； 4. MySQL5.5版本开始Innodb已经成为Mysql的默认引擎(之前是MyISAM)，说明其优势是有目共睹的，如果你不知道用什么，那就用InnoDB，至少不会差。 InnoDB为什么推荐使用自增ID作为主键？ 答：自增ID可以保证每次插入时B+索引是从右边扩展的，可以避免B+树和频繁合并和分裂（对比使用UUID）。如果使用字符串主键和随机主键，会使得数据随机插入，效率比较差。 innodb引擎的4大特性 插入缓冲（insert buffer),二次写(double write),自适应哈希索引(ahi),预读(read ahead) 四类事务 事务的 四个特征（ACID） 事务具有四个特征：原子性（ Atomicity ）、一致性（ Consistency ）、隔离性（ Isolation ）和持续性（ Durability ）。这四个特性简称为 ACID 特性。 1 、原子性。事务是数据库的逻辑工作单位，事务中包含的各操作要么都做，要么都不做 2 、一致性。事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。因此当数据库只包含成功事务提交的结果时，就说数据库处于一致性状态。 物理方面：如果数据库系统运行中发生故障，有些事务尚未完成就被迫中断，这些未完成事务对数据库所做的修改有一部分已写入物理数据库，这时数据库就处于一种不正确的状态，或者说是不一致的状态。（持续性 ） 程序方面： 单线程情况下，事务要么一起提交要么全部不做（原子性） ​ 高并发条件下，不同事物最终的处理结果要符合逻辑的一致性（隔离性） 3 、隔离性。一个事务的执行不能其它事务干扰。即一个事务内部的操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务之间不能互相干扰。 4 、持续性。也称永久性，指一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的。接下来的其它操作或故障不应该对其执行结果有任何影响。 一致性是基础，也是最终目的，其他三个特性（原子性、隔离性和持久性）都是为了保证一致性的 在比较简单的场景（没有高并发）下，可能会发生一些数据库崩溃等情况，这个时候，依赖于对日志的 REDO/UNDO 操作就可以保证一致性 而在比较复杂的场景（有高并发）下，可能会有很多事务并行的执行，这个时候，就很可能导致最终的结果无法保证一致性，比如 事务1需要将100元转入帐号A：先读取帐号A的值，然后在这个值上加上100。但是，在这两个操作之间，另一个事务2修改了帐号A的值，为它增加了100元。那么最后的结果应该是A增加了200元。但事实上，事务1最终完成后，帐号A只增加了100元，因为事务2的修改结果被事务1覆盖掉了。 这个时候，原子性不能保证一致性。因为从单个事务的角度看，不管是事务 1 还是事务 2，它们都保证的原子性（单个事务内的所有操作全部成功了），但最终，它们并没有保证数据库的一致性（因为从逻辑上说，账户 A 应该增加了 200 元，而不是 100 元） 所以，为了保证并发情况下的一致性，又引入了隔离性的概念 隔离性：即事务之间感知不到彼此的存在，就好像只存在本身一个事务一样 而对于怎样实现隔离性，又涉及到了乐观锁和悲观锁的概念 不考虑隔离性的时候，可能导致脏读、幻读和不可重复读的问题（这些问题，其实就是导致无法保证一致性的几种情况） 而隔离级别的概念，就是为了解决上述三个问题 SQL Server中事务被分为3类常见的事务： 自动提交事务：是SQL Server默认的一种事务模式，每条Sql语句都被看成一个事务进行处理，你应该没有见过，一条Update 修改2个字段的语句，只修该了1个字段而另外一个字段没有修改。。 显式事务：T-sql标明，由Begin Transaction开启事务开始，由Commit Transaction 提交事务、Rollback Transaction 回滚事务结束。 隐式事务：使用Set IMPLICIT_TRANSACTIONS ON 将将隐式事务模式打开，不用Begin Transaction开启事务，当一个事务结束，这个模式会自动启用下一个事务，只用Commit Transaction 提交事务、Rollback Transaction 回滚事务即可。 常用语句就四个。 Begin Transaction：标记事务开始。 Commit Transaction：事务已经成功执行，数据已经处理妥当。 Rollback Transaction：数据处理过程中出错，回滚到没有处理之前的数据状态，或回滚到事务内部的保存点。 Save Transaction：事务内部设置的保存点，就是事务可以不全部回滚，只回滚到这里，保证事务内部不出错的前提下。 Mysql的四种隔离级别 SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。 Read Uncommitted（读取未提交内容/未提交读） 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 Read Committed（读取提交内容/提交读） 这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别会导致所谓的不可重复读（Nonrepeatable Read）现象，如果一个用户在一个事务中多次读取一条数据，而另外一个用户则同时更新啦这条数据，造成第一个用户多次读取数据不一致。 Repeatable Read（可重读） 这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，它也存在另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指第一个事务读取一个结果集后，第二个事务，对这个结果集经行增删操作（第一个事务暂时没用到，没添加行锁），然而第一个事务中再次对这个结果集进行查询时，数据发现丢失或新增，出现了“幻影”。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。 Serializable（可串行化） 这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 出现问题 这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如： 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。（一个事务中的更新操作在提交时才写入数据库中，未Commit时在副本操作，持续性，更高隔离级的处理方式也都兼容低级的） 不可重复读(Non-repeatable read):事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果不一致。主要针对的是修改的情况，用到行锁（使用共享锁/排他锁都可以，低隔离性） 幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。侧重于新增和删除，需用到表锁，如间隙锁（高隔离性） 在MySQL中，实现了这四种隔离级别，分别有可能产生问题如下所示： 锁机制 共享锁与排他锁 共享锁也是基于信号量的读者与写者问题为基础的对互斥和共享资源的更高的级别的抽象 共享锁（读锁）：其他事务可以读，但不能写。 排他锁（写锁） ：其他事务不能读取，也不能写。 粒度锁 MySQL 不同的存储引擎支持不同的锁机制，所有的存储引擎都以自己的方式显现了锁机制，服务器层完全不了解存储引擎中的锁实现： MyISAM 和 MEMORY 存储引擎采用的是表级锁（table-level locking） BDB 存储引擎采用的是页面锁（page-level locking），但也支持表级锁 InnoDB 存储引擎既支持行级锁（row-level locking），也支持表级锁，但默认情况下是采用行级锁。 默认情况下，表锁和行锁都是自动获得的， 不需要额外的命令。 但是在有的情况下， 用户需要明确地进行锁表或者进行事务的控制， 以便确保整个事务的完整性，这样就需要使用事务控制和锁定语句来完成。 不同粒度锁的比较： 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。 这些存储引擎通过总是一次性同时获取所有需要的锁以及总是按相同的顺序获取表锁来避免死锁。 表级锁更适合于以查询为主，并发用户少，只有少量按索引条件更新数据的应用，如Web 应用 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 最大程度的支持并发，同时也带来了最大的锁开销。 在 InnoDB 中，除单个 SQL 组成的事务外， 锁是逐步获得的，这就决定了在 InnoDB 中发生死锁是可能的。 行级锁只在存储引擎层实现，而Mysql服务器层没有实现。 行级锁更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理（OLTP）系统 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 MyISAM 表锁 MyISAM表级锁模式： 表共享读锁 （Table Read Lock）：不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求； 表独占写锁 （Table Write Lock）：会阻塞其他用户对同一表的读和写操作； MyISAM 表的读操作与写操作之间，以及写操作之间是串行的。当一个线程获得对一个表的写锁后， 只有持有锁的线程可以对表进行更新操作。 其他线程的读、 写操作都会等待，直到锁被释放为止。 默认情况下，写锁比读锁具有更高的优先级：当一个锁释放时，这个锁会优先给写锁队列中等候的获取锁请求，然后再给读锁队列中等候的获取锁请求。 （This ensures that updates to a table are not “starved” even when there is heavy SELECT activity for the table. However, if there are many updates for a table, SELECT statements wait until there are no more updates.）。 这也正是 MyISAM 表不太适合于有大量更新操作和查询操作应用的原因，因为，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。同时，一些需要长时间运行的查询操作，也会使写线程“饿死” ，应用中应尽量避免出现长时间运行的查询操作（在可能的情况下可以通过使用中间表等措施对SQL语句做一定的“分解” ，使每一步查询都能在较短时间完成，从而减少锁冲突。如果复杂查询不可避免，应尽量安排在数据库空闲时段执行，比如一些定期统计可以安排在夜间执行）。 可以设置改变读锁和写锁的优先级： 通过指定启动参数low-priority-updates，使MyISAM引擎默认给予读请求以优先的权利。 通过执行命令SET LOW_PRIORITY_UPDATES=1，使该连接发出的更新请求优先级降低。 通过指定INSERT、UPDATE、DELETE语句的LOW_PRIORITY属性，降低该语句的优先级。 给系统参数max_write_lock_count设置一个合适的值，当一个表的读锁达到这个值后，MySQL就暂时将写请求的优先级降低，给读进程一定获得锁的机会。 MyISAM加表锁方法： MyISAM 在执行查询语句（SELECT）前，会自动给涉及的表加读锁，在执行更新操作 （UPDATE、DELETE、INSERT 等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用 LOCK TABLE 命令给 MyISAM 表显式加锁。 在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，这也正是 MyISAM 表不会出现死锁（Deadlock Free）的原因。 MyISAM存储引擎支持并发插入，以减少给定表的读和写操作之间的争用： 如果MyISAM表在数据文件中间没有空闲块，则行始终插入数据文件的末尾。 在这种情况下，你可以自由混合并发使用MyISAM表的INSERT和SELECT语句而不需要加锁——你可以在其他线程进行读操作的时候，同时将行插入到MyISAM表中。 文件中间的空闲块可能是从表格中间删除或更新的行而产生的。 如果文件中间有空闲快，则并发插入会被禁用，但是当所有空闲块都填充有新数据时，它又会自动重新启用。 要控制此行为，可以使用MySQL的concurrent_insert系统变量。 如果你使用LOCK TABLES显式获取表锁，则可以请求READ LOCAL锁而不是READ锁，以便在锁定表时，其他会话可以使用并发插入。 当concurrent_insert设置为0时，不允许并发插入。 当concurrent_insert设置为1时，如果MyISAM表中没有空洞（即表的中间没有被删除的行），MyISAM允许在一个线程读表的同时，另一个线程从表尾插入记录。这也是MySQL的默认设置。 当concurrent_insert设置为2时，无论MyISAM表中有没有空洞，都允许在表尾并发插入记录。 查询表级锁争用情况： 可以通过检查 table_locks_waited 和 table_locks_immediate 状态变量来分析系统上的表锁的争夺，如果 Table_locks_waited 的值比较高，则说明存在着较严重的表级锁争用情况： mysql> SHOW STATUS LIKE 'Table%'; +-----------------------+---------+ | Variable_name | Value | +-----------------------+---------+ | Table_locks_immediate | 1151552 | | Table_locks_waited | 15324 | +-----------------------+---------+ InnoDB行级锁和表级锁 InnoDB锁模式： InnoDB 实现了以下两种类型的行锁： 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 排他锁（X）：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。 为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁： 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的 IX 锁。 锁模式的兼容情况： （如果一个事务请求的锁模式与当前的锁兼容， InnoDB 就将请求的锁授予该事务； 反之， 如果两者不兼容，该事务就要等待锁释放。） InnoDB加锁方法： 意向锁是 InnoDB 自动加的， 不需用户干预。 对于 UPDATE、 DELETE 和 INSERT 语句， InnoDB 会自动给涉及数据集加排他锁（X)； 对于普通 SELECT 语句，InnoDB 不会加任何锁； 事务可以通过以下语句显式给记录集加共享锁或排他锁： 共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE。 其他 session 仍然可以查询记录，并也可以对该记录加 share mode 的共享锁。但是如果当前事务需要对该记录进行更新操作，则很有可能造成死锁。 排他锁（X)：SELECT * FROM table_name WHERE ... FOR UPDATE。其他 session 可以查询该记录，但是不能对该记录加共享锁或排他锁，而是等待获得锁 InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！ 隐式锁定： InnoDB在事务执行过程中，使用两阶段锁协议： 随时都可以执行锁定，InnoDB会根据隔离级别在需要的时候自动加锁； 锁只有在执行commit或者rollback的时候才会释放，并且所有的锁都是在同一时刻被释放。 显式锁定 ： select ... lock in share mode //共享锁 select ... for update //排他锁 select for update： 在执行这个 select 查询语句的时候，会将对应的索引访问条目进行上排他锁（X 锁），也就是说这个语句对应的锁就相当于update带来的效果。 select * for update 的使用场景：为了让自己查到的数据确保是最新数据，并且查到后的数据只允许自己来修改的时候，需要用到 for update 子句。 select lock in share mode ：in share mode 子句的作用就是将查找到的数据加上一个 share 锁，这个就是表示其他的事务只能对这些数据进行简单的select 操作，并不能够进行 DML 操作。select * lock in share mode 使用场景：为了确保自己查到的数据没有被其他的事务正在修改，也就是说确保查到的数据是最新的数据，并且不允许其他人来修改数据。但是自己不一定能够修改数据，因为有可能其他的事务也对这些数据 使用了 in share mode 的方式上了 S 锁。 性能影响： select for update 语句，相当于一个 update 语句。在业务繁忙的情况下，如果事务没有及时的commit或者rollback 可能会造成其他事务长时间的等待，从而影响数据库的并发使用效率。 select lock in share mode 语句是一个给查找的数据上一个共享锁（S 锁）的功能，它允许其他的事务也对该数据上S锁，但是不能够允许对该数据进行修改。如果不及时的commit 或者rollback 也可能会造成大量的事务等待。 for update 和 lock in share mode 的区别： 前一个上的是排他锁（X 锁），一旦一个事务获取了这个锁，其他的事务是没法在这些数据上执行 for update ；后一个是共享锁，多个事务可以同时的对相同数据执行 lock in share mode。 InnoDB 行锁实现方式： InnoDB 行锁是通过给索引上的索引项加锁来实现的，这一点 MySQL 与 Oracle 不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB 这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB 才使用行级锁，否则，InnoDB 将使用表锁！ 不论是使用主键索引、唯一索引或普通索引，InnoDB 都会使用行锁来对数据加锁。 只有执行计划真正使用了索引，才能使用行锁：即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。因此，在分析锁冲突时， 别忘了检查 SQL 的执行计划（可以通过 explain 检查 SQL 的执行计划），以确认是否真正使用了索引。（更多阅读：MySQL索引总结） 由于 MySQL 的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然多个session是访问不同行的记录， 但是如果是使用相同的索引键， 是会出现锁冲突的（后使用这些索引的session需要等待先使用索引的session释放锁后，才能获取锁）。 应用设计的时候要注意这一点。 InnoDB的间隙锁： 当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）。 很显然，在使用范围条件检索并锁定记录时，InnoDB这种加锁机制会阻塞符合条件范围内键值的并发插入，这往往会造成严重的锁等待。因此，在实际应用开发中，尤其是并发插入比较多的应用，我们要尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件。 InnoDB使用间隙锁的目的： 防止幻读，以满足相关隔离级别的要求； 满足恢复和复制的需要： MySQL 通过 BINLOG 录入执行成功的 INSERT、UPDATE、DELETE 等更新数据的 SQL 语句，并由此实现 MySQL 数据库的恢复和主从复制。MySQL 的恢复机制（复制其实就是在 Slave Mysql 不断做基于 BINLOG 的恢复）有以下特点： 一是 MySQL 的恢复是 SQL 语句级的，也就是重新执行 BINLOG 中的 SQL 语句。 二是 MySQL 的 Binlog 是按照事务提交的先后顺序记录的， 恢复也是按这个顺序进行的。 由此可见，MySQL 的恢复机制要求：在一个事务未提交前，其他并发事务不能插入满足其锁定条件的任何记录，也就是不允许出现幻读。 InnoDB 在不同隔离级别下的一致性读及锁的差异： 锁和多版本数据（MVCC）是 InnoDB 实现一致性读和 ISO/ANSI SQL92 隔离级别的手段。 因此，在不同的隔离级别下，InnoDB 处理 SQL 时采用的一致性读策略和需要的锁是不同的： 对于许多 SQL，隔离级别越高，InnoDB 给记录集加的锁就越严格（尤其是使用范围条件的时候），产生锁冲突的可能性也就越高，从而对并发性事务处理性能的 影响也就越大。 因此， 我们在应用中， 应该尽量使用较低的隔离级别， 以减少锁争用的机率。实际上，通过优化事务逻辑，大部分应用使用 Read Commited 隔离级别就足够了。对于一些确实需要更高隔离级别的事务， 可以通过在程序中执行 SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ 或 SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE 动态改变隔离级别的方式满足需求。 获取 InnoDB 行锁争用情况： 可以通过检查 InnoDB_row_lock 状态变量来分析系统上的行锁的争夺情况： mysql> show status like 'innodb_row_lock%'; +-------------------------------+-------+ | Variable_name | Value | +-------------------------------+-------+ | InnoDB_row_lock_current_waits | 0 | | InnoDB_row_lock_time | 0 | | InnoDB_row_lock_time_avg | 0 | | InnoDB_row_lock_time_max | 0 | | InnoDB_row_lock_waits | 0 | +-------------------------------+-------+ 5 rows in set (0.01 sec) LOCK TABLES 和 UNLOCK TABLES Mysql也支持lock tables和unlock tables，这都是在服务器层（MySQL Server层）实现的，和存储引擎无关，它们有自己的用途，并不能替代事务处理。 （除了禁用了autocommint后可以使用，其他情况不建议使用）： LOCK TABLES 可以锁定用于当前线程的表。如果表被其他线程锁定，则当前线程会等待，直到可以获取所有锁定为止。 UNLOCK TABLES 可以释放当前线程获得的任何锁定。当前线程执行另一个 LOCK TABLES 时， 或当与服务器的连接被关闭时，所有由当前线程锁定的表被隐含地解锁 LOCK TABLES语法： 在用 LOCK TABLES 对 InnoDB 表加锁时要注意，要将 AUTOCOMMIT 设为 0，否则MySQL 不会给表加锁； 事务结束前，不要用 UNLOCK TABLES 释放表锁，因为 UNLOCK TABLES会隐含地提交事务； COMMIT 或 ROLLBACK 并不能释放用 LOCK TABLES 加的表级锁，必须用UNLOCK TABLES 释放表锁。 正确的方式见如下语句： 例如，如果需要写表 t1 并从表 t 读，可以按如下做： SET AUTOCOMMIT=0; LOCK TABLES t1 WRITE, t2 READ, ...; [do something with tables t1 and t2 here]; COMMIT; UNLOCK TABLES; 使用LOCK TABLES的场景： 给表显示加表级锁（InnoDB表和MyISAM都可以），一般是为了在一定程度模拟事务操作，实现对某一时间点多个表的一致性读取。（与MyISAM默认的表锁行为类似） 在用 LOCK TABLES 给表显式加表锁时，必须同时取得所有涉及到表的锁，并且 MySQL 不支持锁升级。也就是说，在执行 LOCK TABLES 后，只能访问显式加锁的这些表，不能访问未加锁的表；同时，如果加的是读锁，那么只能执行查询操作，而不能执行更新操作。 其实，在MyISAM自动加锁（表锁）的情况下也大致如此，MyISAM 总是一次获得 SQL 语句所需要的全部锁，这也正是 MyISAM 表不会出现死锁（Deadlock Free）的原因。 例如，有一个订单表 orders，其中记录有各订单的总金额 total，同时还有一个 订单明细表 order_detail，其中记录有各订单每一产品的金额小计 subtotal，假设我们需要检 查这两个表的金额合计是否相符，可能就需要执行如下两条 SQL： Select sum(total) from orders; Select sum(subtotal) from order_detail; 这时，如果不先给两个表加锁，就可能产生错误的结果，因为第一条语句执行过程中， order_detail 表可能已经发生了改变。因此，正确的方法应该是： Lock tables orders read local, order_detail read local; Select sum(total) from orders; Select sum(subtotal) from order_detail; Unlock tables; （在 LOCK TABLES 时加了“local”选项，其作用就是允许当你持有表的读锁时，其他用户可以在满足 MyISAM 表并发插入条件的情况下，在表尾并发插入记录（MyISAM 存储引擎支持“并发插入”）） 死锁（Deadlock Free） 死锁产生的原因 可归结为如下两点： a. 竞争资源 系统中的资源可以分为两类： 可剥夺资源，是指某进程在获得这类资源后，该资源可以再被其他进程或系统剥夺，CPU和主存均属于可剥夺性资源； 另一类资源是不可剥夺资源，当系统把这类资源分配给某进程后，再不能强行收回，只能在进程用完后自行释放，如磁带机、打印机等。 产生死锁中的竞争资源之一指的是竞争不可剥夺资源（例如：系统中只有一台打印机，可供进程P1使用，假定P1已占用了打印机，若P2继续要求打印机打印将阻塞） 产生死锁中的竞争资源另外一种资源指的是竞争临时资源（临时资源包括硬件中断、信号、消息、缓冲区内的消息等），通常消息通信顺序进行不当，则会产生死锁 b. 进程间推进顺序非法 若P1保持了资源R1,P2保持了资源R2，系统处于不安全状态，因为这两个进程再向前推进，便可能发生死锁 例如，当P1运行到P1：Request（R2）时，将因R2已被P2占用而阻塞；当P2运行到P2：Request（R1）时，也将因R1已被P1占用而阻塞，于是发生进程死锁 死锁产生四个必要条件： 产生死锁的四个必要条件： （1） 互斥条件：一个资源每次只能被一个进程使用。 （2） 占有且等待：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 （3）不可强行占有:进程已获得的资源，在末使用完之前，不能强行剥夺。 （4） 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环。 当事务试图以不同的顺序锁定资源时，就可能产生死锁。多个事务同时锁定同一个资源时也可能会产生死锁。 锁的行为和顺序和存储引擎相关。以同样的顺序执行语句，有些存储引擎会产生死锁有些不会——死锁有双重原因：真正的数据冲突；存储引擎的实现方式。 检测死锁：数据库系统实现了各种死锁检测和死锁超时的机制。InnoDB存储引擎能检测到死锁的循环依赖并立即返回一个错误。 死锁恢复：死锁发生以后，只有部分或完全回滚其中一个事务，才能打破死锁，InnoDB目前处理死锁的方法是，将持有最少行级排他锁的事务进行回滚。所以事务型应用程序在设计时必须考虑如何处理死锁，多数情况下只需要重新执行因死锁回滚的事务即可。 外部锁的死锁检测：发生死锁后，InnoDB 一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。但在涉及外部锁，或涉及表锁的情况下，InnoDB 并不能完全自动检测到死锁， 这需要通过设置锁等待超时参数 innodb_lock_wait_timeout 来解决 死锁影响性能：死锁会影响性能而不是会产生严重错误，因为InnoDB会自动检测死锁状况并回滚其中一个受影响的事务。在高并发系统上，当许多线程等待同一个锁时，死锁检测可能导致速度变慢。 有时当发生死锁时，禁用死锁检测（使用innodb_deadlock_detect配置选项）可能会更有效，这时可以依赖innodb_lock_wait_timeout设置进行事务回滚。 预防死锁： 资源一次性分配：一次性分配所有资源，这样就不会再有请求了：（破坏请求条件） 只要有一个资源得不到分配，也不给这个进程分配其他的资源：（破坏请保持条件） 可剥夺资源：即当某进程获得了部分资源，但得不到其它资源，则释放已占有的资源（破坏不可剥夺条件）超时放弃 资源有序分配法：系统给每类资源赋予一个编号，每一个进程按编号递增的顺序请求资源，释放则相反（破坏环路等待条件）以确定的顺序获得锁 避免死锁: 银行家算法：首先需要定义状态和安全状态的概念。系统的状态是当前给进程分配的资源情况。因此，状态包含两个向量Resource（系统中每种资源的总量）和Available（未分配给进程的每种资源的总量）及两个矩阵Claim（表示进程对资源的需求）和Allocation（表示当前分配给进程的资源）。 安全状态是指至少有一个资源分配序列不会导致死锁。当进程请求一组资源时，假设同意该请求，从而改变了系统的状态，然后确定其结果是否还处于安全状态。如果是，同意这个请求；如果不是，阻塞该进程知道同意该请求后系统状态仍然是安全的。 死锁检测 1、Jstack命令 jstack是java虚拟机自带的一种堆栈跟踪工具。jstack用于打印出给定的java进程ID或core file或远程调试服务的Java堆栈信息。 Jstack工具可以用于生成java虚拟机当前时刻的线程快照。线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 2、JConsole工具 Jconsole是JDK自带的监控工具，在JDK/bin目录下可以找到。它用于连接正在运行的本地或者远程的JVM，对运行在Java应用程序的资源消耗和性能进行监控，并画出大量的图表，提供强大的可视化界面。而且本身占用的服务器内存很小，甚至可以说几乎不消耗。 解除死锁: 当发现有进程死锁后，便应立即把它从死锁状态中解脱出来，常采用的方法有： 剥夺资源：从其它进程剥夺足够数量的资源给死锁进程，以解除死锁状态； 撤消进程：可以直接撤消死锁进程或撤消代价最小的进程，直至有足够的资源可用，死锁状态.消除为止；所谓代价是指优先级、运行代价、进程的重要性和价值等。 MyISAM避免死锁： 在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，所以 MyISAM 表不会出现死锁。 InnoDB避免死锁： 为了在单个InnoDB表上执行多个并发写入操作时避免死锁，可以在事务开始时通过为预期要修改的每个元祖（行）使用SELECT ... FOR UPDATE语句来获取必要的锁，即使这些行的更改语句是在之后才执行的。 在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应先申请共享锁、更新时再申请排他锁，因为这时候当用户再申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁 如果事务需要修改或锁定多个表，则应在每个事务中以相同的顺序使用加锁语句。 在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会 通过SELECT ... LOCK IN SHARE MODE获取行的读锁后，如果当前事务再需要对该记录进行更新操作，则很有可能造成死锁。 改变事务隔离级别 如果出现死锁，可以用 SHOW INNODB STATUS 命令来确定最后一个死锁产生的原因。返回结果中包括死锁相关事务的详细信息，如引发死锁的 SQL 语句，事务已经获得的锁，正在等待什么锁，以及被回滚的事务等。据此可以分析死锁产生的原因和改进措施。 一些优化锁性能的建议 尽量使用较低的隔离级别； 精心设计索引， 并尽量使用索引访问数据， 使加锁更精确， 从而减少锁冲突的机会 选择合理的事务大小，小事务发生锁冲突的几率也更小 给记录集显示加锁时，最好一次性请求足够级别的锁。比如要修改数据的话，最好直接申请排他锁，而不是先申请共享锁，修改时再请求排他锁，这样容易产生死锁 不同的程序访问一组表时，应尽量约定以相同的顺序访问各表，对一个表而言，尽可能以固定的顺序存取表中的行。这样可以大大减少死锁的机会 尽量用相等条件访问数据，这样可以避免间隙锁对并发插入的影响 不要申请超过实际需要的锁级别 除非必须，查询时不要显示加锁。 MySQL的MVCC可以实现事务中的查询不用加锁，优化事务性能；MVCC只在COMMITTED READ（读提交）和REPEATABLE READ（可重复读）两种隔离级别下工作 对于一些特定的事务，可以使用表锁来提高处理速度或减少死锁的可能 乐观锁、悲观锁 乐观锁(Optimistic Lock)：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 乐观锁不能解决脏读的问题。 乐观锁, 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。 如修改时先进行version等CAS判断手段 悲观锁(Pessimistic Lock)：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。 悲观锁，顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 使用事务,进行索引的使用时,会依据事务的隔离级别,若是应对读提交隔离级别的不可重复读问题,则用行锁,若是应对可重读的幻读问题,则用表锁等. MySQL 存储过程 优点 存储过程可封装，并隐藏复杂的商业逻辑。 存储过程可以回传值，并可以接受参数。 存储过程无法使用 SELECT 指令来运行，因为它是子程序，与查看表，数据表或用户定义函数不同。 存储过程可以用在数据检验，强制实行商业逻辑等。 缺点 存储过程，往往定制化于特定的数据库上，因为支持的编程语言不同。当切换到其他厂商的数据库系统时，需要重写原有的存储过程。 存储过程的性能调校与撰写，受限于各种数据库系统。 一、存储过程的创建和调用 存储过程就是具有名字的一段代码，用来完成一个特定的功能。 创建的存储过程保存在数据库的数据字典中。 创建存储过程 CREATE [DEFINER = { user | CURRENT_USER }] 　PROCEDURE sp_name ([proc_parameter[,...]]) [characteristic ...] routine_body proc_parameter: [ IN | OUT | INOUT ] param_name type characteristic: COMMENT 'string' | LANGUAGE SQL | [NOT] DETERMINISTIC | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } routine_body: 　　Valid SQL routine statement [begin_label:] BEGIN 　　[statement_list] 　　　　…… END [end_label] MYSQL 存储过程中的关键语法 声明语句结束符，可以自定义: DELIMITER $$ 或 DELIMITER // 声明存储过程: CREATE PROCEDURE demo_in_parameter(IN p_in int) 存储过程开始和结束符号: BEGIN .... END 变量赋值: SET @p_in=1 变量定义: DECLARE l_int int unsigned default 4000000; 创建mysql存储过程、存储函数: create procedure 存储过程名(参数) 存储过程体: create function 存储函数名(参数) 实例 下面是存储过程的例子，删除给定球员参加的所有比赛： mysql> delimiter $$　　#将语句的结束符号从分号;临时改为两个$$(可以是自定义) mysql> CREATE PROCEDURE delete_matches(IN p_playerno INTEGER) -> BEGIN -> 　　DELETE FROM MATCHES -> WHERE playerno = p_playerno; -> END$$ Query OK, 0 rows affected (0.01 sec) mysql> delimiter;　　#将语句的结束符号恢复为分号 解析：默认情况下，存储过程和默认数据库相关联，如果想指定存储过程创建在某个特定的数据库下，那么在过程名前面加数据库名做前缀。 在定义过程时，使用 DELIMITER $$ 命令将语句的结束符号从分号 ; 临时改为两个 $$，使得过程体中使用的分号被直接传递到服务器，而不会被客户端（如mysql）解释。 调用存储过程： call sp_name[(传参)]; mysql> select * from MATCHES; +---------+--------+----------+-----+------+ | MATCHNO | TEAMNO | PLAYERNO | WON | LOST | +---------+--------+----------+-----+------+ | 1 | 1 | 6 | 3 | 1 | | 7 | 1 | 57 | 3 | 0 | | 8 | 1 | 8 | 0 | 3 | | 9 | 2 | 27 | 3 | 2 | | 11 | 2 | 112 | 2 | 3 | +---------+--------+----------+-----+------+ 5 rows in set (0.00 sec) mysql> call delete_matches(57); Query OK, 1 row affected (0.03 sec) mysql> select * from MATCHES; +---------+--------+----------+-----+------+ | MATCHNO | TEAMNO | PLAYERNO | WON | LOST | +---------+--------+----------+-----+------+ | 1 | 1 | 6 | 3 | 1 | | 8 | 1 | 8 | 0 | 3 | | 9 | 2 | 27 | 3 | 2 | | 11 | 2 | 112 | 2 | 3 | +---------+--------+----------+-----+------+ 4 rows in set (0.00 sec) 解析：在存储过程中设置了需要传参的变量p_playerno，调用存储过程的时候，通过传参将57赋值给p_playerno，然后进行存储过程里的SQL操作。 存储过程体 存储过程体包含了在过程调用时必须执行的语句，例如：dml、ddl语句，if-then-else和while-do语句、声明变量的declare语句等 过程体格式：以begin开始，以end结束(可嵌套) BEGIN 　　BEGIN 　　　　BEGIN 　　　　　　statements; 　　　　END 　　END END 注意：每个嵌套块及其中的每条语句，必须以分号结束，表示过程体结束的begin-end块(又叫做复合语句compound statement)，则不需要分号。 为语句块贴标签: [begin_label:] BEGIN 　　[statement_list] END [end_label] 二、存储过程的参数 MySQL存储过程的参数用在存储过程的定义，共有三种参数类型,IN,OUT,INOUT,形式如： CREATEPROCEDURE 存储过程名([[IN |OUT |INOUT ] 参数名 数据类形...]) IN 输入参数：表示调用者向过程传入值（传入值可以是字面量或变量） OUT 输出参数：表示过程向调用者传出值(可以返回多个值)（传出值只能是变量） INOUT 输入输出参数：既表示调用者向过程传入值，又表示过程向调用者传出值（值只能是变量） 三、变量 1. 变量定义 局部变量声明一定要放在存储过程体的开始： DECLAREvariable_name [,variable_name...] datatype [DEFAULT value]; 其中，datatype 为 MySQL 的数据类型，如: int, float, date,varchar(length) 例如: DECLARE l_int int unsigned default 4000000; DECLARE l_numeric number(8,2) DEFAULT 9.95; DECLARE l_date date DEFAULT '1999-12-31'; DECLARE l_datetime datetime DEFAULT '1999-12-31 23:59:59'; DECLARE l_varchar varchar(255) DEFAULT 'This will not be padded'; 2. 变量赋值 SET 变量名 = 表达式值 [,variable_name = expression ...] 四、注释 MySQL 存储过程可使用两种风格的注释 两个横杆--：该风格一般用于单行注释。 c 风格： 一般用于多行注释。 例如： mysql > DELIMITER // mysql > CREATE PROCEDURE proc1 --name存储过程名 -> (IN parameter1 INTEGER) -> BEGIN -> DECLARE variable1 CHAR(10); -> IF parameter1 = 17 THEN -> SET variable1 = 'birds'; -> ELSE -> SET variable1 = 'beasts'; -> END IF; -> INSERT INTO table1 VALUES (variable1); -> END -> // mysql > DELIMITER ; MySQL存储过程的调用 用call和你过程名以及一个括号，括号里面根据需要，加入参数，参数包括输入参数、输出参数、输入输出参数。具体的调用方法可以参看上面的例子。 MySQL存储过程的查询 我们像知道一个数据库下面有那些表，我们一般采用 showtables; 进行查看。那么我们要查看某个数据库下面的存储过程，是否也可以采用呢？答案是，我们可以查看某个数据库下面的存储过程，但是是另一钟方式。 我们可以用以下语句进行查询： selectname from mysql.proc where db='数据库名'; 或者 selectroutine_name from information_schema.routines where routine_schema='数据库名'; 或者 showprocedure status where db='数据库名'; MySQL查看存储过程的详细 SHOWCREATE PROCEDURE 数据库.存储过程名; 就可以查看当前存储过程的详细。 MySQL存储过程的修改 ALTER PROCEDURE 更改用 CREATE PROCEDURE 建立的预先指定的存储过程，其不会影响相关存储过程或存储功能。 MySQL存储过程的删除 删除一个存储过程比较简单，和删除表一样： DROPPROCEDURE MySQL存储过程的控制语句 (1). 变量作用域 内部的变量在其作用域范围内享有更高的优先权，当执行到 end。变量时，内部变量消失，此时已经在其作用域外，变量不再可见了，应为在存储过程外再也不能找到这个申明的变量，但是你可以通过 out 参数或者将其值指派给会话变量来保存其值。 mysql > DELIMITER // mysql > CREATE PROCEDURE proc3() -> begin -> declare x1 varchar(5) default 'outer'; -> begin -> declare x1 varchar(5) default 'inner'; -> select x1; -> end; -> select x1; -> end; -> // mysql > DELIMITER ; (2). 条件语句 if-then-else 语句 mysql > DELIMITER // mysql > CREATE PROCEDURE proc2(IN parameter int) -> begin -> declare var int; -> set var=parameter+1; -> if var=0 then -> insert into t values(17); -> end if; -> if parameter=0 then -> update t set s1=s1+1; -> else -> update t set s1=s1+2; -> end if; -> end; -> // mysql > DELIMITER ; case语句： mysql > DELIMITER // mysql > CREATE PROCEDURE proc3 (in parameter int) -> begin -> declare var int; -> set var=parameter+1; -> case var -> when 0 then -> insert into t values(17); -> when 1 then -> insert into t values(18); -> else -> insert into t values(19); -> end case; -> end; -> // mysql > DELIMITER ; case when var=0 then insert into t values(30); when var>0 then when var (3). 循环语句 while ···· end while mysql > DELIMITER // mysql > CREATE PROCEDURE proc4() -> begin -> declare var int; -> set var=0; -> while var insert into t values(var); -> set var=var+1; -> end while; -> end; -> // mysql > DELIMITER ; while 条件 do --循环体 end while repeat···· end repeat 它在执行操作后检查结果，而 while 则是执行前进行检查。 mysql > DELIMITER // mysql > CREATE PROCEDURE proc5 () -> begin -> declare v int; -> set v=0; -> repeat -> insert into t values(v); -> set v=v+1; -> until v>=5 -> end repeat; -> end; -> // mysql > DELIMITER ; repeat --循环体 until 循环条件 end repeat; loop ·····endloop loop 循环不需要初始条件，这点和 while 循环相似，同时和 repeat 循环一样不需要结束条件, leave 语句的意义是离开循环。 mysql > DELIMITER // mysql > CREATE PROCEDURE proc6 () -> begin -> declare v int; -> set v=0; -> LOOP_LABLE:loop -> insert into t values(v); -> set v=v+1; -> if v >=5 then -> leave LOOP_LABLE; -> end if; -> end loop; -> end; -> // mysql > DELIMITER ; LABLES 标号： 标号可以用在 begin repeat while 或者 loop 语句前，语句标号只能在合法的语句前面使用。可以跳出循环，使运行指令达到复合语句的最后一步。 (4). ITERATE迭代 ITERATE 通过引用复合语句的标号,来从新开始复合语句: mysql > DELIMITER // mysql > CREATE PROCEDURE proc10 () -> begin -> declare v int; -> set v=0; -> LOOP_LABLE:loop -> if v=3 then -> set v=v+1; -> ITERATE LOOP_LABLE; -> end if; -> insert into t values(v); -> set v=v+1; -> if v>=5 then -> leave LOOP_LABLE; -> end if; -> end loop; -> end; -> // mysql > DELIMITER ; 索引长度限制 MySQL中给varchar和text字段建立普通索引时,最好指定索引长度. InnoDB单列索引长度不能超过767字节,联合索引不能超过3072字节. utf8每个字符占用3个字节,那么该列索引最大长度为floor(767/3)=255. utf8mb4每个字符占用4个字节,那么该列索引最大长度为floor(767/4)=191. 比如WordPress的wp_posts表的post_title字段类型为utf8mb4_unicode_520_ci,它的索引定义是: KEY post_name (post_name(191)) 其中191就是索引长度. 全文索引应该是没有长度限制的. 错误提示是语法错误,全文索引是这样创建的, 比如给attr_value_ids字段创建全文索引: text CREATE TABLE IF NOT EXISTS `goods_attr_value_fts` ( `goods_id` bigint unsigned NOT NULL COMMENT '商品编号', `attr_value_ids` text NOT NULL COMMENT '商品具有的属性值编号', PRIMARY KEY (`goods_id`), FULLTEXT KEY `ft_attr_value_ids` (`attr_value_ids`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci COMMENT='商品和属性筛选表'; powered by Gitbook该文件最后修改时间： 2021-07-21 23:18:59 "},"配置中心/disConf.html":{"url":"配置中心/disConf.html","title":"DisConf","keywords":"","body":"简介 Distributed Configuration Management Platform（分布式配置管理） https://blog.csdn.net/qinxuefly/article/details/53287189 模块架构图 客户端使用流程 导入jar包 com.baidu.disconf disconf-client 2.6.30 在classpath.xml添加Disconf启动支持 classpath*:dms-admin.properties 在客户端应用的classpath下新增disconf.properties文件 disconf.properties会在DIsconfMgrBean注册时(实现了BeanDefinitionRegistryPostProcessor接口，成功注册了BeanDefinition，使得之后BeanFactory使用BeanDefinition进行Bean的实例化、注入属性，前置后置处理、实现Aware接口，初始化垫了基础)在FirstScan方法调用时被读取入DisClientConfig对象中 # 是否使用远程配置文件 # true(默认)会从远程获取配置 false则直接获取本地配置 enable.remote.conf=true # # 配置服务器的 HOST,用逗号分隔 127.0.0.1:8000,127.0.0.1:8000 # conf_server_host=127.0.0.1:8080 # 版本, 请采用 X_X_X_X 格式 version=1_0_0_0 # APP 请采用 产品线_服务名 格式 app=disconf_demo # 环境 env=rd # debug debug=true # 忽略哪些分布式配置，用逗号分隔 ignore= # 获取远程配置 重试次数，默认是3次 conf_server_url_retry_times=1 # 获取远程配置 重试时休眠时间，默认是5秒 conf_server_url_retry_sleep_seconds=1 详细设计请参考： http://disconf.readthedocs.io/zh_CN/latest/design/index.html 配置项注解使用 为这个类定义 @DisconfFile 注解，指定文件名为 code.properties 。 定义域codeError，并使用Eclipse为其自动生成 get&set 方法。 为该域的get方法上添加注解 @DisconfFileItem 。添加标记 name, 表示配置文件中的KEY名，这是必填的 标记associateField是可选的，它表示此get方法相关连的域的名字，如果此标记未填，则系统会自动 分析get方法，猜测其相对应于域名。强烈建议添加associateField标记，这样就可以避免Eclipse生成的Get/Set方法不符合 Java规范的问题。 标记它为Spring托管的类 （使用@Service），且 \"scope\" 都必须是singleton的。 5.配置更新回调 实现IDisconfUpdate接口，并且该类是由spring管理 注解@DisconfUpdateService， confFileKeys为监控配置文件更新，itemKeys为监控配置项更新 @Slf4j @Configuration @Component @DisconfFile(filename = \"producer.kafka.value.json\") @DisconfUpdateService(classes = {KafkaProducerBeanFactory.class}) public class KafkaProducerBeanFactory implements InitializingBean,IDisconfUpdate{ 若是程序本身：程序不会自动reload配置，需要自己写回调函数(实现IDisconfUpdate接口，并添加DisconfUpdateService注解)。 若仅仅是托管文件本身： classpath*:dms-admin.properties 服务端部署 Linux：https://blog.csdn.net/qinxuefly/article/details/53287189 windows：https://www.jianshu.com/p/131c56f2a934 疑问 4.1 Disconf怎么做到实时修改？ Disconf主要是依靠zookeeper的Watch机制来做配置实时修改的,我们都知道ZK是通过目录挂载的方式来做服务的自动注册与发布。客户端启动时注册了一个回调接口,当zk目录发生变化时会回调所有客户端节点,从而做到\"实时\"更新配置的目的。 4.2 Disconf怎么做到数据持久化？ 在配置中心添加的配置数据都被持久化到了DB中,每次客户端启动的时候会调用Disconf的Http接口获取最新的配置数据,如果网络不通,默认会重试三次,如果还不通,则抛出异常。如果第一次拉取配置就有问题,作为配置中心来讲是肯定是无解的,需要客户端去解决（一般这种情况是网络问题或者配置中心服务不可用导致）。 我们这里不需要考虑第一次加载配置就失败的情况.那么问题来了： 1.如果第一次加载配置后,配置中心不可用,会影响到客户端吗？答案：不会。因为动态修改的配置已经加载到内存了,是不影响的。 2.如果配置中心宕机,加上客户端需要发布新版本面临重启呢？答案：不影响,因为第一次从配置中心拉取配置后会持久化到磁盘固定目录上,如果配置中心不可用,会读取缓存文件的数据(需要做文件防篡改操作) 4.3 如果ZK宕机会影响配置中心使用吗？ 答案:不会,因为disconf会单独起一个线程做重连操作。 4.4 Disconf怎么保证同一项目、环境所在节点都修改配置成功。 答案:没有做这方面的保证。因为客户端连接到配置中心上以后会将机器名挂载到zk目录下,可以通过界面查看配置使用的机器数。 Disconf自定义功能扩展 1 客户端接入功能增强,增加默认配置属性。 2 Disconf针对key-value配置属性没有做持久化,当配置中心宕机、客户端重启时无法拉取历史配置信息。 3 增强Disconf节点之间配置不同步的问题 4 去掉修改配置同步发送功能,改成异步(它采用的是同步发送,非常卡慢) 配置时各个bean的作用 DisconfMgrBean 此Bean实现了BeanFactoryPostProcessor和PriorityOrdered接口。它的Bean初始化Order是最高优先级的。 因此，当Spring扫描了所有的Bean信息后，在所有Bean初始化（init）之前，DisconfMgrBean的postProcessBeanFactory方法将被调用，在这里，Disconf-Client会进行第一次扫描。 扫描按顺序做了以下几个事情： 初始化Disconf-client自己的配置模块。 初始化Scan模块。 初始化Core模块，并极联初始化Watch，Fetcher，Restful模块。 扫描用户类，整合分布式配置注解相关的静态类信息至配置仓库里。 执行Core模块，从disconf-web平台上下载配置数据：配置文件下载到本地，配置项直接下载。 配置文件和配置项的数据会注入到配置仓库里。 使用watch模块为所有配置关联ZK上的结点。 DisconfMgrBeanSecond DisconfMgrBean的扫描主要是静态数据的初始化，并未涉及到动态数据。DisconfMgrBeanSecond Bean则是将一些动态的数据写到仓库里。 本次扫描按顺序做了以下几个事情： 将配置更新回调实例放到配置仓库里 为配置实例注入值。 当DisconfMgrBean第一次扫描时，watcher监控不到相应的类时，但是对应的disconf.user_define_download_dir中再本地存在相应的properties，MgrBeanSecond也能执行注入 ReloadablePropertiesFactoryBean ReloadablePropertiesFactoryBean继承了PropertiesFactoryBean类，它主要做到： 托管配置文件至disconf仓库，并下载至本地。 解析配置数据传递到 ReloadingPropertyPlaceholderConfigurer ReloadingPropertyPlaceholderConfigurer ReloadingPropertyPlaceholderConfigurer继承自Spring的配置类PropertyPlaceholderConfigurer，它会在Spring启动时将配置数据与Bean做映射，以便在检查到配置文件更改时，可以实现Bean相关域值的自动注入。 如果想配置文件，但是不想自动reload，那么该怎么办？ myserver.properties 在这里，myserver.properties被disconf托管，当在disconf-web上修改配置文件时，配置文件会被自动下载至本地，但是不会reload到系统里。 ReloadConfigurationMonitor 它是一个Timer类，定时校验配置是否有更改，进而促发 ReloadingPropertyPlaceholderConfigurer 类来分析要对哪些 Bean实例进行重新注入。 powered by Gitbook该文件最后修改时间： 2021-09-26 22:24:52 "},"项目结构/ccs项目结构.html":{"url":"项目结构/ccs项目结构.html","title":"Ccs项目结构","keywords":"","body":"数据结构设计 项目模块的结构 ccs-business： 项目业务服务代码的实现模块 ccs-cache: Redis 缓存的具体配置功能，在bean.xml中实现了对tm_config和xxx的redis启动刷新 ccs-web：web项目构建模块 ccs-common:这是项目的公共模块，主要用到的有Util包中的JsonUtil和HttpUtil用于别的系统接口调用和对cache取出的config等json包进行映射使用 gradle 配置 项目通过gradle-eclipse 清理项目eclipse配置和 启动eclipse task和eclipseJettyLaunch task eclipse和 eclispseJettyLaucn 都是为了加载eclipse的classpath配置等 同时因为项目的Build LifeStyle因此初始化了gradle 的Project对象,并触发了项目的build.gradle，给Project添加配置,通过setting.gradle的include Task 导入了其他模块的build gradle. 而gradle中设置最多的便是project 的 gradle配置 其中需要添加什么依赖可以到project模块的build.gradle添加依赖 javaProject.gradle 定义了项目以java形式运行时的配置如output的路径 jibx.gradle是一个为Java提供的XML数据绑定框架,但是系统中没看到 webappProject.gradle 是打包时的配置的,如清除webapp生成数据的cleanWebappRuns和服务运行的jetty容器配置等 包的结构 设计一个后台功能大概需要涉及的包(ccs-business): rest(Controller模块,返回给前端的需要用Reslut/RestResult封装返回) manager层,service层,biz层 manager层： 负责将Dao层中的数据库操作组合复用，主要是一些缓存方案，中间件的处理，以及对第三方平台封装的层。 service层： 更加关注业务逻辑，是业务处理层，将manager组合过的操作和业务逻辑组合在一起，再封装成业务操作。 biz层： 包含service层，service层注重基础业务的处理，biz层是复杂应用层的业务层。 repository,dao(若是用到数据库映射框架,都是hibernate) 公司的dao层主要面对本系统内部的,直接面对数据库的 repository是需要通过https获取其他系统的数据,再面向数据库的 domain(dto/request等数据对象,用hibernate的话可以用xml或者在dto对象上用@table注释) 数据库方面:目前只知道dms和tms 的测试数据库地址,ccs和excp都不认识 前端交互流程 1、评审阶段：产品召集前后端进行需求评审，前后端各自捋清楚自己的业务量以及联调之间工作量，从而进行开发时间评估。 2、开发准备阶段：前后端一起商量需求中需要联调的部分，进行接口的口头协议交流。 3、接口定义阶段：前后端中的一方根据之前的口头协议拟定出一份详细的接口，并书写APl文档，完成后由另一方确认。有疑问的地方重新商量直至双方都没有问题。 4、开发阶段：双方根据协商出来的接口为基础进行开发，如在开发过程中发现需要新增或删除一些字段，重复步骤3。 注意：前端在开发过程中记得跟进接口，mock数据进行本地测试。 5、联调阶段：双方独自的工作完成，开始前后端联调，如在联调过程发现有疑问，重复步骤3，直至联调完成。 6、产品体验阶段：将完成的需求交给产品，让其体验，直至产品这边没有问题 7、提测阶段：将完成的需求提给测试人员，让其对该需求进行测试，如发现问题，及时通知开发并让其修改，直至需求没有bug。 8、评审单发布阶段：前后端中的一人进行评审单的拟定，发送给对应的领导，表明需求发布的程序，包括影响到的页面及业务，发布的流程，发布的回滚方案等。 9、发布阶段：前后端双方在保证步骤1—8都没有问题了，进行各自的代码发布，完成后由测试人员在线上进行相应的测试，如果有bug，重复步骤7和9，直至需求成功上线。 可以在我开发前先提供一些需要的参数,先设定各个方法需要的dto的属性,和接口名等信息 也可以在我做完后再一起提供接口名和dto信息,方便前端联调 接口注意事项 接口命名 查询 queryXXX/getXXX 插入 insertXXX/addXXX 更新 updateXXX 删除 deleteXXX 判断 checkXXX(数据库用is_xxx,但需要表明映射) 统计 staticXXX 代码质量 接口是没有静态代码块的statci{}，也没有构造方法，一个类的父类只能有一个，但是一个类可以同时实现多个接口 格式：public class Myinterfaceimpl implements MyinterfaceA,MyInterfaceB{覆盖重写所有的抽象方法} 如果多个接口含有相同的抽象方法，则直接重写一回即可 如果没有覆盖重写全部的，必须是抽象类 接口中有相同的默认方法必须要重写，抽象类中也要重写 一个类如果直接父类的方法与接口的默认方法冲突，优先使用父类的方法*/ 注释文档 个人的困惑 编程的时候,刚开始初步浪费时间的较多的地方,便是在数据库和dto,方法等的命名上,对于各自的系统,应该各自对高频关键字应该有一份推荐的模板,有益于提高开发效率,也减少以后阅读维修的难度 对于各个中间件,如redis和kafka,最好注释或者文档能清晰的标识其配置逻辑、已经使用的有哪些与能哪些类型的功能需要被使用的情况，不然刚上手如我现在需要问人是否要先从cache中获取，以及是否有获取不到的情况放到数据库中还是刷新缓存等处理 对于需求，对于一些需求有什么特殊的配置需求（存储、中间件使用等），可以共同编辑到需求文档中，如现在的添加配置用tm_config这张表而不是新建表。 对于前端后台交互，针对一些直接交互的控件：哪个功能是后台逻辑实现哪个是前端实现可以先前期交流并加入到需求文档中 对于数据库、表的功能还有各个的测试环境地址，小组有专门的文档，可以加快项目的理解速度，而不需要找到各个功能的dao层去查看 powered by Gitbook该文件最后修改时间： 2021-08-08 22:35:20 "}}